uh Hey guys uh welcome uh to S Pune office and welcome to uh devday so uh this D day is a I'll just give a brief about what D day is uh D Day say we we as in Pro provide a platform where we monthly talk about different technical uh topics technical topics be it be on kubernetes be it be on how the mechanical sympathy works or be it be on any Docker related thing GC jvm we have lot of topics covered all of you can see those topics on the YouTube channel so those are there uh we'll provide you the link uh maybe at the at the end you can go through if you have not already seen it um so today we are going to cover about the bootstrapping of communities this topic and Prashant is going to talk about it uh I'm handing over to pant hey okay hello everyone you can see all this okay so let's start so so I will just give you like why we thought of uh doing that so there is a internal uh kind of a program that we do to basically talk about infrastructure things it was like that that stream is at least about the infrastructure uh related topics so one of the thing that we were covering is the like kubernetes architecture how how the kubernetes actually works to a bit of a uh deep level in the sense that how the nodes themselves are configured and all of that uh this uh actually so uh this is basically done as part of that thing one thing that comes out out of this is if you have this understanding how kubernetes itself is working then it makes lot of things easier in terms of figuring out okay which component do I need to look at that possibly have the issues this uh for any given problem that you have or which component is probably not working the expected way and all of that so that will actually that is where this concept essentially came in anyway you might have looked at the architecture and all of this but looking at actually doing it on a node that will actually clarify a lot of things that how it really works okay so this is basically the agenda will start with a kubernetes node the machine could we'll start with a kubernetes node uh uh this the node is not completely set up that's the whole point of it and then what we'll do is we'll try to run a workload on top of it uh and obviously it will not work so now we have what we need to do it so instead of basically going and doing everything up front we are going to actually do it the other way around we like take a like a discovery kind of a route and say that okay what do I need to get ahead with this particular error so whatever is the current error how do I actually solve it and that's it we will not look at the whole picture we will just try to see that particular thing and then see that okay what am I missing in here and then basically just uh install or deploy that thing and then then move forward from there so you will see a lot of issues that will come in we will also see that how can I actually look at and how can I actually solve those issues okay so incrementally set up and learn how the components works and fit together at a high level okay so at very start what do I have okay so obviously I I just have a single node ec2 instance cluster it's like a single node that is there and what we are already installing on top of it is so we have Docker installed okay so Docker is already running on on that machine then we have a like a Docker shim the CRI Docker Docker D is a Docker shim that is installed so basically Docker does not follow the CRI standard out of the box okay and then kubernetes cannot talk with Docker because it is talking with the like with the the language is the CRI abstraction okay so and if the docker is not following it then you need a way to translate those commands into the docker's own way of doing it so that's where the docker shim comes in it was earlier part of the kubernetes main code itself but then they removed it and that was like that was the main change in 1.24 I think uh which was a big deal that kubernetes is dropping docker port and all of that so that is not the case but it is basically the shrim that is what is being removed and now there are other uh separately maintained uh projects that are actually implementing those things so one of them is the SI do so I am using that as a shame here so that way my kubernetes can talk with the docker essentially then I already have the uh Cube cutle as a uh Cube as a service installed okay so cuet as a service is already running on the instance okay uh then I'm going to use Cube ADM to do the whole kubernetes setup okay so any components or anything that we will do we will basically use Cube ADM to do it uh the thing is so Cube ADM has like one like Global command that you can run and then it will do everything for you but it also has sub commands very specific commands that you can actually use and see what happen so that is the thing that we are going to use we are basically going to use very specific commands uh and only configure those particular components uh to move throughout the uh Discovery okay so Cube ADM is there then the cubet is set up so we need a client to talk with the kubernetes so Cube ADM is also there so same machine we are doing everything on that same machine so apart from just the installation I have done some primary setup in terms of some commands of the Cub adum like pre-flight and all of those those commands are executed so some configuration of cuet is done okay then uh I have already uh used the Cub adum to create create a cube config so that means my Cube cutle can actually talk with a kubernetes at least it is thinking that that's where I need to talk to so that configuration is already also in place so Cube config is created and then uh cube cutle is essentially using that good Cube config okay but apart from this none nothing else is there on that instance okay I will quickly show you uh repo that will be shared at the end okay so I will share this uh GitHub repo as well all the steps and everything uh is there it is not yet public but I will make it public and then share it across uh you if you want later on you can also follow the entire thing everything uh that is required is part of this repo including the instructions the prese setup and uh the the flow that we are going to follow as part of this stock that is all everything will be there okay so if you see uh at this start you can see that what all things I talked about already is there okay and uh uh so I will quickly show that how is it set up uh in the sense that uh how the IM how the image and uh the E2 instance is created so I'm using the terraform code for this uh to set up the2 instance and then there is a there is a script that it essentially uses to set up all of this so it is actually creating a fresh instance so when you actually use it in your uh if you are basically doing it later on you can just run the terraform script and it will create a node for you so you just need to follow these steps that are there so you can do terraform is it visible or should I zoom in okay is this visible Zoom a bit more okay so all this setup and everything is already available here uh I haven't uploaded the slides but I will add that link here as well okay so that is the so that is basically what we have right now coming back to the presentation okay so this is where we are okay henceforth I will basically not talk about all the other things that are not really part of the kubernetes architecture so basically Cube ADM is just a tool that we are using so it's not kuet is cube cutle is a client so it's not part of the server Cube config is again used by the cube cutle so I'm not going to talk about that so from a k point of view from a node point of view this is all we have okay we have a CRI which is Docker and we have a cube Lade that is running that is the only component that we have installed now for the for this node nothing else is there okay so now let's see what are we trying to actually do okay so our job or our aim is to basically run this command and make this command work uh for us okay and as part of running this we will explore everything that is available uh everything uh that we need to set up we will essentially do that setup okay so that's uh that's where we are so let's try to SS into that instance first okay so I'm inside that instance I can quickly show you what is there running so if you see the systems C okay so cuate is one servy that is running okay ignore all the errors and everything then you can also see that Docker is running okay so Docker is running there it's available then the main thing is cube cutle is also available okay so all of this is all of this is already installed okay now coming back to what we are trying to do okay we said that the very start we said that this is what we want to essentially Run Okay so the main aim is to get this command running okay so let's start so if I just run this command uh it is saying that it is trying to connect to this particular IP which is 10 13927 which is the local IP you can see that here as well that's the IP of the node essentially it is trying to connect to this port 6443 and it is saying that connection is refused are you using the right host and Port okay if we want more details we can basically see what it is actually doing okay so if you actually put in a verbos uh or enable the verbos city then you can basically say that it's actually trying to do a c command or it is trying to send a uh HTTP request to the same port and IP with a slash API as the endpoint and basically it is not able to connect to it so that's what you are getting connect connection refused okay so then what do you think is missing what it is trying to connect to what is running on this what might be expected to run on on this port 6443 which will give you the apis which apis it is trying to connect to any ideas in the kubernetes when you are trying to use the cube CLE Cube CTL commands what are you trying to talk to which component you are trying to talk to API server right so kubernetes API server is what is missing you can also so we can try any other commands also not necessarily we need to create a pod we can also say that just give me the pods I'm just doing a get okay I'm expecting that I will get exactly the same error because the fundamental component that I'm trying to talk to itself is not there okay so it not it is not about creation of the Pod I'm not able to talk to the kubernetes at all okay so who is the front component that actually receives every request that you are getting from any client that is the main API that's how it works okay so that that means we are essentially missing the kubernetes API component okay we can also try to look at so we can check whether anything is running on that 6443 so you can see that there is nothing there it will tell you everything but there's nothing on the 6443 okay so that is what is act missing so then how how do we go about installing it so that's where the uh so we are basically using the cube ADM to do all the setup so Cube ADM if you see the cube ADM documentation it has a lot of phases that are there essentially the init init is the main thing but if you go to the cube ADM and you go to cube ADM in it that is what you use use to set up the control plan you can see that here as well I should Zoom it out okay okay so it is basically in it is the primary phase but we are not going to do in it like at the very top because you can do that and then it will do everything for you but we want to only talk about the aps that's what is missing okay so if you go inside there are a lot of things here but I will go at very bottom and because I I don't want to do in it I want to do a specific phase so there is a CU cub adum in it phase okay so if I go to cube ADM in it phase I should be able to figure out where is the where is the API in it okay so it does a lot of things you can easier to look at it on the right hand side it is doing the pre-flight cubet start and all of this what we want is the control plane okay okay that's where the you can set up the entire control plan in one command but we don't want that we just want the API server to essentially be there for us okay to start with we can use this command uh to basically set up the API server but we will run it in a like a verbos mode just to see what it is doing and then try to understand that okay that is how you do the setup for a particular component Okay so so if I now run this command okay one one thing I should talk about is that there is also a file I have created which is the cube ADM config I will quickly show this file okay so there are only two interesting so a lot of things is there but only two important uh lines are there in this one is the CRI socket so in uh you Bas basically because we are using Docker and and the docker shim and all that you have to specifically say that how do I connect what is the docker socket to which the kubernetes can connect to so this is basically the path to it so you are basically passing in the path uh then advertise address is BAS is the Local Host so I am saying that let me access on this particular private IP because I'm not accessing it from outside otherwise you will do like a outside and so you will basically give a load balancer IP and all of that here okay but we are not doing that we are just trying to understand for our own thing so this is not really a production kind of a setup but this allows me to essentially connect to the component we are saying that allow me to connect or expose those components on this particular IP so that when the traffic comes in on that particular IP it will basically be taken by the components okay then uh the only thing apart from that we are doing is the uh uh the Pod subnet which is the cedar okay so we are basically saying the pods should use this Cedar to create the Pod IPS and all of that so that is the only setup uh you will see why we did the Pod subnet thing later on uh but that is the only setup that is there in the Cub adum config so the reason we uh I showed that there is that we should we need to use that c ADM config uh to make all of this essentially work so if I go to the where is that okay so this is the command I'm going to use I just added sudo nothing else okay apart from that we want to make it run it in verbos to see what what it is going on and the last thing is I need to pass in the config I hope I'm using the right let me check only config okay okay so I'm passing in the config the only thing I did apart from whatever is there in the documentation is barbos and the config okay okay so let's look at what it is doing first thing is it is like not every detail is important okay for us at least we will not go into every detail but we are trying to understand at the kubernetes level okay what are the component what are they doing and how they are being set up that's the main uh thing here so it is doing few things like it is trying to figure out which Cube cutle uh kubernetes version you are trying to use and all of that I am trying to use the uh 1.24 okay because time I created all of this was at that time 1.24 was there were earlier there were later releases as well but that was what I was working with so I used that you can see that it is saying that there is a version available 1.29 but the cubet and everything else that is installed is 1.24 that's why it is essentially trying to use the 1.24 now it is saying that fetching kubernetes version 1.24 okay it is doing certification and all of them that all of that is not really important there are two lines that are quite important for us one is it is saying that creating a static pod okay static F manifest for cube API server okay that is one thing and the another is it is saying writing static P manifest to component for component Cube API server to hcd kubernetes etc etc okay so this path essentially so then what is the first do you have any questions on this what is this thing have you heard of a static pod what is the static part okay so that is surely a interesting question we don't know what is the static for then another thing is Manifest okay what is a manifest okay and then last thing is is saying that I'm creating a static pod manifest for the compon component is f we are basically saying that this is the component you want to use and it is basically creating a yaml file in this particular folder okay so then let's see what is there in that folder okay so what you have in that folder is actually so the the folder name is the Manifest okay so manifest directory is there and this is the file name which is a yaml and then we are create we are basically specifying a pod there in that so Cube ADM essentially created this file for us okay we didn't do thisum just did that for us okay and then it is saying that it is setting up uh so you can see that what is the containers that are being set up so this is the name uh Cube API server is the name of that pod then the containers uh if you go to so command is fine but go to image first so if you see the image it is running the cube API server image essentially that's part of it and then it is passing in like advertise address is passed in and some other things are passed in so whatever is required as part of it is passing in the port also matches for us so our guess is correct 6443 is what was missing and it is adding that and you can see the combination okay this is the advertise uh address and this is the port okay that means if you see the earlier failure at the very start when we did uh Cube cutle where is that okay now hopefully we should able to connect to this because that exactly what the cube API is trying to run on okay and then it has like lot of other configuration okay but essentially it's a pod pod definition of some kind okay but why is the Pod definition this is is this how we create the Pod we don't create the Pod like this right we do Cube cutle apply and then actually create the part inside the kubernetes okay so we basically talk to to the API server to create the Pod but API server itself does not exist right now okay we don't have API server and have you ever wondered that even if API server is not there and all of that is not there all the component of the kubernetes themselves runs at part you have seen that right Cube if you go to cube system and you say that give me the parts it will give you all the parts all the component of the parts but they are themselves running as as Parts but then there is no Cube API when you are setting up the cube API itself correct then how how do we create that thing it's like a loop okay so Cube API server is required to create a normal pod okay but yeah yeah yes so if you see the Manifest file is autogenerated by Cube ADM because we didn't create it Cube ADM is essentially saying if you see this line it is essentially saying that uh it is writing the static pod manifest for for cube ADM into this particular folder so it is the cube adum who is actually creating the Manifest file we did not create it okay we just use the command to basically uh set up the API server okay uh so what I was trying to to say is so normal P creation requires the API server because that's how you are communicating and asking for a pod but the API server itself is not there so you need a different mechanism that's where the static pod essentially comes in so if you see the documentation of kubernetes I hope that I have put it in here somewhere let me find if I can get link okay not this so if you see the kubernetes uh static part static part documentation some of the key things like important things is it is saying that static spots are managed by the tubulate demon okay so that is main definition of static part static part is something that does not require anything else apart from cubet the only thing that is required is cubet okay so understand that right now we just create Cube ADM just created a file it did not do anything else okay just creation of a f yamal will not create a pod for us so somebody has to look at that particular file and then create a corresponding pod out of it okay so that's what the static definition static part essentially is saying that anything that is is managed directly by the cubet outside of the uh the route of the kubernetes API and everything directly managed by the cuate is are the static parts so that's why it basically does not require API server and all of that okay so that means in our case uh going back okay what happened okay so this is what we have we had that we have that manifest F manifest directory okay SL Etc kubernetes SL manifest okay so what the cubet is doing cubet is essentially watching that file watching that folder okay it is looking at for any changes into the uh in that folder so any new files that are added or any changes into the existing files will be picked up by cuet and cuet will try to create a corresponding uh container for it so who is creating those files for us it is the cube ADM that is creating those files for us okay we are not creating those files manually Cube ADM is doing it we only said that I want an API server and then cued essentially Cube ADM essentially created a file for us putting that file in in the Manifest file cubet should essentially pick that up and start creating the container to create a container it need to talk to the docker okay so it will ask the docker to actually create a physical container okay so that is what is happening the static part is clear because that is how almost all the components will work henceforth okay cubet is owning the life cycle of thetic part so if it is failed it will recreate is same as any pod okay if your pod basically fails to be honest every pod is managed by CU cubet every pod your part including static pods okay and non-static parts everything is managed by cubet on your worker noes also you have cuetes but main thing main distinction between the normal parts that we create and the static Parts is cubet is the only component that is directly managing it nobody is telling cubet to manage those parts or nobody is nothing else is required to create those PS the only thing you need is the definition of the p and that's it and cubet in normal things the definition of the of the actual so if you create a part the part that we are trying to create engine X part we are trying to actually use the API server to basically tell it to the cuade to create the part eventually okay so there is a API server need in this case nothing else is required only cubet is required that's the main difference rest of that all the parts are actually managed by the cubet so on worker notes your cuet is actually ensuring that the parts are getting created yeah no the definition so is question is are the non static Parts also goes into the Manifest directory the answer is no the definition of the static F itself basically says that you you need to put in a yamal only for the static parts that become that makes them static part so you if you want your own static pod you can do that but not every pod essentially go becomes a static pod that's that's the difference we will see where the rest of the part goes in okay but right now just understand that I have created a yaml put it into that man manifest directory and it is directly managed by the cubet and that is why it is called a static part directly managed that's the important thing rest of the parts are also managed by cuet but not directly okay they are being cuet is being fed that information okay I need this okay I need that cuet doesn't know that there is a change required so when I do a cube cutle edit and I edit the definition of the Pod actually that definition is first going to the uh API server because Cub cube cutle is talking with the API server okay is that clear so if any change you do to your Own Parts is actually that information first goes to the API server because that's how the communication is happening okay but in this case the cubet is directly managing everything so there's no API nothing is required okay coming back so let's see what happens to I have I have one question set up what we set up uh am I audible we should have that qu right what else is there are nonstatic P also present in the same directory I'm not on M how do I listen to this if there is a question you have to write it down I think because I can't hear people who are on the CH on the call I already answer the last question which was the non-static part does it go into the Manifest directory the answer is no because whatever it goes into manifest directory are the static part that's the definition of it okay so coming back uh okay but we are still getting the same issue right we did the installation so what is the problem now how do I check I can't see the parts okay so if I can't see the parts how do I debug this information where is the failure because my failure is still the same so what how can I deug this you can do do verbos it will not tell you anything because yeah so you can do verbos and what it is saying is saying exactly the same thing and trying to connect to/ API and the connection is refused and yeah it is doing some I think it tried two times all of the times the connection is refused and then it gives up and says that are you really connecting to the right host or is the thing missing so how do I debug further what is a part who is actually creating a physical process that you all talk about as P it is this Docker right in our case it is Docker it might be any other CRI as well whatever is supported but it is Docker right so bare minimum we should be able to ask Docker what happened to my container okay so let's see so pod is not obviously not create so API server is not up for some reason so Cube cter can't help us okay that means then what we need is to check what happened to that container so let's see okay so if I do Docker PS it is saying that there is a container with a pause image okay and nothing else there is only one container okay so what do I do it is saying something about API server you can see that the name is basically API server IP and all of this Cube system so it is telling you it's doing something about it is something about API server okay but it is not API server in itself because see the image it is nothing to do with API server and when we created the part definition we saw that the image is specific for the cube API server so where is the AP server how do I look at it how do I look so there nothing running so what is happening where do I look at for the errors what else can I see in terms of so I can see anything that is killed or exited uh that is tried to create but then the process basically just crashed okay so you can see that now pause is fine but there should be is there anything more than that is yeah I can close the chat ah so you can see that there there are now two containers okay one is this one is this earlier there was only one okay now you have two of two entries and it is saying exited 1 minute but is it what we need it looks like it is what we need so the command is this what is the image here so it's not showing the image container ID then image name where is the image name this this one this is the command pause there should be something here something is missing I don't know whether is because of zoom level or okay but this is the because of the command at least we know that it is exited okay it's not able to run okay so what do I how do I figure out what is wrong here some response on the chat container logs correct so then I can basically look at the container logs and this is the container okay so looking at the container log for the API server I can uh so if you see now you need to okay one thing on uh to understand kuet is is that you should be able to read the go logs okay it's not same as Java so in this case these are like you need to understand which what is the log level of a message okay so in this case it is info the only I at the start shows you that it is info okay so all of these are infos basically so it's fine they're not really errors what we are looking for is error warning something like that okay so w is essentially the warnings okay so there is something interesting here what it is saying is is trying to do a grpc call so it's not a HTTP call is a grpc call and it's trying to basically connect to Local Host 1271 on this particular Port 2379 so API server is trying to connect to something which is basically on this particular Port 2379 error connecting and connection refused so we are the second time the connection refused so something is missing again so API server now depends on some other component so what is that running runs on there 2379 any ideas I should open up the so there is a question on the chat that says can we edit the existing manifest file and then push the changes to the existing one like pod names so the answer to that is yes any changes you do to the Pod manifest or the static pods will be picked up by cuet okay so the answer to that question is yes uh so what is missing now is the database required for your apis so even if you are saying that so you get request when you say Cube CLE so the answer is already given like hcd hcd is what is missing which is the database so if I do Cube cutle Get pods okay if I do this you already saw that so if I do this you can see that is actually trying to do API call and possibly it should I don't know whether it is doing the subsequent call or not most likely it is not doing that but it should do a SL API sln namespace SL default SL parts that's the that's the call it is going to do okay and then what what it is essentially saying is give me the list of parts okay but somebody need to store that list of parts okay where do I store store the information so that's the database so you need a database and API server is essentially complaining that the database is missing okay so and it is trying to connect to that 2379 Port where the hcd is expected but is not there okay and that's why the Pod keeps so it will come back up the Pod will come back up if you see it will actually come back the container will come back up will will be killed will come back up will be killed cuet is essentially trying to continuously make it run okay because uh that's the job of the cuet it has to ensure that the things are running but at the same time the API server is not able to connect so it is crashing so it the question is what is the CD Port configur so all of that is like a standard Port okay so we are using the standard Port the issue is not that the port is wrong the issue is the hcd itself is not there okay so HD is missing so database is missing you need API you also need the database to get that the state of the things okay so it CD is essentially missing so we can follow the same approach we can look at the okay so I will quickly just look at what I did here so till this is fine okay now what we will do is we will basically do the exact same thing we we we will ask the cube ADM to basically create give us one more uh static pod manifest and create one more part for us okay because H CD is what is missing and you can see that it will exactly do that if I go to adum so in it phase there is a hcd phase okay now you can see that hcd is not part of the at least from a cubet point of it's not part of the uh control plane it is not assumed to be part of the control plan the reason for that is though the kubernetes is actually using H CD H CD is not really part of the kubernetes H is a standard on project and it was there even before kubernetes was there okay so that's why hcd is separate it's like you you need hcd but it is not part of the kubernetes project itself okay it is not created for kubernetes okay and then in case of hcd we are going to the local because right now we are installing it on the local machine so that's where the local comes in so I will basically just copy this we'll do the usual minus config ADM config okay uh so this should give us the hcd essentially what happened I don't remember that dou so if you see uh almost similar things is be done what it is doing it is saying that creating static pod manifest in this particular directory and writing a static F manifest into into this thing okay so that means we should be able to see that there should be two of them now so there are now two yaml files in there one is hcd another is Cube ADM uh Cube API server okay so two components that are there that means we are expecting two separate parts and containers running in here okay so let's look at it so if I say Cube CLE get ORS minus minus so it is not yet up if it doesn't work again we will have to look at it but let's hope let's give few time to see whether anything comes up okay so this also that means till now the API server itself is not up okay let's see whether it comes up or we can look at the containers as well so either you can look at this or you can basically say sudo Docker P yes and see whether the container themselves so there is H CD container that's it and there is no API server yet so there are two parts containers but no API server API server is not yet up so it is doing that looping so it is crashing and then it after a while it takes cuade basically figures out okay uh something is missing it is expected to be there but it's not there then it will try to recreate that prod so there is a gap in in when it understands and looks at it and then do should come back now what is the question pause containers so pause containers okay so the way so if you see the Manifest okay there no pause container okay you are you never specify any pods containers or anything but that is how pods containers are created for every container that you create in kubernetes every yeah I'm coming back coming to it okay so even if it is your parts or the control plane Parts okay doesn't matter part coners will be created for you okay now why why do we need PA container pause containers are essentially created uh to give you so it is responsible to create some name spaces so the way Docker works is basically everything is still running on our machine okay the only thing Docker is doing is it is creating the name spaces in Linux and gives you an illusion that this is your machine though is actually single machine but gives you an illusion that okay you are able to Only See this much okay so there is a network name space there is like process name space there is CPU memory and all of that is there c groups are required to control how much you can access and all so visibility is one thing and control is another that visibility is I will if I say give tell me how many CPUs I have it will tell you okay this many CPUs you have okay but you physical Hardware has more than that okay so who will stop me from accessing rest of the CPU that's where the c groups comes in so basically stops you from you using more than what we are supposed to use okay so name spaces basically gives you visibility the the the pause containers are responsible for creating the the network and some other name spaces so that's why they are required okay so if you go inside a Docker or container and then you say psus EF give me all the processes it will actually only give you processes that are really inside the container it will not give you on the host machine though those processes processes are actually running on the host okay but it will still give you only those which are visible because you are essentially constrained in the in terms of visibility from the name spaces okay going back let's see what happened okay so we have this running now okay some so now because of CD and because of API server both of them are there now so that because of that Now API server is up okay because API server is up cube cutle is able to talk with the API server now when we ask give me the parts we are able to get the answer okay because we are getting the answer it is showing us the answers the only parts that exist right now in the systems are two okay one is the hcd IP hcd basically and another is the kubernetes API server another hint in terms of this knowing whether something is a static part or not see the naming static parts are always specific to a node that no cuet is managing them so that no cuate basically puts in the host as part of it so you can see that H cd/ IP and then the entire IP of that host is added okay so this is not actually this does not really exist uh or is not managed by the [Music] normal uh things basically you are managing it through the Manifest it is only the entries that are created okay in the in to be honest the entries are not created in h CD right we didn't say create a pod for me okay cuet already had the pods but then there is a separate entry created just for the static parts so that you know that okay there's something running It's Not Invisible okay you can see the documentation as well they says that this is a what it is mirror mirror Parts is what is created it's a entry in in hcd in the database to tell you that these parts also exist those they are static Parts they are not managed by the normal channels essentially okay so great so we have something running so should we try our own parts now name spaces so this is the only part that are running how do we read the entries in hcd server about the Pod list say example APS P so then we have to connect to hcd for that okay so I don't know whether so we did all of this uh I will talk about so the question is can we look at the hcd entries the answer is obviously yes it's a database you are able to connect to it and you can see all of all the details and everything uh I'm just checking whether so I I'm going to look at it I'm just looking at whether we want to do that right now or later okay maybe we can do that right now as well okay so let's let's try to do it so there is a hcdl uh command line basically to connect to hcd and you can look at it and do that but do we have any pod in our CD right now the normal pod so there's no point in looking at the Pod yet okay so let's come that's why it is later let's come back to it let's go with our own command for now that is okay right whoever asked it on the CH on the channel right now we don't don't have a pod uh in the H at least not the normal one okay so we'll go back and basically do what we are trying to do the first thing which is to set up a engine X is that okay so we are back to this this is the whole point we are trying to do this okay it might or might not give you interesting details but so now the the error is changed okay now the error is it is not saying that I'm not able to connect to API server API server is for sure exists now and it's up and running so now it is saying that forbidden pod engine X is forbidden okay why it is forbidden it is saying that error looking up this service account okay is saying that service account when it is trying to find the service account there is an error what is the error it is saying default SL default service account default not form okay so what it is saying is it is expecting that there is a default service account the name of that service account is default and it is expecting it in the default name space okay so let's see whether that is the case so if I say give me the a which is the service account and I am obviously connected there is only like Nam space is is default by default so I'm looking at the default name space and just saying it does not exist okay any other kubernetes uh when when you create any Nam space so default name space is by default created for you when you create any name space uh you you will end up with a default essay by default okay so it is there but why it is not there in our cluster it should be there right so cube cutle is we are asking that give me all the asss uh and it is saying there nothing there in the name space so what is missing now so we have the API server we have the H CD we have something something else is missing that is actually is required to create this service account what else is there so we can look at all the control plane components and then try to have a guess whether sorry scheder why scheder is required for creation of a service account as the name suggest scheder does what scheduling right so do we need any scheduling for service account so we don't possibly need connecting to Docker hub for downloading image required service account uh the so that is the question that is asked on the chat the answer is no it's not about the getting images from the service account I can get the image if you want so I can basically do run engine X and it will give me the engine X command so you can see that it is able to pull the image so image pulling is not a problem okay any part you create okay I should fill in that in okay any part is trying to run the engine X we don't need to run the engine any part we create okay uh in kubernetes it will by default associate cre that part with a service account if you do not provide a service account which we are not providing if you see the command we are not saying anything okay we are just saying that just create engine export for us okay and that restart never is essentially saying that I don't care about the deployment or anything else just create a pod don't create deployments okay a single pod nothing else so I don't need to manage that pod or anything like that so that's the only thing we are saying okay that means whatever is the Y generated out of this will be will have the essay it so we are not specifying any service account that means the default service account that is associated is the default service account okay so that's why it is complaining that the service account does not exist so now need we need to figure out which components is required to install the service account so going back to the control plan let's see what else is remaining so so we did the API server okay hcd is separate we did that there are only two of them at least missing in this list one is the scheduler okay does the scheder make sense so we are eliminating rather than figuring out what is the actual thing but that is also fine okay so scheder does not make sense because part scheduling we are not even able to create a p what are we scheduling okay so right now we are not able to create the p so that means let's try the controller manager and then we'll look at what controller manager is actually doing in all of that but let's first get get the progress a bit okay so then I will do the exact same thing that we did okay it is doing again the same thing it is essentially writing uh pod manifest static pod manifest uh and writing that here so you can if you look at the ls minus L what is this uh hcd quetes manifest if you look at the directory now you have three of them C CLE uh hcd uh Cube API server and Cube controller manager so controller manager is what we added just now let's see whether we are able to see that get parts okay so it is saying that control manager is created it is basically there but why did we deploy it uh so to basically see that somebody needs to create a service account let's see service account is created for us okay so you can see that now service account is actually created for us the moment we deployed the uh controller manager the service account is created for us okay so a bit of a progress now let me check what is the okay so a bit of controller manager so we just basically we don't know whether it will actually create a service manager or we just looked at okay these are the four five components that are that basically creates the kubernetes control plane and then we said that okay scheder does not make sense rest of the two already exist the only one that remains is the controll manager we deployed it and we are able to figure out so now we know that controller manager created the essay for us okay but what is a controller manager any guesses why it is called as controller manager what it is [Music] doing okay yes so almost everything in kubernetes is like components themselves are created on something called as a control loop it's like a infinite Loop okay and as he is saying that it looks at the desired State and it ensures that the current state has to match to the state so it will continuously try to do that tubulate is doing the same when we created a static part it will continuously look at that and it will ensure that the parts exist okay it will continue to do that continuously kuet is doing that API server does not do it API server is basically not a controller in in that sense it is just the apis hcd is a separate it's not a control plane so it is not it's a database controller manager in the name itself you can see that it is actually the controller so it does that thing it basically looks at the desire State and it looks at the current state and ensure that those are matches if there is a difference it will try to fix it and that's that's the loop okay but is not so there are a lot of things in kubernetes that requires this Loop okay and those are separate than each other but the deployment of all of them doing that is like a too too many things you need to deploy otherwise okay so controller manager is the like a they are separate controllers but they are all combined together they are all clubbed together as one deployment so they are all deploying as one thing so you can see that so I have opened up uh so I should tell you where am I opening it up so there is kubernetes documentation reference okay in the in that you can go to component tools okay that is basically the kubernetes components okay and then you can see that what what all flags all of them are like CLI tools okay what are flags that you can pass in so you can see at the server is there controller manager is there your proxy scheder and all of those are there okay so these are all the command line flags that you can pass in to a uh controller manager okay when you say controller manager minus minus help it will tell you that all of these flags you can pass in okay and that's how the Manifest you you saw the Manifest as well the advertised flag and all of that was actually passed in as commands to the CLI okay so the interesting bit here the reason I opened this up is what will it check for having the desired state with CU is looking at the yeah very good question where is the desired State stored so the question is you are talking about desired state but where is the desired state so desired state is essentially stored in the H CD okay so that's where you are saying that create a pod so it will create entry in the HD and then rest of the kubernetes components will ensure that that exist okay uh where is it ah so if you see the controllers okay so controller manager I said that is a group of a lot of controllers okay that so this flag actually allows you to control which controllers you want which want you want to disable enable and all of that you can see that you can say that lot of controllers a lot of controllers to enable star will enable all of them but you can see the individual components as well okay so important some so cronjob controller okay so this is basically when you say when you create a Cron job this is the controller that looking at the KOB okay then there must be a deployment controller so whenever you create a deployment this is the controller that looks at the changes in the deployment and then it takes cares of something okay then then there is a endpoint slash endpoint controller import slice then is anything else job controller is there so a lot of controllers are there there must be a service account controller as well this is the code token service account controller this is basically what created our service account okay this is what was missing and then we this this controller essentially looked at the name spaces and then created it for us coming back okay so the control manager is clear it is set of lot of controllers you can choose to disable enable whatever you want and it's a interesting thing to explore disable all of them and see what works what doesn't work then enable one of them then see what works what doesn't work and then you will understand what exactly all of these are doing we will briefly talk about it if time permits okay so let's look at the uh it CD itself so looking inside it CD you can look at it uh but it is tricky because the U everything that is stored inside is like it is using Proto encoding so it is all encoded uh I haven't done the decoding so I can't show you the exact yamal or something like that but I can still show you how to connect to it so one way of doing that is so I'm just going to use the H CD CTL which is the command line tool to connect to hcd okay so all it is doing in this is basically just installing getting the getting the tar getting the H CD files uh the installation it is basically giving me all of that oh how do I zoom okay so yeah I'm going inside so this is where everything so it will include the hcd server as well but we don't care about the server we only care about the like CTL so we just need CTL you can now do hcd help to check whether it is working so at least it is giving us the help so this is where so this is the thing I'm using essentially so Local Host not local local directory and the binary and then help will give me what is the help and how do I connect to it and all of that now like try to use it 2.2 our H CD okay so the protocol we are saying that I want to use the API 3 because that's what is being used by kubernetes then you have to pass in lot of things the main part here is the end point okay so end point is wrong here uh what is the end points okay I should do get parts first okay what should be the end point you can see that hcd command line tool and then you are passing in lots of flack to it okay the important bit for us is the advertised C URL so it is expecting a client connection on this particular address so that means I need to send my traffic on this address so instead of Local Host and that I will basically use so what the reason I'm showing you this is don't just don't just follow the um don't just follow the read me because I will not know your IPS okay so then you have to figure out what is the right IP if you are going to do this later okay so Endo then there are bunch of certificates I'm not going to go into that key certificates and all of that but it will it so the main thing we are doing is so all of these are flags okay the sh is a flag key is a flag the ca sht is a flag okay and the endpoint is flag I already talked about the endpoint the main thing here is the command what is the command the command is get okay so I'm trying to get something from hcd and this is the path where am I saying that give me from like basically like give me everything and then we are saying only give me the keys basically the paths so hcd everything is stored like a path okay so if I look at now uh look at the SD entries hopefully I uh so obviously H CD CTL is not on the class path because it's I haven't moved it here I have to run it like this and possibly I have to do sud sudo I think as well okay so now you can see that it is giving you a lot of entries okay one observation is everything is actually stored inside something called AS Slash registry okay and a lot of things exist right now okay so a lot of things are created for you who do you think created this which component control manager because we created it only for essay but it actually bunch of lot of things that are there so it is basically creating a lot of things so inside registry so if you go to let's say pods where is p tell me if you see the P I don't think it is maybe I should [Music] do I can buy part H so I can see Parts okay it is saying that there are three parts all of them are Cube system parts okay so it has created three entries for our part that's what is there in the HD you can also try and look into like you can remove Keys only and prefix and all of that and try to look inside what is there so I can directly use this particular object or path and say show me this but you will see just the garbage okay some some things are possible to read but not very good okay you can basically read it but you have to do decoding because it's all encoded using Proto I tried it but I was not able to do that okay there is a way because of time crunch I didn't do it okay but at CD you can access the hcd is there a question on the chat the IP of hcd pod yes so no so we are not using the IP of the HD port we are basically using the advertise address and if you see the advertise address is actually the Local Host this is the address that we connect to I think so yes so we are connecting on the current address not on the Pod of the so it is basically there is a port mapping which is which it is doing and then the port is available on the the host machine not on the not only inside the P or not only inside the container okay so moving on okay so let's try to create our P we didn't try to create the Pod yet okay let's try to do that the essay is in place so let's try to create our pod where is that command okay so now pod is actually created this is the first time you do not see any error okay so good now let's see what is happening with the Pod CLE get pods okay so there is a pod it is understood that there is a pod if you see the cube cutle uh we should be able to see the parts as well uh I should say okay you can see that there is a part entry now in HD as well okay okay so get parts but do you think anything will happen with this that's the question that I'm asking I'm asking the same question I am asking the same question that why it is pending logs will you be able to see a log of a pending part pending board means it does not even there no container you can see that we can check whether there is a container for this is there are a container for this so this is one entry which is Cube controller okay this is another entry which is the Pod pods pod this is another entry which is the cube API server this is another which is CD then this is pause this is pause so our container does not even exist yet so what do I look at in terms of logs it is pending means it is not even running it does not exist so why it is pending when you when you say A Part being pending in kubernetes what does that mean yeah so there there is a so on the chat they are saying that because it does not know where to place this pod so ites it goes to the pending State okay uh just looking at saying that the Pod is in pending state does not necessarily mean that it does not know where to go but in this case this is true okay so let's look at the describe part okay so you can see that there are two things that you should observe here one is in the node section there none that means this pod does not have any node assigned to it nobody is basically saying that okay so we said that I need a part that is all we said the actual running that part on a particular node is done by someone else okay inside kubernetes we don't know we don't say that run this on this particular uh node or something like that we just say I need a pod engine X you run it on any node that you want and then somebody will figure out where to put it in okay so that is what whoever figures out basically figures out okay this is the node where I should run this part okay and that's where the this basically will should be fill in another thing is there is not a single event happening on this SP nothing is happening on this SP basically just entry in it City nobody else is doing anything so what is missing soeder is missing right you are trying to create a part and the shed is not there so nobody is there to schedule it but see see the way kubernetes works missing scheder does not stop you from creating a part from a pod point of view creation of a pod point of view it is just entry in HD nothing else nothing more nothing less it is just saying that okay I need this part to be created that's it not doing anything else whether it will run work not work it's Sol later the the API server has done its work which basically created an entry in ND and then then someone will take care of it so scheder is missing so what shular we do sorry means what when you say it is scheduling a part means what what it is doing is not doing that scheder is on your so understand that kubernetes has multi- node cluster right now it's like single node but you will have workers so your pod might also work on container actually runs on a worker node okay your QBE Scher is running inside the control plane which is the master nodes okay so if Master nodes a scheder is on Master nodes it is not going to look at the container inside some another are you getting my point is not even responsible for creation of the cluster creation of the Pod so what it is exactly doing when it is saying scheduling what it is scheduling what it doing should doing are you sure sched calls to KET is the so if I don't have any in this case I have cubet but let's say right now my master node generally when you create a pod it will never run on a master node okay this is a master node and it is trying to run uh it will even eventually run it so the reason we are able to do that is we are not marked this node as Master yet purposefully that's why it is running but otherwise so basically you haven't added the tent to the node once that tent is there it will not run the parts there unless you tolerate the tent okay nobody will tolerate the tent so you don't want your part running inside a master node so Master node has t so that's why it does not run that okay the thing is okay memory check the requirement of Parts it labels and accordingly place the pod on the Node which meet the Pod requirement that is what so yes all of that is correct memory requirement the availability of the of of the resources and everything it will look at all of that but what what do you mean by play the pod on the Node when you say it will place will place the pod on the Node what do you mean by that all it is going to do is to update the state inside the hcd with the node that it thinks where the part should run okay so the thing that I showed you so see kubernetes is highly highly decoupled and completely even driven every component is just changing the state and every other component just listening to the state that's all they are doing so scheduler will only say that okay this part where it should run it will look at all the allocation but it will not look at the node them it will look at the desire State and say okay this is what it should run on this node it will try to figure out so whatever you are saying in the chat uh it will look at the memory and requirement and labels and Tents and all kinds of other things uh and it will choose a one node but what do you mean by choosing it will actually make a entry update the Pod entry and say that okay now this part it will populate the node field and say that it should run on this okay so that's what it does the moment it does that cubet essentially keeps on looking at the what is my state what is state of my node it keeps on looking at inside the API server so all of these are communicating through API server so they will call API server and API server will tell you okay this is the supposed to be your state and then it looks at okay this part is should be there but it is not there on my node so I will create it so pu that's why I said at the start cuet is the one that is actually talking with the docker nobody else even if kubernetes is about container orchestration it does not create containers or manage containers so coming back to so we need scheduler okay so let's create a scheduler sudo sched minus V 9-- how do I get rid of this yeah okay oh okay okay so it will do exactly the same thing now you know that it basically puts in a manifest and you will see the thing but now you are getting where the why the need of every component is right because we are looking at the issue and then we are creating the component we are not creating everything up front so then we then you now you know that which component actually trying to do what okay so but installation is standard is same one time you will not know the static part but next time like four times you have done the same thing so all of these are created as static Parts okay so now we should be able to say get pods minus minus all name spaces okay so now it is saying that it CD is there API server there control manager is there cube scheder is there okay but our pod is still pending what do I do so let's look at two things we looked at two things one we said that it should basically put in the name uh the Pod the it should populate the node for the Pod okay or else it should tell us that something is wrong okay with that pod I'm not able to run the Pod and all of that okay so now this is another uh good way to figure out which component is doing what whatever I'm now showing you so if I say describe pod inine X now see that again the node is still empty okay obviously otherwise it would have run it by now so when it is not it is in pending that means we know that it is not able to f figure out which node to run it on and we'll see why okay but the key thing here is that in the events we have something so at least someone is looking at the part definition now someone is trying to do something about the part that you created inside H CD okay so which component so you can see that it is a warning it is saying that why it is failed it is saying that failed scheduling okay the main thing here is you should see from okay from which component so generally a controller will come in here and even if you have like custom components and all of that it will you will still see the similar thing they will also raise events and then you will know okay who is actually say telling me this information okay so even if I looking at the part and let's say I don't know about whether scheder is responsible for doing all of this from this I will know that okay there is something called as a default as scheder default scheduler which is basically telling me which is raise this event what was what is the event failed scheduling and it is saying that no nodes are available okay zero nodes out of one node are available has untolerated tent okay which is basically not ready so it is saying that you there is no node uh which exist or there zero nodes exist which has this tent untolerated I cannot tolerate so my part cannot tolerate this particular tent okay so we basically in in case of kubernetes sched there is a con called tent and Toleration okay so in that case what you can say is okay you have a tent unless you tolerate that tent scheder will not put you there basically you are saying that I'm okay to have this particular constraint put me there okay that's how Master Parts also works Master nod has a tent and only the kubernetes component parts will have the Toleration so they will go and sit there okay but none of our parts goes there because we don't tolerate that tent so scheder will not schedu us there okay so that is what it is saying but in this case the tent it is talking about is not ready that means our node has this tent our node right now there is no node that doesn't have this T only no one node we have one node cluster basically right now that means our node has this tent right now that is our node is not ready essentially and that is what it is saying and complaining that that's why I cannot choose this particular basically there is no there is no node that I can choose okay you can confirm this if you want to cube cutle get nodes the node is not ready you can see that here okay you can also see if you want to see the C CLE describe node what is the not this is the okay you can see that there is possibly a tent there should be a tent somewhere okay the tent is given that not ready that is the tent on our uh node and it is saying no schedule that means do not schedule on this note okay so the effect of the tent is basically no schle that's why it is not scheding and because it is not it's not assigning any node to it it will never come it will never run so what do we do why it is not read so who makes who who who basically which component of kubernetes basically says that the node is ready or not ready who reports that so obviously you can see that when I say Cube cutle Get nodes okay our so we are talking with the API server API server responding back as the node status saying that node is not ready that means CD knows that the node is not ready but then who is so if we make it ready or who is basically telling at CD on and the API server that the node is not ready which component essentially is reports the health of the node can controller do that is controller a node level thing so what about the worker nodes worker node your controller does not exist on worker node CU proxy is not responsible for the health of the node which component exist in every node exist in every node every worker node or even the control plane node we started with whatuh ap Ser was not there right we started with something what all things were there so Docker was there but Docker is not a kubernetes component what was the other component that there Cub adum is not kuet is component again what are we started with so we said that all of this exist but only this things are related to H so Cub ADM is not the answer in the chat you are saying cuet which is what is basically uh there so cubet is running as a service okay and cubet if you see cubet exists in the control plane machines like Master nodes as well as the worker nodes okay so because it exists on the worker no and cuet is the one that is actually managing everything talks with the docker and all of that cubet if cubet is up then it knows that and cubet basically continues to tell it to the API server that I'm am up I'm basically I'm healthy not healthy whatever it is it will basically continue to tell that and then that will go into the hcd and that's how everyone else knows that is this node ready or not so let's look at the cuate why cubet is complaining that our node is not ready okay cubet okay who has tented the nde for no schedule okay so good question what do you think who has tented the node who is managing the node very good question actually who is managing the node huh so there has to be a node controller or something here that basically will do that for us can you can you look at it okay so this is probably the controller that is essentially doing that okay uh and managing those things but it might also be the reporting from the cubet so I'm not exactly sure which API cubet is going to is is basically talking but depending on what cubet is doing it is it is basically say if it says I am not ready whether the tent itself is added as part of it or tent is added later by the controller I'm not not 100% sure because but now see the understanding of the architecture now you even you could basically thought through and see that okay probably there is a controller that does this because for every type of resources you have a corresponding controller okay so somebody is making changes into your H CD okay so that is what the whole Crux of this stock is that you should be able to inally figure out okay where should I look at or which component actually doing this okay you will not know everything obviously but then at least you know where what is the direction to go to okay so it is not ready what do I look at how is the cuet running cuet is running here right on this note okay what is the question does that mean that when we join a new node by default the node controller will tend to no schedule till it is basically not ready then it will if if the cubet essentially so there are two things when when cuet comes in and when you add a new node okay cubet possibly reports only after the node is joined or it might join as a not not ready so it will basically say I exist but I am not ready and then you might have a few Span in which it will say that node is not ready and then once it joins and healthy and everything is there then it will declare that okay I'm ready and then parts will get scheduled on on that note but till that point it it might happen that it is not ready okay so let's move on uh so okay what what do I look at cubet is running as what by the way cuet is the one that is doing all of this magic till now all the static parts are managed by cubet so how are we running the cubet so it is a cube okay so it is basically running it like this and then how do I look at the uh logs you blade sorry okay so if you see the logs maybe I should do no pager or something no how do I WRA it okay so what it is saying is it is saying that container runtime network not ready okay network not ready network not ready Network plugin not ready that is the error and that's why it is not marking the marking the the node as ready and because node is not ready nothing is getting scheduled on that node and then pod Remains the pending state so how do I make it how do I make the network plugin ready proxy will the grxy so in case of kubernetes what is the network plug-in when you set up a cluster you you choose something like a network plugin there's a cni right in case of kubernetes okay container network interface cni and there are bunch of them so if you see this is a component that is kubernetes is using but is not maintaining itself it is basically lived out the part to someone else so you have to fit in that component uh where is the list of it networking not policies should give me the list of components okay but one of them is the Calico essentially so you have others as well Calico is there helium is there V we used to use but it's not there anymore and there are others as but Calico is quite famous in that sense that means that is what it is complaining essentially it is saying that something is missing okay so that mean we need to go and install the network plugin okay which is the Calico in this case so in so let's go and try to install this okay so I have like quick start guide we are not going into detail of how the calicor and everything works so the only thing it is saying that so it is saying that do a c cub adum in it and do all of this and that if you do cub adum in it it will do everything so we are not obviously doing that okay and we don't need it because half of the things is already done on what the in it is doing okay let's look at the installation of the Calico itself it is saying that first create the Manifest required for something called as uh Tira Calico operator okay and then it is saying that actually create the necessary custom resources that is required so this is basically creating some custom resources and then it is going to create a controller create the is like a operator essentially creates the CR crds there is a operator that operator will manage the crds and then if you create a crd it will actually create something uh important for you so one of the famous operator is like Prometheus operator so instead of saying create a Prometheus pod you say create Prometheus Prometheus itself is a type like pod create Prometheus and then PR but kubernetes doesn't have Prometheus so you have to basically create a custom definition that now I am creating something that is named Prometheus and then once you create Prometheus the Prometheus operator so you'll need operator now you need a controller again when you create a part you'll need a controller when you create a deployment you will need a controller because the controller is doing the actual work so in case of crd also you'll need an operator which is the controller controller will look at okay I created a one is the definition another is actual creation so pod is definition but creation of the Pod itself so similarly if I create a definition of Prometheus I can create actual Prometheus requirement I will create a Prometheus and then the controller will look at okay Prometheus is created I need to do something and it will actually do the deployment of Prometheus similarly in this case Calico it is actually using a uh tigera Calico operator okay and then that is basically creating uh the custom resources for us so we can look at what it is it it did okay you can see that it is is creating bunch of custom resources okay a lot of them and then uh these are the types BG configuration BG filter BG Vibes you don't need to go into lot of details of this but this is what it is creating so all of these are custom is not something that kubernetes understands but now because you added a custom resource now you can use the cube cutle and the kubernetes API to actually ask for this you can say Cube Cube cutle give me this Cube cut give me Prometheus perties will be able to give you the Prometheus suddenly the API is now expanded for you okay so all of this is possible uh most of these are custom resources you can see that here then there is a service account created which is fine then there is a cluster role created which is required by the operator then all of this so this is what it is Crea okay uh deployment there is a deployment created into the so we can look at what it is created get pods I'm only going to look at the pods so it is saying that something called uh tagra operator is running but it is not running really it is giving you an error and uh basically it's not up okay but it has created different name space which is this and it's it's it's failing essentially okay should we look at why it is failing but right now if you see we have only created the CL custom definitions and the controller we did not create the actual instance of the custom definition we just said that this is what is the Pod but we haven't created a pod similarly we just saying this is what it is but we haven't created the it yet okay that is the second part but this itself is the controller itself is not running so let's look at why it is not running how do I look at it okay so what it is saying is it is saying that it is not running because it is trying to connect to this this IP this port okay with SL API so that should give you an hint and it is saying IO time off it is not able to reach to something it's Ed out but what is this IP yeah but trying to reach to what but API server IP are you sure this is an API server IP but my IP is something else right host name minus I my IP is 13927 this IP is not my IP so where is it going most likely it is going to API server because that's what it is saying SL API so which is this IP where is this come from and it is timing out okay and if you see where is the cube cutle itself is connecting to talk with the API server you can basically see that as well so get parts and you can say minus V 9 or 4 I don't know whether four will also no four will not be it should tell you which IP it is connecting to it is connecting to 10 0 13927 which is exact exactly what my IP is so this IP that it is trying to connect to though it looks like it is going to the API and now you can see the entire thing API slv1 namespaces default slps so it is asking for pods okay in this case it looks like it is going to the API server but this IP is something weird is this IP coming from so what all things have IPS in kubernetes where do we assign IPS in kubernetes PS and and speak up I can't hear you yl yeah Parts is one is there anything else Services right the second part is the services okay we can look at the parts okay let's see let's say if I do parts and I say minus o wide hopefully all name spaces okay you can see that there is not a single part that will match this IP so what is the IP 9601 10 9601 none of these are 1096 01 okay so yeah so question I'm the one asking the question so question is what is the uh the operator is trying to connect to so if you see the logs of the operator it says that it is trying to connect to this IP okay and this is the path it is trying to connect to/ API so hint is basically it is the API server it is trying to connect to but where where is this IP come from okay that is the question okay so we looked at where how the how the cube cutle is connecting to API server the cube cutle is using this IP which is my local IP this is the nod IP and this IP is 96 which is different than this one okay that means it is not connecting trying to connect to this IP so what it is trying to connect to we are also looked at the Pod IPS none of the part IPS matches the uh the IP it is trying to connect so the other thing that has IPS is the services okay so there is a service see the IP of this 10961 so it is actually trying to use this service to connect to the IP of the kubernetes so let's see whether it is actually doing that so if I say Cube CLE disc cice service good question service this Services controller created it okay so in this case uh you can see that the service IP is where is it this is the IP then you can also see that there is a end point that it is trying to connect to which is 103927 which is exactly my IP is and see the port as well so this thing exactly matches to what the CU cube cutter is trying to connect to but it is actually sending traffic to this not to this so something is missing that is not routing the traffic that is sent to the uh the service IP it is not sending it to the end point it is not sending the service is not sending it to the parts so what is that thing is missing who is responsible for creating the path from the basically the services connect the services to the parts which component is missing sorry proy the only thing that is missing is cu proxy now yes so Cube proxy is a component that is actually responsible for making sure your services are working the networking so basically does the IP table updates and it ensures that the routing between the service IP and the Pod IPS are intact but there is no Cube proxy yet so there is no path created from this IP to that IP so it doesn't exist okay in case of if you see Cube ADM Cube ADM does not consider this to be a control plan so there is no CU proxy here and actually if you see the add-ons you will find the CU proxy in the add-on section okay I don't know why it is done that but yeah okay so that means I can now do the same thing that we have done a number of times config you config so proxy is what is missing it will do exactly the same thing U put in the bunch of things there and now we should be able to see what is happening with that W okay now this gu is in the crash back Loop of most likely we have to kill this part let's scale it because then we will uh we will see the update much better much earlier delete pod ah minus n it's already running so now it is able to connect to the API server because it is able to connect to the API server the error is gone okay but we only did half of the things all we created for the Calico implementation all we did is created a bunch of definitions custom definitions and a controller that understands how to handle those custom definition but we did not created any instance of the definition okay so that's where the other part comes in if you see the other yamal if I apply this okay now it will actually create the real thing so what it is saying is creating installation operator so this is the type essentially the entire thing but it is creating installation and API server okay so I should be able to say get installation installation okay and I should also be able to say get API server okay great but the uh just creating thems are not enough the operator should pick that up and do something about it that means we should see lot many lot many parts now okay so the installation and ultimately everything is a part in kubernetes okay or entry in CD or some networking things these are the only thing that exist so ultimately everything that need to run is a part okay so even if the controller and all of that is done ultimately there will be a corresponding part that will be created the only thing is you are not directly asking to create a part you are basically going through hoops and say that okay I'm creating an installation for Calico that in turns goes to the the operator then operator says okay for this installation I have to create this part or this deployment or this service or whatever which in turn will do ultimately it will be a part okay so that means it created this tons of Parts you can see four parts for calipo uh the engine XP now can you see the engine X so the engine X is now running okay that means our node is now ready okay so ultimate whatever whatever we were trying to do basically get our part running while also understanding how all the components works with each other is done the only thing we did is to schedule one part so in a way we didn't do much but we basically yeah okay there is a question what was the cause of this crashback Loop error how to eliminate this in the case of for deployment this will be impact on the prad environment yeah so in case of so cash back Loop basically that can happen uh for any part for any other different reasons the mechanism of the crash actual error might be different in different uh scenarios but in case of uh like any like production system and all there is a possibility that crash look can happen but if you are using deployments and all all of that because deployments and you should also use liveliness checks the da checks and everything as long as your parts are not really ready it will not uh the deployments essentially will not delete the older Parts okay because it will not delete the older part your service is not down your new parts are failing but the older parts are still working okay so it is not necessarily a issue you have to fix it obviously but there is no downtime going on in production if this happens as long as you make sure that kubernetes can figure out is your part ready and live on ready basically liveliness and Readiness if you if you configure that then your rolling will be basically not go through it will fail and it will not touch the older Parts the newer parts will continue to fail because a loop it will continue to fail but the newer parts are still running and they will still get the traffic and the service is still up okay uh I don't think there is anything else that's all is there H by the way that magic C is from here so I just use it the reason being the reason because I use just this is then then I don't need to basically go and change the yams here I can just copy paste and it is saying here so there is no space there is no special treatment to that number the only thing I avoided is that uh that I don't need a replacement uh where is it ah so all I avoided is this that if you read the content make sure the settings are correct to your environment for example you may need to change the default pod seeder which matches the Pod seeder so I don't want to do that change and that's why I just just use the same value in my config Cube config uh the cube ADM config so that I can just copy paste and then just work with it and it is already there in the initial setup so you don't need to make any changes if you choose to change the Ceder make sure you have corresponding change in the yaml that you copy from this so instead of basically then you have to edit the yaml so download the AML edit and then apply rather than just applying from the from the internet location okay any other question do we want to quickly talk about how the Pod actually gets scheduled when you create a deployment very quickly and I think you should fill me up so if I created a deployment can you take couple of minutes to again uh to explain again about the Cal operator proxy and then the engineer is getting scheduled okay yeah so if you if I go back okay A bit so I am explaining the Calico failure if it was not clear so if you see the logs that we checked at the Calo so this is where we looked at the logs for the uh the Tira operator uh and in this it was saying that it is failing to talk to this particular IP okay then we said that what what is this IP and general question is which component has have IPS in kubernetes there are two components that have IPS one is the parts another is the services okay so that means when I looking at the Pod IPS I can look at all the Pod IPS so this is how we looked at the part get the parts and so wi is basically otherwise you will not see the IP so you have to do wide and it will tell you the IPS of all the parts none of these parts IPS matches the IP that we are looking at so trra is trying to connect to an IP that is not actually a pod IP so it has to be a service IP so if you look at this service there is a service called kubernetes and which has exactly that IP 1096 something something okay this is what the T the T operator is trying to connect to now uh if you do a describe of this particular service you will see that the IP exist it also understand where to send the traffic to but somehow it is not able to send the traffic to so what is missing that missing part was the CU proxy so Cube proxy makees sure that the connection between the services it basically does the IP IP table manipulation does the routing manipulation so where how how should the traffic should go through in in Linux so for this IP it should route it to this IPS that is what we want so the services the the traffic that is coming to the service IP should get routed to the Pod IPS okay and that routing is missing and that is what is done by the C proxy so when we installed C proxy so this is why we installing the Q proxy so when we install Q proxy uh the the connection between the the service IP and the Pod IP is done because that is done then uh then when we basically looked at the Tira again uh that operator part it was crashing so I just deleted it so that we look at it faster and then after that it came back up okay so it is basically uh running that so that is not visible here because I did a watch here but that was running at that point so because we fixed the we installed the C proxy their link was filled up and then the operator was healthy and once the operator was healthy then we created the Calico the actual Calico parts to be created because we just created a definition and the operator but actual instances of the definition did not create that is where we created it so it created two things one is installation and another is API server we looked at installation API server as well because those are the crds and then we looked at the all the parts okay so this because we created those custom resources Calico the operator essentially created Calico pods and the because we created Calico Parts the network issue gone that made the the kubernetes node healthy and because the kubernetes node became healthy our part get scheduled on it so all of those components we are already running it's all Loop kubernetes is all every component is a loop so once you make one change everyone knows and it's all events they are not talking with each other they are just looking at the desire State and trying to do their work okay is is that clear so I if I go into Cube proxy and get into H CD host can I see the cont I not sure whether it is in exd host or not it does the IP table entries there is a way to look at it I forgot the command but it is possible to look at the entries of the services and so you can go low level and see the IP table entries that is possible it won't be in I don't think it will be in XD host but it is possible if if if if I find that I I I probably have that information if I find that I will put it in the repo okay yeah that's all I had I don't know whether we want to talk about that deployment thing but all else I can just inally talk about it but this was the main Crux of the whole components basically one part forces us to move through the entire kubernetes system to make it run and we went through everything so if there are any questions yeah thank you about that any other questions otherwise sorry IDE okay so uh so okay yeah so there is something called as kubernetes the hard way okay if you if you know that which extremely low level okay the original thing is so I wanted to do something similar but I don't want to be that low level because something is you need to know where you want to stop in terms of where to focus okay you cannot go and in everywhere you don't want to go into depth and then you lose the ability to understand the high level I wanted it to be high level but I wanted to also basically see that okay this is how it works at the very core so that then you can understand why it is working and then I said that okay why can't we use like we are not using Q ADM we using cops or eks or something like that to set up but they are very high level so I basically said that okay probably Cube ADM is a best tool that low level enough to show the actual thing but not that to low level that there is no tool basically the kubernetes hardware does not use any tool it will do everything manual okay so manifest creation manual actual deployment of Cu cuate and everything including the certificates everything is manual you can go and follow that as well that will give you immense understanding of kuet but this level is also fine okay so you need to choose whether to go that deep or not but that's where the idea came from then we did it internally once for Learning and then we said okay it's a good idea to open it up so that's what we are doing any other question applic that has about 2,000 daily entries on to my server let say so what kind of configuration do I have to do for my so Lear part if I have a react application next I have my cluster set up what should be the steps that I should foll for containerization okay I can answer that but that's not really part of this stock uh but so you can see that not if it is not containerized how will you run it if you know that then containerization is much easier what so there are two things here one is do you know what resources you need to run your appliation that is one so how much is the need of your application that is one and then actually doing conization of it is another okay and then there are other things like stateful is it stateful or not in this case it's not but other things are also there and then that so moving it to there are some challenges I'm not going to go into that but first thing you need to understand is what is the footprint of my application what is the need of it how much CPU memory and things and things it needs if I know that then I can translate it into the the container world and then basically say okay this is what it is it won't be 100% oneon-one but you need to know your needs basically to configure that on the definition and generally create your applications that does not know anything about kubernetes or cloud or keep them agnostic so that way that will allow you to move them to anywhere and if there is some dependency on kubernetes or uh like cloudd and all keep those component minimum or outside so that way you can do so then you don't need to know anything at the kubernetes level okay I think that that's all I have thanks all for