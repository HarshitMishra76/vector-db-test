uh so welcome everyone to the D talk on internal of database and I'm Lo uh so here I'm here to talk about some of the internals of databases fundamentals of it and trying to provide some use case on top of it which we have built our own database for that um I might jump here and there between slides I might go fast I might go slow if you find something which you want me to correct or stop please feel free to do that uh I'm happy with it so let's get to the next slide so yeah this is my doodle and I already said my name I'm I'm working as a solution consultant at sah um so I have around like four years of experience all of it in sah um so it's been good for me I'm I'm predominantly a backend developer um um I worked a lot on dis systems I love working with the high scale systems that's what interests me is a lot and i a r enthusias like I love developing application rust it really gives me a easier way to solve uh High scale systems you don't have to worry too much about the data uh data there because it gives you those things out of the box uh and yeah personal some bit about me I'm an anime lover you will see lot more anime things in my slide and unfortunately I am a RCV fan and this Ison we couldn't do much better hopefully next time we will do much I know I'm in Chennai this is a hard place to be as a ACV fan but fine I have spent around like 10 years surviving school college and the work inment being RCV fan so I I think I can do it yeah cool uh so moving to the next thing so I think the first question that you would have all have in your mind why do we need to know the internals oft database uh I can simply write SQL queries at works and if it doesn't work I can put it in stack workflow they will give me some other answers you can like play with and get get it working why do you really have to know that anyone I think again disclaimer this will have a lot of question this session uh you have to be lot more local uh so please feel free to share your opinion it's completely fine any answers why do we really need to anyone sorry okay good any other answers any other perceptions yeah yeah if yeah there are a lot more to it uh okay it is how can I stop this okay this will go out right the yes I just have to wait for it is or I have to pull it down no the top cool I think it is hidden here but at least I will give you the statement uh have you heard of this statement mechanical sympathy anyone I think some of my colleagues will know about that term we use it a lot in our uh but uh cool thanks that's much better uh still okay I will close this gu good um okay have you heard of this term mechanical sympathy anyone no yeah I know I know we know that but yeah I got introduced to this themm uh recently but I was doing it even before it's a very interesting uh analogy that someone has come up with uh why I okay now it's working cool so there is a codee made by a he driver named Jackie stewards uh he said you don't need to be a engineer to be a racing driver you just need to have a mechanical sympathy what does that mean like what is that mechanical sympathy so we all drive cars right at least bikes cars something that you know how to drive so you would have come up with some assumption saying that like I have to do this to get this done we all have been doing engine brakes right it is not it is good to do engine breaks in a slopy thing it is it's like when we take a overtake we do reduce our gears if the car is not giving us the throttle right why because like inside the gear switch happens and it will not give you the throttle at that point so you need to come back which will give you a better throttle so understanding those things is what he said make him a better driver uh because he knows how the vehicle works so he can use it to the optimal but how does it connected to the database I okay I was clicking at the thing okay how does it connected to database same thing like someone said right I want to do it efficiently I want to optimize it to the code I don't want uh mere optimal thing right I want to do it in a much better way and databases are the core of lot of our applications now most of our applications are dependent on databases that's what the major bottleneck for most of the high scale systems right so how does it related to database so when you know the internals of database you can use the pick the right database for your use case right you cannot use the same database for every use case one could be a analytical use case where you can use you cannot use a transaction data sore for that similarly one could be a what can one could be a different thing like uh I it can be unstructured data you cannot use a structur data relational table for such use cases so you need to understand how it does actually works behind the scene where to actually put the index where not to put an index which index is optimal for this use case whereas which index is optimized for other use case well you know the internals of it it makes you do a better job in what you do so that's what Jackie also said like it makes it him better driver if he knows the understanding if he has the understanding of how the car Works behind the SC similarly it it makes us a better developer if we know the of database and yeah disclaimer uh database knowledge is subjected to Market Risk please read all database related document thoroughly uh because each database has it own implementation of whatever things they want to talk about because they have taken different tradeoff for their use case which you need to understand when to use what they have done it for a reason we need to understand when to use what database uh again another disclaimer I won't be talking about in memory databases I will be specifically talking about disas databases and I will be talking about single instance databases that means uh I will not cover anything related to distributor systems or I will not cover anything related to implementing a inmemory database like f is and whatnot and the next thing so for the sake of Simplicity let's assume that we just visualize one table in the database okay uh basically database replicat the same thing for multiple tables so you can visualize that how a particular table works then the same thing will be replicated for the other tables also uh any questions so far am I going too fuse too slow optimal right cool uh cool the first anime reference let's go uh so what database file organization so why do we really need a database we can simply write it to a file system right I just need a persistence I just need a way to write to something and read it from something that will be persistent why do we really need database that question is still valid you can still write to file system and you can retrieve it you can do all those things basically database is just a bunch of files organized in a specific format uh it doesn't do anything that you don't do in a basic things right in our basic College we would have like simply open the file read from something right write it back to word that's basically what database do but they do it in a way it is highly optimized for storage access and update uh so we will talk about that later how do how do they efficiently do it how do they make sure that access is much more optimal for a particular use case which they have picked up to solve for and uh how do they make sure that they won't use unnecessary storage also for writing a particular record uh so most of the databases if you see they will have separate files for each table you can think of it like one file for each table uh except one uh use case except one database which is sqlite how many of you have used it what anyone have thought of it why as my sqlite is different from other there is a term for it they are called embeded databases they usually sit with apps they usually sit with uh websites they usually try to sit with the edge devices so they don't need a very high they are not they not supposed they are not meant for handling High scale use cases they are meant for handling a very specific small use case that's why they thought like I don't even need multiple files for it I just need one file which I do optimally that is the use Cas that they picked and solved it properly but most of the use case that I am thinking of talking right and I have used are on high scale so most of them actually have uh single files for each of the table and also it has separate data data files and index files so what is the data files and data file and index file we will look into it little data but why do we really need to separate these two things I can even store both of them together right for a single table I can have a same data file same data in a same index and data in a same file why do we need to have two different any answers okay okay anything else exactly so you won't access every data right you have like maybe thousands millions of Records in your system you won't be accessing everything so let's assume that uh you index is simply like a registry in a hotel so I just need to know where my friend is staying in this hotel I go to the registry I look at it and then I say he's staying at room number 31 and that data can be casted and lot of people can come and then ask for the data if it is casted in O and it is smaller and if I store it separately then it solves a lot of problem for me but if I store it along with the data file then I have to read the whole data and then I have to leave out all the unnecessary things and then I have to F the actual data so that's a reason why database systems prefer to keep index files separately from data files uh moving to the next thing which is a data file uh I already said data files means it stores the data records which is basically your uh insert operation with the data saying that this is my name uh full name uh whatever right whatever table columns and then the rows that you have that data is sitting in a data file and there are okay okay and there are two major classifications of data files one is index organized and another one is Heap organized what are they why are they different uh so there is a reason there is a specific tradeoff that they have picked up and they have solved it one is index organized what does that mean index organized means they store the data along with the index you might think like hey you said something here you said like data files and index files stores separately because for the faster access blah blah blah but you said like here it stores together yeah that's true they Sol a different use case again the use case that they have solved here is they want to have a faster range quins if the data is getting stored together along with the index and if if you store the data along with the index I don't need to do a separate dis read if you look at this one right the heat poiz I have to go through this part I will go from here to here to here based on the query based on the particular key whever it exist on the de ro and from there I have to do another dis read to here that means at the two discrete form but if you look at this one I just have to do the same discrete I don't have to go to a different place to look for the data the data is already in the index but what is the problem with that can anyone explain because you need more such Pages because if I can have hundreds 100 pointers in the single page here but I cannot have 100 pointers here because data is also reing I will only have 10 pointers that means I will have more such deep notes then the disk the index seek will take more time but the disk seek will be less so that's a tradeoff that they have solved one more thing the sequential reads here all the leaf notes would be connected so if I have to do a range query I I would say like give me all the records between ID 100 and ID 200 all of them will be sitting together from this side to this side so I can just go to a particular this pointer and then read everything from the pointer say like give me all the things and we all know that like uh dis works with the dis has a pointer or the what do they call needle which actually goes through the sector or the segment and and we all know that the sequential rights and sequential reads are much faster than the random RS and random reads so that is the thing that these guys have optimized for this guy is optimized for a better ins insertion so if you look at the insertion here right this one is written in a sequential manner so if I have to do a random right which sits in the middle of this thing I have to move everything in the middle to the left most or the right most to fit this particular guy then I have to do a lot of discribes whereas this guy will just have to do a single pointer insertion at any point you don't have to worry about where the data file is sitting you just have to worry about where I have to point this is really good for faster insertion but this is not good for range queries and better reads so this is the two different way of optimizing what use case that you have so every time you pick a use case you need to understand what my reads and wrs should be how I want my reads and rights to be whether I want a better latency for read whether I want a better latency for wrs those are two major things that you have to consider whenever you pick up any databases uh let's move to the next bit it will be will also be on the same line of this one uh the index phas I already talked about so if you go to the [Music] one that's a nice question so actually I think I thought of talking about it maybe in the later but I will talk about it now so how many of you have used posters how many of you know which one the posters do keep only and how many of you have used MySQL most of you and MySQL do the first thing so we all know that like okay MySQL post both of them SQL databases what is the big thing I will just use one over the other whichever use case I get I will just use my SQL because I know my SQL better that's what I have done for post I learned my lessons uh I I have I'm a very big fan of post and I have used post for one of the usec which I will talk about later where I have burned my own hands and found that post is not a good fit for that particular use case uh so I thanks for the question so in index organized tables everything is brought up to the memory and from the memory only the are which one for either case index file it Mak the data files are the dis so it's Mak for the yeah whereas in index organized table doesn't it bring it from the dis to the memory when it is loading the index itself yeah it will all this internal nodes will be cached and even if you do this right if you go through the disk OS way of doing it every discreate will go to the OS cach and then only from the kernel cach it will go to the user cach and from there it will come to your whatever application that you're using so yes it will go through the in memory flow cool any other questions on this so far the data files cool uh the next one the index FES I already said that why do we need an index it is for faster uh select and it also gives you a easier way to locate a record in the data and just like I said like I can't go to every room and knock and then see whether it's my friend or not okay that will be very inefficient right but if I know the registry is that and if I can look at the regist and regist say like he's staying at 31 I don't have to worry about knocking every door I just go to the 31 and then knock him and then B so yeah uh that's the whole reason why do we need an index and there are two types of index again I said clustered index and non-clustered index and they map to these two things again so the cluster index is similar to the index organiz table and non-cluster index is similar to the index F thing uh so the basic difference here is in clustered index when you save a data data is getting stored in a sorted order so if you have an index and that index is in some integer or some some integer right and if you have if you WR 1 to 10 and if you are writing something like file again so it will come back to the middle of it and it will update the file okay so when it updates a file in the middle up it has to move everything to the top or bottom it has either do arithmetic left or arithmetic right kind of thing uh so that's a bigger problem with the cluster index but it again gives you a better range queries and it again gives you a better reads whereas non-cluster index it is good for better rights uh again going to the next thing okay let's get to the next fundamentals of databases can you name some data structures that you know that the databases are using and the data sectes are powering the databases in okay ashing what isse maybe for Hep we can use the priority queue okay binary Cy okay anything else cool uh let's go to I think there are few things which are unanswered in this thing I have some surprise for you uh let's go to the next one this is basically not a data structure but it's a really important feature which is being implemented in all the databases I think most of the databases at least so what is a Val Val means right ahead loging uh what is right ahead loging it's basically up and only file uh whenever a database writes a right or right means like it can include both insert and update whenever database do or right that that means it is changing a state right from State one to state two and what is the biggest thing in database the asset principles right stands for durability the moment the database says like hey I have your data saved that means the data has to be there in the system whenever I come back even it goes to enormous amount of recovery and failures it has to have the data so this is a big okay okay sorry I press the wrong button so this is a bigger thing that powers that particular feature so what is a Val again coming back to it Val is basically up and only file so up and only file you you all I think most of you would have uh done this work right doing opening a file with the particular mode read mode write Moree and up and mode okay that's basically this opening a file in up and mode what is the big thing with that when you open a file with app and mode you don't get to edit anything in the file you can only up and to the bottom what does that mean that means you can do only sequential rights there is no way you can do any random rights in the file so why does it matter because again sequential rights are much faster in this than random rights uh again how does the Val work Val whenever you write a particular entry to the database it gets added to the Val first and Val records are flush to the data flush to the disk irresp itive of how much time it happens okay whereas your rights will not be fleshed to the dis every time it depends you can set a configuration to do it but vales are always plus to dis for every right it is not efficient but it has to be done for the durability so why do we need to do that against for the data integrity and the durability the moment the database says that I got your request that means the request has to be there so how does it work so when database crashes it comes back it reads A Val it won't read anything else it first goes to the Val file check like is there any uncommitted data for me if it is not uncommitted if it is uncommitted then I will take it and will try to redo the operation if it is one committed then I will go from this state to the other state say that like I am available for the further processing or else I will keep on processing the data so that's the whole point of having a Val and again I said it do it is most important for recovery process and again you all have that question if I write everything twice to the Val and then to the actual data file will it be using will he be using more dis yes honestly yes so that's why Val will be ared once the changes are committed so it will have a checkpoint once the changes are committed periodically it will remove unnecessary data it only keeps the valid data which it needs for the next recovery cycle so moving to the next data SE is there any questions on this one so far on Val why do we need it how is it being implemented yeah uh so do we have any diagram for this like to visualize better uh actually on the B I don't have but for the other data structures I have but it's because basically a file system with up only thing uh I can have a picture for it but it will not be a huge picture uh maybe I will try to find something and share it later yeah share it on LinkedIn yeah yeah I think um there is a documentation by post if you just search Val internals right post has a really good documentation on B uh you can read about it uh but yeah I will share about it in my LinkedIn cool so moving to the next thing which is BR B this is ax pattern iing okay if you find it I'm happy uh so I think some of you have talked about it yes B3 is one of the very widely used data structure in databases uh what is a v tree how does it differ from the binary sear tree so V Tre is basically a generalization of binary or version of the binary search tree uh which can have more than two children if you look at it it can have multiple children and uh it's self balancing that means that I don't have to worry about the insertion order any order I can insert it will get into the balance state so that any of my further inserts or updates or deletes or a select everything will be o of login uh that's the biggest uh biggest thing with the B+ tree and anyone have any idea why b+3 is very popular why not any other tree why not rbl tree R some other please why is it so popular okay yeah one but you have to get into the leap for that right but you can do the same in other uh binary search right like I'm saying like even there is a big difference between and B plus3 I forgot to talk about it thanks for finding it out so the big difference with b and b+3 is that in B trees you can have records in the middle of the nodes middle of the tree that means even this nodes can have data records whereas on b+3 only the replot can have the data records and then the all the internal things are just pointers and another major thing in B+ C all the leaf notes are connected so that any of the sequential operation that you do it will be much faster because I don't have to go from here to here and here I can just do 5 7 11 12 15 16 19 20 in a sequential order I don't have to worry about any of the random jumps that I have to do that's the difference between B and B plus G but yeah that's a difference that's the reason why B plus3 is popular but why even the B3 any idea no rebalancing is that rebalancing is one important thing of B3 which is why it is also being used but what are the other reasons any other reason that you feel like that could be the reason why it is that any guesses I can wait for some for faster access okay but you can also do that in other SE right that is self balancing that point is already covered like you get a self balancing that makes sure that the time complexity BL there are other self balancing trees right why not that over this okay that is the self balancing again everything is talking related to self balance right no idea okay good I I will go so self balancing s it is the first major reason why it is being used but there are other reasons also you won't see the B is being used on in in memory structure right you hardly see that it is very optimized for dis Bas storage the reason is that every node here it can be a page in OS it can just be a page in O and I can just cat all the internal nodes for any access I do it is just over one access dis access because everything else is already sitting in memory I can just cash all the internal notes of the B3 or b+3 and only the leaf note has to be thisse that means it's just the over this for me it is highly optimized for dis space thing where I can have a better read latency and again the flexible node structure when you look at this this number how many branching that it can have you can Define it if you feel like I need a better branching Factor you can do that I need a small smaller branching Factor you can do that whereas on binary search it's always to right uh again going back to the okay uh and it supports a range query like someone said I can go to the leaf node I can Traverse the whole thing uh those are the major reasons why I feel like B+ 3 is much widely used in the industry there are few other things where like it's like a Java uh it is there it is being used it is production tested and it is running I will continue with it uh so yeah I don't like Java so loes I have one doubt yeah yeah so you said that uh this stores it node in in memory right yeah so let's say we have data of 100 GB and it's my uh no it doesn't store the records in in memory it actually caches the internal thing in memory all okay wait is the so if you look at this this 20 or all of this thing it is sitting in disk but it is cached by OS page so when you do a seek it doesn't have to do a dis seek it is already available in memory because it is cashed by the O so it don't have to do a dis seek it is already available in memory but it is also there in disk so whenever you do a write the OS will flush the inmemory page to the dis uh did I answer your question so specifically what it actually cashes like uh the the record or just an hash value of that no the whole page itself will be cached like if you look at this structure right if you look at this structure this whole 20 this bar or this particular node this particular node even this can be cached anything cash me cash right like it's a OS which takes the decision for OS it doesn't matter it's a B3 node or something else it's just a disk page I have to I'm be like consistently being called for consistently being fed for so I will cash it so that I am having a hypothesis that it would be used in further calls also so it's say A O optimization did I answer your question yeah okay I will read about more yeah yeah uh moving to the next one uh which is LSM tree this is how many of you know about one okay I have a really interesting thing to talk about so this is one of my favorite data structure I got to know about it in candra I I think we were using Cassandra a lot in my previous project so Cassandra was using LSM tree uh that's how I got to know about LSM Tre but let's understand how LSM Tre works because whenever I read this you will all be like I don't understand anything I will get into the image part so whenever a Right comes to NS SMP right this is how it works so it goes to the Val to make it it is persistent it is durable so it writes to the Val it to make sure that it is persistent and it write to a m table M table by the name of it it means it is in memory so it have in memory structure which holds the ratest inserts and that's all whenever you write thing it just writes to the memory and it's persisted through the Val nothing else so that's why this is much much more optimized for WR based workloads so you because you're just doing one dis right no index updates nothing just dis right and I'm writing to in memory you can ask me like how does it work what if my system crashes how does it actually persist the data that's where the background process comes from the picture so whenever this m table there are multiple parameters again there are time based flushing or then there are size based flushing so moment it reaches some defined size maybe like 100 MB 200 MB it flushes it to a dis and the structure that it flushes is called sted Sing string table SS table so stter string table by the name of it it is in a sorted order why because it's an immutable thing I will not and update anything and it is in a sorter Manner and it's again good for range queries just like how we have seen for the cluster Index right if it is a sorted manner my range queries will be much faster so same thing here SS table I will flush it but when I flush it I will keep it in a sorted manner but when I write it here it's a in memory structure I don't have to keep it in a sorted manner even in my in memory strcture keeping it sorted will not be as hard in terms of latency and throw as keeping it in disk because you know the napkin math right in Ram access in nanometer Nan whereas the dis access is in a millisecond or micros sorry uh micro or M microc yeah uh so even if you do sorting on the memory it will be much much more faster than sorting in a dis so whenever they flush it they keep it sorted and they also keep the uh unique data and write it to SS table now you can ask me what if I have multiple entries for a same key okay because one first time when I write to a m table let's assume that it's a first M table one okay it has ID for flage okay and I come back after a hour I write it again I have a new record I write it again the new records again goes to a different SS table okay how does that table knows that I have to actually check with this also so that's where the level comes into the picture and that's where the compaction also comes into the picture so the levels if you look at this it is ordered in the oldest to the newest so from the top it is newest and to the bottom it is oldest so I want the latest record I will go through this hierarchical order I will always check the inmemory structure which is the latest one that I have so anyone who have written it will be stor in the inory so I will write it to the in memory I will first check the in memory structure once it is not there I will go to the next level which is level zero I will check all the SS tables in the level zero in the order which is written and then I will go to the level one if it is not there again I go to the level two it is not there again I will keep on retray it till I reach the end of it that's how the read Works in LSM you can yeah I think from the looks of it you know like it's very inefficient yes it is inefficient for reads there are optimizations which are done on top of it like Bloom filters and having a separate index for it but it's a topic for a different day so I have to yeah okay thanks for finding that out so that's why the next Cas is what will happen when I do multiple updates for the same key and I have a key for locate and I keep on updating the record again and again and again that means on Ln to the l0 I will have key for location different files right so the key will be from here to here but the latest one in me table is a is going to be the one that I need all the other things are dead records which I have to clean up right then that's where the compassion comes into the picture that's where the merge also comes into the picture so let's look at the the name of it it is log structured merrye the log that means it is up and only so whatever I write here it's kind of immutable it's like a lock I will up I will just keep it I will not touch it even if I have to update it I will write it here I will write it in new file I will not write the old file there is no in place update there is only a delete and insert okay so I insert a new file and I delete the old records so when I delete the old old records I have to remove the old values also right or else I will be just having lots and lots of older data which I don't need and I will be using unnecessary dis for it so that's where the compaction comes into the picture that's where the merge s also comes in the picture all of you at least know the merge right how it work it takes two things compar first one first one merge it second one second one merge it and it create the newer sorted thing right so that's how it also works so it takes two level sorry yeah I will finish this yeah so if you look at this one there is a in level zero there are two sort of tables one has key1 key2 key3 and same 1 two 4 when I merge it I compare this one and this one and I know which one is the latest based on the metadata that I have and I take the latest value here and I will drop the oldest value how will I drop it I will not drop particular record I will drop the whole file so even if you look at this I take the key one one from the second table and key one2 from the second table key 13 from the first table key4 from the second table I drop both this table I won't drop particular part of it I drop both of it so now I have a new table which is a merged version of the both older tables which has only the latest records or only the unique records and that's how the leveling works so level Zero Records are merged and moved to level one level one records are mosted and move to level two level two records are mered and move to level three four five like that there are other algorithms for it this is level based that is size based that is time based uh and each of it has it own perks but let's not get into it in this particular session because it is a very different tangent to the to what I wanted to cover uh so I think that's on the okay coming back to the actual test I want to dra so loog seure most three it stores data in hierarchical structure like we see M table level zero level one level n and it is optimize for both reach and right how is it optimized for reach and right you said like re s right because you have in memory structure if I write and read from it at the moment the right is already in the in memory so the possibility of me going to the level n will be very less it's amortized thing okay uh again coming back to it leverage both in memory and display storage that's why it is optimized for both reach and rights still I don't agree to the Deep thing but yeah it's really good for rights uh optimize for right heavy because it every right it is just every right it is just getting stored to the memory nothing going out of that only one dis right which is on the Val which you do anyways irrespective of it is B tree or LSM tree which is there and as part of the LSM tree there is no right here there is a right behind the seene so For Your Right operation it is in a latency of milliseconds because maybe even better or one digit millisecond whereas on B trees it will be two digit milliseconds uh that really matters in High skin system getting to the questionaries before moving to here you have any questions on LSM before I move to the comparison the compaction so does that happen in the backround or when you make it happens back all this flush as well as the compaction both are async so it happens periodically based on some of the parameters that you configure you can say flush it every 15 minutes or say flush it every 100 MBS or something like that similarly compassion you can say FL it when the dead records are more than this percentage or FL it once every 30 minutes you can there are multiple configurations provided by different databases based on their needs again but two question yes it's not synchronous that means this part is only the the right to M table is what you have to worry about all this part is taken care by the database behind the scene yes my actual question was when I do the mer s so I do an l0 and L1 yeah it does it go back to the L1 l0 or l so I merge all the records on l0 move it to L1 I merge all the records of L1 move it to L3 yeah any other questions so supp the I not ask suppose it crashes I have to go through all the SS tables and look at the Val and that's how I reconstruct right or you don't need to okay I think reiterating the question for the people in Zoom has asked a question like how does a recovery work in LSM Tre uh so in LSM Tre what will actually be missing when the uh when the system crash only the inmemory thing will be missing right yes you don't have to worry about any of this because it is already immutable that records are already persistent I don't even have to worry about all these things I just have to worry about the M table so I will have all the records that I have inserted to M table there is a checkpoint on the Val saying that from this point I haven't committed I haven't pushed to the flush to the disk so from that point I will write it again all of them back to the m table and I will leave it to the flushing mechanism to take care of the rest so it will get back to the state it was before when the recover when the crash happens did I answer question any other question for the people who are in the Mee in this there is no uh okay again reting the reiterating for the people who are in the call and the question is like is there any index in this LSM how does it work one thing for the M table it is already in a sorted thing that is a data structure called skip list uh you can read about it that's how the most of the databases actually implement this thing it is started in the way it is already so it's kind of index for the M table for the SS tables most of the implementations have optimizations on top of it some of them have index files for each of the SS table for each SS table there will be index file also so that you can just check the index file just to make sure that whether the record is there or not and there are even better implementations like Bloom filters uh for people who don't know what is Bloom filter Bloom filter is basically a probabilistic data structure which gives you a probability saying that this record can be presented it cannot guarantee you that this will be this will not be there so the probability is always on the positive side so it will have false positive but it will not have false negative false negative means it can it will never say that whe When the record is there it will never say it is not there but it will always say when the record is not there it is there that means it can say false positive the one headache is that you will go to the SS table with the empty hand saying that there is no such Rec I went into it I read the whole SS table but there is no but that is fine but there is a positive side to it because the probability data structures will just take very small bites compared to the whole index because I will not hold the whole data I will just hold the probability uh it's a different conversation for it but look into bloom filters for such thing those are the optimizations which are done uh so if you look at the this slide most of these things apach candra Rock TV I think even TV I have seen Bloom filters for each of them uh so let's get back so getting back to the differences when to use LSM when to use V3 I think these are like kind of already bolded for you to read but I am reiterating it LSM is known for right in intensive workloads uh where the where you have to store a large amount of data and you are using disc and drives uh whereas a B3 it is a traditional interesting structure which is there for a very long time and it is very very good for reads because you are just doing a direct access whereas on LSM you have to do multiple SS table query to make sure that particular record is there or not it's amortized thing it is all not it will not give you o of login kind of a thing similarly for right flow like I said it is a inmemory right it will much more faster similarly it's an up andon structure it uses the um sequential right so it will be much more faster and when to use what uh lsm3 are mostly used when you have a large amount of data and it's a right heavy system okay like such as time serious databases uh whereas the read intensive workload with a very small to medium data sizes and it's a relational thing uh it is much better to use uh LSM uh let me this guy okay good moving to the next yeah loish I have one yeah yeah please yeah so uh so LSM trees are implemented for altp database and the data varable type database are using B+ stre you can you can classify it I don't know the exact thing because sometimes people also use it the other way around basically his question is that like is B3 meant for oltp workloads which is transactional workload whereas LSM is meant for analytical workloads yes kind I said I said it reverse like thep is made for L LSM trees or no o yeah go ahead yeah can you show the difference slide yeah I think if you look at this slide right whatever they seeing the post SQL these are oil TP data stores whereas the other side that you have I think Cassandra I won't classify it as oil AP uh it's a white column store uh apach has space yeah kind of and influx is also a Time serious yes it's a oap rock CP is a key value store but all of these guys are using LSM uh there are like typical LSM implementation for oap which are columnar also which is what I would classify as pure oap uh like pinot and other things they have a different implementation of it but more or less on the LSM thing yeah so databases like Google big query and the graph T be like new forg so do they internally Implement B um I I remember reading that big queries is on LSM if I'm not wrong I have to reindex my own index uh I don't know uh maybe I have to Google it but uh uh actually LSM is for heavy right workers right yes and big query is for analytical purpose yeah as for my understanding so it should be B No it should be LSM like like you said right you said everything the last word is the opposite of it right like if it is analytical thing uh you do lot of wrs more than reads because you dump a lot of data and you do analytics on top of it how often you do analytics queries but you write tons of tons of data to it right so it's right heavy more than read see even for us we use Cassandra for one of the analytical use case in the project that I was in uh so it is the LSM is meant more on the analytical side uh you can I think there are implementation that also provides if you look at gab TV right it's basically a uh distributed database so it also supports o TP so that's why I won't classify these things to the O TP and O AP World because that classification is completely for a different site it's on the access level whereas this one is on the implementation level okay yeah goodh moving to the next thing are you ready to apply the knowledge again my favorite guy Goku uh are you all ready uh should I wait for some questions how is that LSM implementation system no it's a search system it provides the indexing the reverse indexing but you can also use it for other no no no they provide a indexing which actually do the reverse index so data is stored as it is data files and index files right how what is the index that index is a reverse index so you will have an index for each word which points the document in elastic search so that is the index for that thing so in elastic search what you usually do is like I type something give me all the suggestions for it right what are all the things that has this word it's basically reverse index I have a text what are the documents that have my text okay it's basically like a key is the word for it okay there is nothing much there so that's the thing here again moving to the next bit this is the most uh interesting bit for me I will move this guy okay cool okay going back to the project I worked on for some time it's interesting to talk about it yeah so I was working in this project for around three years now I moved out of it now uh but this is a very interesting use case I solve that uh so I think the domain that we wear in is on the connected vehicle platform uh we were working with one of the car manufacturer uh instead of getting into the what we developed for them I will get into the what use case which this particular talks is on uh so there was a use case where vehicle will stand stream of data so connected vehicle means they have a Sim in it they can send the Telemetry data to the server so they send a Telemetry data to our backend server they just stream it whenever they generate data base the sensor that they have based on like datas like whether the door is open or not what acceleration that I meant whether the pedal is pressed or not what gear I'm using all those informations so Those sensors will be captured and will be forwarded to the backend system and the backend system has tons of business use case built on top of it one of the use case actually um we had to keep the latest record that we get from the vehicle so that means there is a stream of data at every point I need to know what is the latest record that I got from the vehicle um so basically we listen to all the stream of vehicles data and we put it to a database and the structure is like this we have a vehicle ID and the latest value it's kind of a Json that's why I put it it has lot of sensor values and we were using posters for it that was a mistake I said uh so everything worked well till the morning where as was down the entire application was down the major reason is that it was a Monday morning it's the interesting thing is that everyone wants to go to office and use the car on Monday morning right because like how many of you go to office on Monday morning prefer to go to office on Monday morning Monday at least we do come I know how many people but at least like if you are going to office not that's a different thing but at least uh so very interesting thing one of the Monday there was a huge peak of load where we were inserting around 20,000 records every second and our iops load was just 15,000 we were not able to handle that load and our system crashed we couldn't find out how to fix it because a load is coming in all like we could do is increase the iops we did that and it eventually fixed it but we know that that the problem will come on any day because the car manufacturer was selling more such cars and and our prediction was 10x vehicles in the next year so we have to support this use case much better in upcoming months we can't wait for it that's where the request comes to have a better solution for this use case what can we do about it any solutions anyone have any rough idea what you could have done some of the people are sitting here we did all those if you look at this we have replicas but the problem is that our I is hit at the peak that means that in the whole cluster I can't write anymore because my this cannot handle more than that I'm getting huge load of data but I can't insert because I don't have the bandwidth for it I don't have a throughput for it so because this rights are failing because the database is already overed with the existing rights the rights yeah that system so you can go to a distributor systems which can handle parall rights right on multiple systems not just on a single Master server thing we can have multimaster or multiple things which are listening for the rights okay like I said the one that we were using the post which was not in a high aity mode in terms of multimaster thing so we were not having partitions we just had a single partition which is just replicated to the different ACS and just for the availability purpose it is not for the throw purpose purpose okay uh so I think let's get into the actual problem with for why is it not so let's understand how postgress Works to understand what's the problem that we faced so in postgress when you do update it's not actual inut in place update it is actually a delete and insert for deleting the record it also puts a one more entry called Tombstone saying that this record has to be deleted and it also writes a new record to it since it's a Json data and a huge data it can't fit into the page it puts it in something called to post has something called toast which is kind of a heap which doesn't have a pce structure optimized for reach and R so we were unaware of all these things we did like okay posters is working why can't we just use it uh we started with a very small number of vehicles so it scaled for a very long time for two years it was working properly we didn't had any issues one fine morning we had this issue that's when we understood like okay I do 20,000 records insert that means that I have to delete 20 more records now I have to do 20 right 20K rights now plus another 20K rights for deleting the older records because that dis has to be cleaned up right someone has to go back and then delete do the thing there is a process called Auto vacum and post which actually do the voming thing like removing unnecessary dust out of the system this is the way the post Works anyone have question here but you have understood what's the problem with this right so should we use post or not okay that's a good question again it's specific to the use case so posters has a really good optimization called hot which is update in pup or something like that you can read about it which makes so that it updates the same page instead of writing to the different page it leaves out some page size for such updates to come so that whenever such if it just writes to 60% of the page and keeps 40 40 per remaining for such updates to come so whenever such updates come it will use the remaining space just for the updates but those are optimizations by default you won't get it you have to enable it uh so we didn't use it the way we solved it is we move to Rus in a high availability in a multi partition setup the reason we want is there is an additional catch here so when we if I go back to here right to look at it there is a specific thing in a car where there is a protocol which says whenever a car goes out of the network and comes back it has to send the latest data first not the old data in a message order that means it is there to make sure that car crashes it has to send the crash data first not the older data it is for the safety purpose enforced by the protocols but that is a problem for us because we want the me message order to know what is the latest dater there is no such way we have to handle it hous uh so that's when what we did we read the latest record from the database and then we compare the time stamp saying is this latest than the record that I have on the database or not if it is I will write it if not I will discard it so that's how this one was implemented so if you look at it it is 20K reads plus 20K wres plus 20K deletes so that many iops that we using and our system just cried so we move to a red set cluster for trade we traded of consistency for latency because it's a stream of data and our uh clients are fine with having outdated data for a very short time because anyways I will be updated with the next latest data because steam of data is coming in if if I miss up One update that's fine for them so that's the use case that we had and we looked into it we thought why can't I use the in memory structure which is very optimized for both recent rates right because in memory like I said it is much faster for both re and rs then dis on any given day so that's what we did we moved from post to Red dis but that's not where it stops so we took it we thought like what there is no other option for us to have a better approach but the persistence is also supported we looked into it there are options available but we thought like why can't we write one on our own okay so that's when we actually came up with the internal hack for around two months uh where people we were asked we asked every one of our employee to pick up this problem statement and create their own database which could solve this problem there are few constraints uh like I said the there are heavy reach and rights operations both re and rights has to be considered and the persistent should also be there those are the two main things and it's also update it's kind of a key value St so that is the basic use case they took it they wrapped into a different use case and then added some more con to make it interesting that's where this one comes any questions at this point till now can I go to the next space of session use we do have message broker this part is message broker okay this part is all message broker but how do I know what is the latest data in the message book uh I mean that back pressure is already there so this one crashed but after that we fixed it this back pressure was there so if I can't consume it I won't consume it at this speed if the this consumption is slow this consumption will also be slow but the problem for our client is they open app it is giving 30 minutes whole data and the users will be frustrated right you are using your car and then you're opening your app it has 30 minutes old data which they are not happy with so that's when we move to red and fixed it but coming to the hackathon thing that we did so there are few constraints which are something I picked up and try to uh use it to my solution to get the better solution uh so I participated in the hakatan and these are the various things that I would say it's a important pieces in the problem statement so one thing they have points only for rights not for the reads though the use case actually has both reads and rights at the same level and there is no time limit for system recovery it can take infinite time to get recovered there is no time limit this is an add thing which they did just to make it interesting in a real world scenario this is not possible and another point is you can have additional point for durability this point is very high such that it CES everyone to do persistence and they said like you will get nvme SS in test environment whenever we deploy it there is an nvme SS that means it is really good even for random rights and random RS compared to hard disk which is terrible for it this one can give considerable performance for random rights and random RS and there is a variable payload size why does it matter I can have any size of payload rate it matters because if you have a variable payload L I can't do in place update if I have a first package of size 100 KB and next package of size 150 KB I can't write 150 KB data in 100 KB size I can't do in place of dat I have to write it somewhere else and delete the record right same way I have a 100 KB data sitting there and I have 80 KB data coming in there's a 20 KB is Miss it's an internal fragmentation for the particular page so then I have to have a different process that goes and reads remove the internal fragmentation come back give me back the data give me back the size sorry so that's additional constra which enfor us to go with the up and only thing or new insert and delete approach so let's get into the design Choice which I made uh so B3 like I said it is optimized for reads but it sucks for rights similarly LSM is good for rights you all would have thought like okay you would have gone with the right LSM because right intensive I didn't do that I looked into it I thought I can take a even better approach this is a mixture of both B3 and LSM can I combine this two and then get a better approach which is good for both reads and wrs there is a catch here this records how many records that get inserted is on the there is a limit for it I didn't put it here but there was a limit on how many records that they can inest to the system okay and they also give a size of the ram that we get for the database to be deployed so when I calculated it the size of the ram I can easily hold the index in memory so that's when I took a call okay before going there why I didn't choose the LSM uh there is a bit there so in LSM if you look at it I have to write it to a Val to make it durable right and then I also have to in the Asing process I have to take the me m table write it back to the disk there are two disk operations which are happening be it Asing or sync together it is two a two operation I thought like why can't I merge this two into one can I do much better than this okay that's where my Approach comes so it has inmemory index and and up and P so I thought Val is already holding my record why do I need to have a different structure which holds the same record I will not even delete my Val record I will just keep it which already has my record I just need an index which says where in my Val the record actually resides and I felt that I can keep it in memory to make sure that just one disk seek and dis write for both reach and R if you look at the read flow R1 and R2 it goes to the in memory index it checks whether the particular key is present or not in my system and if it is present the key value it looks like this it holds a file ID whatever the name of the file and the offset where that resides and the size of it how much I have to read from the offset and what is the time so this time samp is needed when I do update later I can compare this with the latest data coming to see whether the latest data or not so this three details are more than enough for me to go to any file read something and then give it back like I said nvme SSS are good for random reads also the performance that NV SS provides is much better for random reads so I thought I will just do basic what posters do like this part was dis in postgress I moved it to in memory this bit was I think for the people in the zoom this bit the inmemory index was also on dis in post I thought I will move that also here that will make all my dis seek just to one so and when I write it I write it just to the active files just like how it was on LSN okay I will have some set of open files I will just write I will just up to that file I will not do any inl operation once I reach a limit I will move to read only operation sorry read only data FS once that is moved I have my own htics and metrics which takes and do compaction which takes the read only data do compaction and gives it back so you can ask like hey this is again 1+ one yes but if it is if you consider that then it is 2 + 1 in LSM if you consider compaction also in to the disk access so yeah this is the design that I have so if you go to the read path again like I said if it check it it will check whether the ID is present in the index or not if it is present it will take the file ID offset and size with that details it is more than enough to query any file system and get any data that you want and I deize the buy to object and return it back and there is a right path which is I get a data I serialize it I write it to one of the open file update the index saying that this is where my new record is and my job is done coming to the compaction part which we have talked before again the same thing the way it happens so it takes multiple records it do m s and then it puts a new record deletes the old record um that's how the compaction works but the way I have done it in my system there are two metrics which I look for not matrics but two things two constraints are of uh one is every minute I check for compaction whether I have to do compaction or not and the way I have I will check it is I will see whether the overall disk date record percentage is greater than 50 or not what is a worst case thing what is the best so let's assume this if my system don't even have de records I don't even have to do a compassion right because all of my data is correct it is needed I don't even even need to delete the records so that's why whenever I write I also make sure that I have missed one record that has to be deleted I hold a counter which says how many records has to be deleted at any given time that gives me a vision or idea there is a question in the chat is anyone looking at the question which one the question all the questions is it okay sorry I think it is uh mentioned that uh all the questions in the chat we will go over it later sorry for that um so I think again the htics right coming back to what is the worst case thing which is I compact unnecessary data which I don't want to do that's when I put this thing 50% that means when I come back I'm going to get better than 50 that means there are 50% worst data 50% best data I will keep only the new data sorry not worst and old and new so I will only keep the new data and I get 50% of fitback so if I do 6040 I have to rewrite like why you can ask me like why is it 50 why can't it be 40 why can't it be 30 something else if it is 30 that means only 30% of my data is bad but for that I'm rewriting 70% of data but if I go with 50 50% of data is worse that means that I have to delete it and when I do it I'm going to do only 49 or something like that so that is always optimal so that's a htic I got uh and this is the process that I did so whenever I do a compaction uh create new files for writing close all the open files so that it will be available for uh uh compaction I trade through the records index instead of going through the SS table like how LSM do I went through the index because for me index is a inmemory seek I don't have to worry about dis hold seek I went through all the index take all the records and write it back to the new F again this is random reads for me but I'm fine with it because it was en uh so this is how it works once all the records have been moved that means I have moved all my old records mer them only kept the new records in the separate files I will delete the older files that's how the compassion I have designed work uh any question till this point no I think think about it so that's what if it is greater than 60 then I have a probability that if I can see my thought process is like I only want to do efficient one what is efficient one 50/50 anything over is efficient because I'm going to write less record delete more record right I can go for 60 70 80 but the problem is that then I will be keeping lot more older records in my system then is a huge process for my compaction to happen so it is better to do smaller smaller compactions if you have run Cassandra I think knows about it smaller compactions much faster much better than running a whole long bigger comparion uh moving to the concurrency Primitives how many of you know about what is concurrency primi what are the concurrency Primitives at least let's go to what is let's understand what is concurr at least people understand what is concurrency right I have multiple threads I want to do a process I want to process multiple things concurrently that means simultaneous at the same time not at the same time but in a timeline I want to do multiple things concurrently concurrency and parallel things are two different things parallel means that I will do in parallel where as concurrency means that I do one after another kind of Tes kind of a thing uh so concurrency lets you utilize your system to the the fullest like you have multiple threads you can perform multiple tasks one waiting one doing something you can mismatch you can um pick and do multiple task utilizing the CPU to the fullest that's what concurrency is and it is very essential for performance and respon responsiveness and specifically resource utilization uh which is your CPU time uh and let's come to the concurrency primitive so concurrency primitive are your newex logs all the logs all the ways of getting the thread safety it's basically a name given to the list of whole umbrella of things which is low level construct that helps manage and synchronous confront execution uh and ensures data threat safety but basically it is your mutex seore message passing Atomic operations all those things it's a whole umbrella te given to the data safety mechanism for in concurrent world okay so why does it even matter in every database you do parallel reads and rates your what are the basic things you don't want one read to affect the other read you don't want your one read to wait for the other read to finish right you don't want some other keys right to block your read I am updating logage but why should an's update has to make me wait for it I am a different key I don't don't even have to wait for it I am living in my own Universe I don't have to worry about it that's like a pure concurrency right key level concurrency I can do wrs for multiple keys in parallel I can also do reads and wres in parall so that's a basic thing and we all have most of the databases that you are seeing they do mutex logs some of them do copy on write uh which are much better version but it takes more disc access uh but I have gone with a different approach so okay I didn't go through this question but what do you feel about this which one is the best among the concurrency concurrency Primitives that you have used I think most of you would have used some logs right new Tex read like logs are much better than mutex because you can separate read and write parts separately right reach can happen in parallel whereas when right happens it will block all the reach and R whereas multiple reach can happen at the same time so that's a optimized version of mutex similarly message passing is there semop for is there and atomic operation there are tons of other things I have written a Blog about it you can find it in my website uh on what are those things but going to it so I felt that having no lock in the system is much better than anything else because your CPU time is much much much faster then your discon and you make your CPU wait for something that is the worst thing that you can do tostem if you want to achieve the entire capacity or the throughput of your system you have to make sure that you don't make anything made for something so it is bit hard to achieve but I have done multiple things to make sure that I can arri at that solution one interesting thing one of my colleague did the similar kind of a design uh it was a coincidence honestly speaking uh the same design in memory structure up and only file there is a database called bitc which is based on this design uh so we both referred it we fa is much better than this we implemented it but he did through interestingly he also did it in he also used the active W framework which I used so it's not a language which gave the performance it's not the framework which gave the performance it's actually The Primitives which gave the performance so I did it using Lock Free he did it using lock and my solution was 20% to 20 to 30% better than this and when we looked into the performance of the database when we did the low test his system was not utilizing the CPU to the it was stuck at 40% and 50% of usage but the throw put is gone mine was at 98 99% of CPU and that's the max throw put that I can get if I want to go even better than that I need to go for a better design I can't do anything with the concurrency primitive anymore so the way I have implemented is so if you look at it there is a inmemory structure which is basically a hashmap and if you are in a Java world you know that there is a concurrent hashmap uh uh which gives you the concurrent way of accessing a hashmap uh I use something called Dash map in Rust which actually takes the concurrent hashmap implementation in Java but do it without locking so the way they have done it is using Atomic operations there is an operation called C compare and swap so it basically works optimally it's not a pessimistic lock it's optimistic lock so what is pessimistic what is optimistic pessimistic means I lock it irrespective of whether this could be accessed in parallel or not I don't even know whether I need it or not but irrespective of it I will lock it whereas optimistic I will try to update it but if I find it like at the last moment this is being accessed by someone else I will come back I will revert so the way it works is there is a compar andap so I say change this value from 1 to two it will change only when the value is one if the value is changed to one to three by someone else it will revert back and then say like someone else changed it your transaction P so that's a hardware level thing being provided by lot of chips there is a c operation being provided like if you write assembly code right there is a c operation provided by your chipset itself that do this at the assembly level there are uh like it was first in software LEL and people find it like this if if this is implemented in Hardware it will be much more efficient that's how we move to the hardware level uh so the one that exists here it uses a hardware level C operation which do the atomic rights so all the atomic integers that you are seeing right in all the language implementation they are based on the C operation uh it actually takes the v data and the new data and it compares the current data with the old data only then it will update the new data or else it will come back and reprocess it okay so my index updates are now Lock Free what is the next thing now I have a disk write I have to write to the disk I have a file Handler for the disk and in R the beautiful R won't let you have the same pointer by more than one thread if it is a mutable thing only one thread can hold access to the pointer so that no one else can access the or up upate the pointer in parallel universe okay that's the way rust Works uh so the way I have did it there is a concept called message passing and the arst uh framework that I use is built on something called actors uh they provide something called actors which is basically a programming model Al together like object oriented functional rate actors is also a different programming model it's all act like a message passing thing from actor a i pass a message to act B all of our uh networking right uh the Sim Sim that you are using all of them are based on airl which is based on actor base that's how they make sure that highly concurrent mobile usage is happening so that is the actor note it down you can read about it later but I use message passing an actor while writing to the dis so when you go back to the first piece here so whenever a dis writer picks something it picks a random one out of the active file and there is there are actors uh receiving message for each of the F so basically it's like one thread for a particular file that thread doesn't have to worry about someone else accessing the file at the same time because he is the only writer and the way that thread gets notified is through actors actors will be fed with messages that the Frameworks will take care of that message passing is happening through the message passing thing that you get on threads and process so that is the way that I have handled the right part so all my rights no longer go through the Locking so it just goes to the dis writer disk writer picks up one of the open file it just puts a message hey insert it to the actor and it will wait for the message to come back from the actor the moment it gets the message back from the actor it says tells the client hey I have your data ready so that's how the write works so then there is a read so the interesting bit that you have the basic read that any of your system that you're using you cannot read two you cannot have two different reads on the same file Handler you open a file you open a file in a read mode you will get a file Handler right whatever variable you getting out of it a file hander that file hander will not be used for multiple reads at the same time that's a implementation thing so there is a better version of it called P read which is parallel read uh whereas in Rust it is read exact at uh which internally use a p in C which internally use a different uh Corner level thing uh so I made sure that I can read from the same file in parallel so this part is also not blocking my read read is not blocking my right is not blocking my index is not blocking what is the next thing I have up only file Handler so that I don't have to worry about parall okay yeah this bit is bit different so I had a different file Handler for read and WR so I open the same file in a read mode and I open the same file in upend mode so that I don't have to use the same file hander for both reach and right so yeah it's bit tricky but try to understand the bit and for the metrics I use Atomic integers again uh that will make sure that this is also not block any questions Lo should I should I ask yeah please feel free to ask so in the fourth line you said use different read only and op fire for parallel read and right to simple yeah so when a system access any file it's log that file right so it does can we parall do read and write it doesn't there is no lock concept for a file you have to put it on your application Level if you want the file should not be accessed by someone else so I think some of these folks have tried it in their training process like someone tried to access right to the same file that was reading it will not give Rie those options out of the box you have to ose it at the application Level you can have a read and write Handler for the same file at any given yeah but actually in operating system there is a concept how this process or any thread executes it uh locks the resources uh so this is what I'm relating so let's say there is a so OS doesn't give you those functionalities out of the box it is the application that provides it you can ask the OS to give the functionality which I didn't which I said like my system will take care of it I know what I'm doing give me the access okay Lo thank you yeah uh okay next thing which is the interesting thing I said database recovery flow uh so in my design the only the in Mor index has to be recovered like how in LSM only the in memory part has to be recovered right similarly in my design only the inmemory index has to be recovered since there was no limit on the recovery time I took my whole long time to read all the files in parallel index the latest record based on the time stamp and throw away the rest of the records because there was no limit on it I took all the time but ideally in Real World what you can do you can have checkpoints and snapshot for the index and post the snapshot you can actually uh update the index and like restore back to the index and then post that you can read and then update the index that's how some of the inmemory databases work and one more optimization there I did buffer reader to speed up the read process why I didn't do the buffer reader for the actual read what is a buffer reader any guesses you don't read everything yeah basically what buffer reader do is you request the index zero it takes and then the buffer size is 100 KB something like that it takes a 100 KB from the disc put it in the buffer and while you are reading it it takes the next 100 G KB from the disc keep it in the buffer so that you can do the next read so it's like kind of a hack that they did or it's not a hack it's a optimization that they did to fish the next record while you are processing This Record it's similar to how CPU uh this thing works what is that pipelining yeah so that's how buffer read works but why I didn't do that for the ACT ual read exactly I'm doing random reads I don't even know whether I need the next bit or not whereas here I'm reading the whole file from zero to the end of the thing okay I damn sure know that I need the next bit so that's why I use buffer readers to P the next record while I'm processing the current record whereas in the actual read flow I went with the basic read uh that's a bit on database recovery and okay before getting into the conclusion bit any questions on the database that I have designed I didn't do it but there was Ro TV implementation by U and I so I know that like somewhere it is better than that but I the thing is that some of the optimization that I have done I didn't include it here that won't fit for it because there are specific to the use case that we were solving like I did a very custom encoding for us because the data I know the data so I did a custom encoding for it but that won't work for every use case because they are they were written for a general purpose so that's why I didn't compare it with the them but I'm thinking of writing a uh I think roxb actually expose API right like roxb is very popular uh because they actually expose an API which anyone can plug in and then use it so lot of engines have plugged in Dr DB and used it so I wanted to implement the same interface and try to compare it uh but I couldn't get time to do that but yeah that's nice question yeah think most SS are very cheap we rarely run databases on hard disk do you really are people really running databases on hard disk no I think Amazon have moved out they still provide gbt2 and gbd3 support but they are also ssps still work it works but it will be slow it is not meant for that any other questions anyone in the chat before I finish matters it why does it matter the question is that like why does variable payload SI matter uh so the thing is that like that's what I said right you have have a different size record the first key the record size was 100 KB second one is 150 KB if I keep if I do in place update I don't have a space for 150 I only have 100 KB because the rest of the things are taken by the other records so I have a page page will have multiple records page is like a page in OS okay let's assume the file okay 0 to 100 index let's assume that one index means one k okay 0 to 100 IND index I have written first record 100 to 250 I have written the second record 250 to 500 I have third okay now for the second record I got a 500 KB next payload what will I do I don't have the space in this I have to move the whole thing out put the space for this and then I have to do the in place update which is very inefficient po poster doesn't do that PO do up and only thing poas also provide mvcc uh which is not included in this call I think answering to the question they do it for a reason they provide multiversion concurrency control you can R back to any version in post so just like how we do on Google Docs and other things right you can go back to different version of the data and restore from that for every key do that so it it is that there for a reason it keepes multiple records for a reason so even if you look at this one you look at it the next version it has a next version index pointer right the older version is having a pointer to newer version this is bit of the multiversion implementation so the index will have the newer record but you can go back to the older record also if the database it say like the newer record is crashed it's not yet committed the transaction aborted it can go back from here to here because it keeps the older record also so they do it for a reason again I'm saying every database have done something for a reason learn about it and use it uh so that is my next bit of thing uh there is no one fit for all but there is one hack here now post is putting themselves as one fit for all they have enough uh plugins to plug to make it analytic database make it a vector database make it a graph database make it a geospatial database they have all those extensions but still I feel like they are not that great compared to picking a database which is specifically done for that purpose they always provide a better performance and like I said right I think asked that question post and my are not same for us it looks like they both are SQL databases from the internal aspect one is clustered index one is nonclustered index one do in place update other do insert and delete so that is the first point second these are all the parameters that we might have to look when you are getting any use case whether it is AP orp R AP or structure on structure row base column base white column base what do I have to do read optimize WR optimize what is my read flow what is my right flow whether I want better reads or whether I want better rights uh so everything you have to consider you need to understand the implementation of each of your databases and then you have to make a call uh so yes it depends it depends like the disclaimer I said everything is subjected to database implementation read about the implementation read about the documentation it helps so thanks uh this is my LinkedIn ID and I have my own website where I share a lot of this knowledge uh I have written few things about databases and I'm a big fan of silv if you know don't know about silv read about it and this is the database internal discard server that I follow it's a I think it's a very big community and it's a world long thing uh people from different places of the world are there and people are who are there are the people who wrote the database internal book and and the other great peoples who have developed this databases so you can go there listen to the conversations it's a very enlighting thing uh and yeah please share your feedback if you find anything that could be improved anything that we should continue anything that could be plus minus things you can put it here uh we will read about it we will change it next time thanks okay how do I check the messages can someone read it is anyone looking at the messages okay we will share the presentation to you somehow you can check my meet yeah we will add it on Meetup but people who didn't sign up through the Meetup directly went to the zoom uh I will post it at least either on in my handle or somewhere else in the website or the handle cool thanks any questions any discussion aspects Hi Lis yeah one last personal question Y how did you master this this much of database because before joining this session I was just backend developer B to so I think to answer your question I was fortunate enough to put in such places to it the situation for me to do that more than I end up doing it on my own uh so it was subjective again uh at least that's what I do uh I think I feel like not everyone will get this opportunity what if I share my knowledge to people they can learn from what the from the mistakes I made that's why I know uh I don't know the answer for you for your question it depends again it's a situation that I got into so you have any road map any resources path like we should follow to master like like you have so there is a very interesting book on database internals of name itself is database internals you can read about it it's a very interesting book that's the book that I referred before implementing it whenever I uh get into this isue this particular Discord channel is also run by those people uh so it's it's a it's a thing that I would say you can refer sorry I think there is only one book in the I don't know actual auth somewh yeah one more thing there is another book on called designing data intensive application DD it's also very popular uh these two books I would definitely refer if you want to become a better backend developer cool thanks everyone yeah yeah one last question yeah so do we have to specifically learn any language like only python so can I Implement featur like this you can do it on python but I think it has see python is for a particular reason just like the databases the languages also are also there for a reason python is not a system level language it's an application language which is meant for solving having a better iteration while you are develop in you get to develop lot more faster in Python but it comes with lot of abstraction that abstraction kills your performance okay uh so you want to develop a database that you have to pick up any of the system uh level programming languages C C++ rest I think even to a point go is also good so for handling this tary data we should use like languages like rust and no no no no no no we use actually Java for that because that's an application that we have buil all of us scale was handled by the database and the distributed systems like kka Cassandra all those things those are implemented in different languages to your surprise both of them are implemented in Java that's the reason I hate both of them there are better versions of it Cassandra Sila is a best better version which is Rewritten of Cassandra in C++ uh similarly for kapka there is something called red panda which actually provides a C++ uh implementation of kka which is not yet picked up the uh B like Sila but it is really good but like you said we don't have to use those languages to implement High scale systems because the thing is that there are tradeoffs that you have to make the biggest tradeoff that you have to make is about development efficiency more than the scale so you can still do the same thing in Java also like how people have done it for Cassandra and Kafka but the good thing with Java is that it is very big Community it is widely known and there are lot of use case that are built on top of it you get lot more developers there and the support that you can do for the particular language is much easy how many people know rust Even in our organization very handful of people know rust so getting a developer in R is very hard you can develop it it will be much the best version of the application but who will maintain so those are all the additional aspect that you will get into uh which will actually throw away any of the optimization that you talk about it's a very 1 percentage of the use case which actually needs this kind of a solution all the 99 things we handle scale very I think recently I read a read a really interesting blog about why big data is dead because very hardly databases or very hardly there are are use cases for Big Data like the what is Big Data anymore because now the system has grown to a level it can handle TVs of data there is a postgress database which can handle T tens of DVS or 15s of TVs single instance databases so the power of the single instance has gone to a level so I think it's a question for a different discussion but yeah I will talk about it for a lot if you ask me that but I would suggest get better at what you are doing if you're doing python get better at how just make sure that you write the best python code okay thank you there are two two in happen one is that we were not able to handle it at the latency that we wanted to do that was one thing like someone asked uh we were giving data at a late period like 30 minutes late data or 1 hour late data which is not acceptable first point the crashing bit which happened because of the constraint enforced by the AWS so when it peaked at the 15K iOS was configured by AWS it started closing the connections okay so it started closing the connection so the connection closing started crashing the application so that is a hardware limit that they have enforced on the Rd we did that we did that and that's how we fix it in the short time so but that's how the crash happened which made us think like he post is the right option for this two because we felt that post is not meant for update heavy operations like I said right post is not yeah so we were thinking of any other thing other than post because post is not good for update heavy it it is very good for right heavy because it just puts it on the up and only mode it goes back but since it's update I have to delete the older records also but if it is in memory I have to just delete the data in memory so that's when we were comparing the all this high high availability High availability things we were even thinking of putting in Cassandra because we already had a cand cluster yes [Music] so we okay we did even that so it's a one year long journey that we have moved from A to B and C so we were using kka and we did partition level caching not on the kka site but on the application site we had local caching and it's a right through cach uh which makes sure that that's the way the second step we did yes so we move to caching we put the cash in between so which reduce the I a huge number we came from 20K to 3K iops from that point because every iops like every insert was happening to the in memory and then only it was going and the reads were fed from the cash itself it didn't go to the actual disk or the uh database so that we fixed but then we thought like it could be done even better and we had a very long debate there was even a solution by AWS called inmemory DB which provides you with the persistence and the red red is with the persistence uh we compared it we compared the cost and then our client took a call that I'm fine with losing one message sometime in rare case but I want to save cost so inmemory DB is much costlier than having a simple uh red cool anything else hi uh YH I had a question yeah uh I think earlier you were talking about concurrency uh point I completely was not able to gather it uh could you please uh explain that part concurrency what uh there was a slide just uh during which you were talking about concurrency uh like I think the second half of the presentation if I'm not wrong yeah I don't have the number of this SL as such so I think uh do you want to redo the same thing I said because the uh you will get this recording if you want to know about it you can go through it I didn't uh the thing was I didn't understand the scenario yes uh yes this point I I understood about the lock uh that you said okay which we use so that uh you know we can handle multiple operations which happen concurrently okay but after that you were explaining uh something related to how you handled it I think I didn't follow that okay this bit I think I did it lock free and then these are the points on how I did it uh I think we are running out of time that's the thing I think I can't explain it again but maybe go through this thing you can ping me on LinkedIn uh I can I'm happy to explain to you okay thank you appreciate it thanks for that any other questions cool thanks everyone thanks for joining