okay uh i have one i'm audible yes hey hi guys welcome to the dev day uh those who are joining for the first time uh just quickly introduce what is dev days so dev day is a monthly formal event for developers to share their experiences ideas and opinions about the technology we organize we as in sahaja organize this event monthly and quite frequently twice a month also uh we regularly update our meetup links uh on on our meetup pages and you can subscribe to those pages and follow us to know about more technology and more about the day of day so today we are going to talk about building a search engine for interactive search we have kishore nalan who is subject matter expert in this area he is a co-founder of type sense is it is an open source search engine optimized for speed and developer productivity uh i will hand it over to kishore now uh thank you hey hi thanks thanks for the introduction so let me share my screen and then get started can you see my screen yes so um yeah i'm kishor um i'm co-founder of type sense um uh we with the way we present type senses as a open source alternative to algolia or easier to use alternative to elasticsearch that's the way we frame it um i'm mostly going to talk about um how why we built type sense uh but from a technical angle in the sense uh what does it take to build a search engine for interactive search because that's what uh type sense is um so i want to basically take that problem statement and kind of derive from first principles like how we ended up with uh the some of the design decisions some of the trade-offs are with type sense and how how one thing led to the other right so uh very quick uh introduction about myself um um so typesense has been uh started off as a nights and weekends project uh working on it for almost seven years uh but uh started doing it full time two years back um we have a uh cloud hosted offering that's how uh we pay our bills uh but it's a hundred percent open source uh product uh not open core fully open source you can run it on your own infrastructure or you can use our cloud if you wish to um um if you wish to just host it not want to watch it yourself and so the problem basically began in my earlier companies right as with most ideas you scratch your age right so um my previous company i was sorry at zapier uh before that i i was the engineer at index and thoughtworks and um and search and my co-founder jason he worked uh he has also worked in this uh with different search technologies he has been in a couple of e-commerce companies so where searches are very much of a uh a revenue driver um good search really can like push the revenue up so we came across a lot of problems uh with search which we felt like the existing solutions uh which is primarily elastic search and solar um really didn't uh really address in adequate terms right um so what we really wanted to do was uh democratize search in the sense uh we wanted to make it available to teams of all sizes uh by you know reducing the search engine learning curve and you know if you're bringing a new product to the market uh which is uh which which is very interactive in nature which has a huge search component uh we just want to make it like really easy to do that uh because we felt like um the existing solutions were uh quite difficult to do uh quite quite difficult and and what we are essentially doing is we are taking something that elasticsearch does uh broadly and then unbundling it for specifically for um interactive search and uh uh and and uh like type ahead searches autocomplete and so on so um as you all know search is everywhere right we we are producing a lot of content consuming a lot of content and um and it does almost be become like a navigational element whether you look at mobile apps or e-commerce or you know um sas apps uh it is it is like a navigational component people want to like use the search bar to really like quickly cut the uh cut the clutter and then go directly to what they are trying to find right and search is not just a search as a as like you type something and you see a bunch of reasons it's a lot always a lot lot of interaction um related to that right so if you go to e-commerce store you're going to have this categories listing you click on it so you're basically filtering and searching at the same time so it's like a browsing plus interactive navigation kind of experience so that's how that's a trend that we are seeing that's how our apps are being built and uh building this uh today is still very difficult right and i'm going to talk about what are the problems and how we are solving that with typicals right so uh we will probably look at two areas one is the speed uh because we want to do interactive search experience so your goal is to be um delivering results especially for type ahead search within maybe 50 to 100 milliseconds anything faster than anything slower than that is going to be very perceptive it's no longer going to seem like an instant search experience for people and uh and the user interface how do you build this whole thing like because if it's a static vendor it's much easier to make an apa call you have a template um the page refreshes and you have it but for when you're doing interactive search so many things are going to be changing on screen um and how do you do that so before i so let me begin by first um showing a demo right of what i mean by interactive search so that uh we can you can get a picture of uh like what i'm going to talk about right so here is a simple e-commerce store and it's powered by type sense uh so when i mean the interactive search i mean this right so when i'm typing samsung the page uh i don't know how interactive it looks on this uh on the on the videos share but it pretty much uh just uh works and you you can see like as i type when i delete some of the words you can see the highlights here changing it's only highlighting what you're typing and then you can like you know quickly drill down remove this and you want to search within a brand like something like this you can search that and that also gets highlighted um filtering by pricing and so on so this is what i made my single page interactive search um something similar will be like a recipe such as another demo so uh searching for pineapple um sugar pineapple and things like that and you can see like the it is taking like 50 to 64 milliseconds on like a 2.2 million data set right so this is what i mean by interactive search uh so coming back to our presentation so uh the speed is one huge uh challenge and the second one is because so many things are getting updated at the same time so uh like really writing uh all of those interactions uh are going to be difficult unless we have some form of framework to do that okay so and as far as the speed is concerned you you have to deal with network latency which is the round trip time from the time that you press your first keystroke and you get the results on your computer right and the query latency is what actually happens on the server uh how the results are fetched how they are ranked and uh how they are sorted and given back given back so that time uh that is mostly an algorithmic uh component there which is a query latency also i'll talk about i'll begin with network latency what are the challenges and then i'll move on to the query latency and then uh we will in the end we will look at the user interface or challenges at the last so uh we let's begin with a typical search infrastructure so you have a user uh typing the query running shoes um and uh that is exposed to uh like that hits an app server it could be django or rails um and then uh it goes to an aggregation server uh which is basically a search cluster let's say it's elastic search which you will typically have like an aggregation or notes and then you're going to have the data nodes right where the actual search indices are kept so your query goes from the app server the app server makes a call to your uh elastic search api which is hitting the proxy or the aggregation server then it is going to talk to you one or more uh search node uh search nodes get the data back re-rank them and then send it back to the user okay this is how a typical search infrastructure looks like so now look at let's let's say that you know the user is in london and your server is in oregon right and um and for most uh people um data stores are typically kept in a single data center even though we have edge now and that's mostly for cdns uh for serving static assets um or like you know serverless is also against services uh but the raw data whether it's your database or your search uh it is still a difficult problem where it is usually confined to a single data center so let's assume that's in oregon or if you're on aws virginia or one of the aws zones and uh and for most web applications the users are going to be global even if not 100 of your users are very widely spread out even if you're primarily used focused 20 to 30 percent could easily come from europe right so you have someone from london uh typing running shoes uh that is going to be 100 millisecond round trip just on the network latency to our data center in oregon uh and then even within your data center uh if you're from from your app server you're going to hit the uh elastic search proxy node and then that talks with its data shards a short data nodes and that is going to be 10 to 15 so you can see all these network hops right they are adding up very quickly if a goal is to serve uh instant search in 50 to 100 milliseconds and that network times uh the netwo cops themselves kind of eat up your entire budget right and we are not even i haven't even mentioned nginx usually you're going to have an nginx in front of your app server even though it might be inter process but it's still it's a it's a it's an additional overhead and then you're going to have a load balancer because you're not going to have a single app so you're going to have a bunch of form of app servers so the user is actually talking to a load balancer which i can process it and so on so as you can see it's a lot of network hops so what can we do like uh to reduce this and you know to like kind of take a handle on the network latency part so the simplest thing is like you know i don't want to talk to an app server let me talk to the elastic search or the search uh cluster directly right so i bypass the app server so i will bypass the load balancer but but immediately you are faced with this problem of in a multi-tenanted environment how do you handle acl how do you handle authentication so let's say you have um search records from multiple customers multiple organizations let's say you belong to an organization and all all of the data are stored on a single search cluster so when you search for something you want the search to be scoped to only your data only your organization only your records so um elasticsearch or solar don't have anything out of the box first to solve that because you have to solve the authentication and much more importantly when you are exposing uh your search cluster to the internet directly for for for you to talk from your web browser or your client uh you need a production grade http server or some form operation create tcp server there right uh it because you do all your data is going to be there it needs to be patched up it needs to it needs to be um it needs to be secure um so you run into that problem so let's say that you know um let's say we solve some of those issues uh somehow and uh yeah shall i ask a question yeah sure go ahead so in this case also like the the person who is running queue is equal to running shoes has to go to the network to identify the http server right um so you will have a dns uh it will be http server yes um so i so i didn't get your question but yeah you're right you have to go through the http server but what here what i'm saying is you can skip this you can skip the app server if you if you're able to talk to your search server directly my question itself had a api yes my question was uh as i as a user i let's say i'm searching for my system uh to talk to that let's say a server any server so http server or something then the network has to happen right yes the network has to happen yes that means that 100 millisecond latency is still there right right i'm not done yet i'm i'm going to derive let me finish this let me finish through this i'm going to derive it so so what what we're trying to do is it's an exercise and seeing how many of these middlemen and we can write that's going to be what we're trying to do so let's let's i'll show you so let's say that um you you don't want to talk to the aggregation server also because that's as you said there's an additional network hop there right you which which has the uh the search index that you that you have you want but that means that you're not going to have uh you can't actually start it right and uh that um parts and you know fetch the data for you but you do reduce the hop by not uh going to the aggregation server and going to the data chart so let's assume that is fine let's say you have a very small uh let's say you don't want to shard uh and you want to talk to the uh data node directly right but still as you said there is still that form there is going to be uh that whatever that data index wherever it is as long as it is in oregon you are going to have a network latency from london going uh making a call into oregon right so what if he what if we have like a geographically distributed thing because because it's hard physics you if you want if you want to cut down on the network latency you have to move the data close to the user as simple as that and that's how cpdns work uh edge networks work uh but let's assume it's not a true edge network you're just distributing data to let's say four or five uh major uh continents and so that uh you know you are putting it close to the user um and uh like for example let's say there is a there is a data center this is a node in europe right and uh and so that work so the customer from london will be sent into europe so that will reduce the search latency to maybe 20 30 milliseconds depending on where exactly it's there in europe and um but we do have need to have a fallback because if that node goes down you need to be able to hit one of the other nodes it will be slower maybe we will hit the load in asia uh it will be a bit more slower but it's okay because that is going to be a temporary mitigation as the uh the main node comes back the european node comes back up you're going to go they're going to look at some other node right so so so unless you move the data close to the user you cannot really solve the network latency issue uh even if you solve the other issues that you that i mentioned uh about like authentication production grade http server sharding and so on so so if you look at the search server requirements right if you want to really do a search uh without having all this middlemen so you can get the end-to-end search within 50 to 75 milliseconds then you need to have a protection grade http servers for your clients to talk to directly you need fine grain acls for client authorization i should be able to generate an api key that says that hey i can use there must be a filter embedded into it like for their organization id x right and the api key itself must bind to that they should not i shouldn't be able to tamper it since i'm making calls from the client side uh and it should be able to be fast even without shorting right um and it should be geographically distributed because you have to move that data close to the user that's the only way to like reduce network hops and uh lastly um since you do not have any load balance of load balancer or any aggregating servers you have to do client-side routing and client-side failover so uh in that previous example that i showed if the europe node goes down the client should be able to fall back right so you become client heavy because you're cutting down the role that the traditional load balancer you will usually play uh in like creaming bad notes or rotating or bad notes and putting back the good ones cool so uh so type sense implements all of this um so uh we worked backwards and we felt like you know if this is what we need to do then uh we we need to have operation here http servers we need to give great acs and we should be able to design the server uh without being without us needing to short it so we will just duplicate the data so the same data x is going to be here in all the regions there is no sharding the data is not going to be split across machines it will be a mirror replica of the data across the nodes and um and uh and since you do not have the benefit of um horizontal sharding you have to start vertically right and uh and uh and you need to be able to do like a geographical distribution so uh this is this is a set of requirements we kind of arrived at and uh with type sense we we we implement all of that so let me talk about uh some of this and how we how we implement or make some of these things right so uh we use raft for clustering um and uh if uh if you're not familiar with draft it's a way for you to ensure that a bunch of nodes are in sync um and um let's say you're storing data and uh and you have like three or five nodes uh you you write data to one node you want all the other nodes to like pick that up if any node dies in between they should be able to come back uh and be in sync right and uh raft is a protocol that helps you achieve this consensus and uh the way raft works is they uh every cluster has a leader that is elected so let's say you have three notes the notes will elect a leader and uh and all rights will go through a leader so your rights are serialized you cannot have this split a scenario where two notes accept a right and one note goes down and you do not know which right should come first right it's a very strongly consistent uh it's a protocol that ensures strong consistency um so rights go through a leader and then you have odd number of nodes so you can do a quorum so any any time a write comes uh you need a quorum which is the majority of nodes to acknowledge it so that uh you you can persist the right so technically you can run like even a seven node or a 9 or 11 node cluster but because you have this requirement that a majority of the nodes need to take part in a right acknowledgement uh it is going to slow down your rights so uh typically three to five nodes is what you will run uh you will want to run more than that um and also raft will uh a good raft like we will talk about this in the next slide will allow uh will support like fault tolerance and automatic recovery and so on um so so what type sense uh uses is a library called draft by baidu uh because type sense is written in c plus uh and uh so we use graph which is a c plus plus library there are other libraries uh in other uh there is uh there is one in uh go by hashicorp raft um so the way it works is you have this it's a state machine basically the whole library is like a state machine you have these different statuses like follower candidate and leader and then you have these interfaces or functions that you need to write um and uh once you write it uh it will um it will it will you just have to implement the those functions and uh it will you will be able to um implement the entire classroom so if i have to show quickly type since code um so you you just uh this is draft um which is the library using you just extend your state from a state machine and then you just have to implement a bunch of functions like you know uh what happens when uh when a when a snapshot is called so how do you snapshot your state whether you want to store it to another directory on your disk i mean another disk on your machine or you want to store it into s3 i will just implement do snapshot so the library will call that when it does the periodic snapshotting um then if you want to do like you know what you do when you want to load the snapshot or what do you want to do when the leader starts what do you want to do on shut down um and so on and so forth so you just have all these hooks uh you implement all of this the the the most important one here is the apply so which is actual right so basically you get an iterator of data that is written uh to the cluster so how do i persist it i could write so i just need to write this implement this particular function and uh and raft the live and the underlying graph library will take care of um see calling the orchestration and doing the synchronization and so on right so if you are um so if you want to build something like this uh like a clustering solution um or a raft is a good choice and uh there are like mature libraries out there that allows you to um like just write a few implementation functions and you get you get the you get the entire clustering working um so the the so what is what is the price that we pay for the low network latency because uh nothing comes with nothing comes without trade-offs right so we have chosen so since our goal as a search for type sense we have chosen specifically to solve for interactive search experiences which is like uh uh which is like a a fraction of the all kinds of search uh uh search use cases that you have your blog search which is which you do not need to be interactive there are like like their searches on like journals and so on which you don't have to be interactive but we have chosen to solve a specific problem that elasticsearch and the existing search solutions are not great at so we are so what we like to call is we are unbundling elasticsearch right so we are solving a specific set of problems that it's not great at uh out of the box so so that so we have to make some affirmative so then the trade-off that we make for this is that uh we cannot short data across the nodes because the minute you have like two nodes that you have a query the slowest of the two nodes is going to uh it is going to be the binding factor for the total response time if one of the nodes is slow uh and networks tend to be a lot flakier than disks so you're going to be stuck on that so and there's going to be high variance in the response time so we will only have one data they're not sharding it so the minute you say that you're not going to shard your data across nodes that means you have to be doing vertical scaling um and uh you have to do in memory indices uh because uh you you have to be as fast as possible um and uh and and all both of these things naturally mean that you're actually focused on uh solving for like searching about 100 million records because um let's it's today you can rent a uh cloud instance on aws or um google cloud like that has like a 500 gb ram or one terabyte ram that's uh pretty easy to do and that usually we had customers index 150 160 even 200 million records depending on how large your records are um into type sense but this is the scale that we want to tackle 100 million records and and when people tell us like what happens when i have a 10 10 terabyte data set that uh i need to index then uh what i what i usually tell them is uh it's like the access knife as the analogy i gave them uh just because both your axe and knife are sharp uh you wouldn't you don't want to use axe to chop your grid right you don't want to you don't want your axe to slice your bread you want to use a knife for that so uh so we choose our battles uh we are focusing on up to 100 million records uh which is like vast majority of the use cases if you really need uh something more than that uh um that is probably not the best fit for type sense and that is okay you don't not every software needs to work for like zero to like uh brazilian records right so it is okay we choose our battle uh so we spoke about our network later like tackling network latency and what it what what are the implications of that means right in terms of the design decisions and so on uh let's now look at uh query latency right um so at the heart of every search engine uh regardless of uh uh even if you take type sends or even google uh the the way search engines are implemented is through inverted index right so let's say that um an inverted index is basically a mapping between you can look up you can look at it as a giant hash map uh the hash map is between the words that are in your corpus and the documents that they occur in and uh additionally you would also want to store the positions that they occurred in the document like for example uh the word apple appeared in the 15th as appeared as the 15th word in the document and so on but i have omitted that for simplicity let's just take that you have a word to document id listing mapping right um and when a query like washington apple comes uh what you would what the search engine will do is it will go look up apple look up at the list of ids that uh it occurs in and then it looks looks up washington looks set looks up the list of ids that that appears in and then you do an intersection you do an and uh and when you do an and you get the resulting documents that contain both the words so you know i've simplified a lot of things here uh but on the very high level this is how our inverted index works um broadly right um but for for i have a question if you don't mind yeah sure so you were talking about you support up to 100 million records uh and then i'm assuming it's scaling vertically and all how much memory does that typically take in in yeah so so a memory so it's a so from what we have seen from from our customers uh you could say that it takes about two to two and a half times your disk storage but that's a very rough uh stick because some data sets it really depends on the shape of your data because some data sets compress very well for example let's say i had a million documents and uh i had only four unique words in the entire million document set let's say most of these are identifiers and they like for example gender things accompany and low coordinately low cardinal cardinality fields that appear a lot a lot of times right then it's going to compress very well because um you're going to have like a word and like list of ids that they appear in so the token data is going to be determined the total memory usage is dependent on the ids right uh and they compress very well numbers components really when we compress them so that is uh that's how it works but if you're if you're talking about like indexing giant text large text um then two to three times on disk storage is what we have noticed and again if your data has a lot of numerical fields uh again it will be much smaller than text fields and um and um and a lot of times people have a lot of numerical fees numerical filters to that goes along with their text so it's a lot of times you'll have a single text field and then bunch of five six filters and sorting fields that you want to apply on on top of it to slice and dice the search so in those cases it is not so bad thanks okay so um so yeah so this uh so you so you look you look up the words and you intersect it right so this is how like a really simple search engine work but that's not enough for uh interactive search right because uh you want to do prefix searching and you want to do typo correction uh because uh when people will make mistakes uh as they type um if you have to tell them like google hey did you mean x and for them to click on it and correct it again that will not be truly interactive so you want to be able to correct the typos as you as people type i know as well as be able to guess what they are typing so you could you might even have to do a typo correction on a prefix so let's say samsung and i'm typing s-u-m-s-u it's obviously not samsung's first four letters uh but you want to be able to do a prefix correct uh typo connected prefix search right um and and um and both of these contributed relevancy uh which is basically what you want you want people to find what they are looking for fast so but how do you do this right how do you i'm typing a pp and you want to guess uh that it must resolve to apple and when i'm typing uh washington with the extra h there uh it must resolve to washington right at least with typo correction you can kind of brute force it uh there's a very uh popular post uh like that comes up on hacker news often by peter norwick uh it is called peter norwick spell checker um peter novick boarded a plane and he was bored and uh he wrote a spell checker um in python which is basically a brute force uh spell checker where you have which we just index all the uh words in your dictionary and then someone types a word with a misspelling what what he tries to do is just or tries removing a random letter replacing it swapping it and then try to find uh you know uh the the typo the word with the corrected typo in the dictionary so that's that's one way to do it but it's very expensive and it's not going to scale uh the other way so but for prefix search you really there's nothing much you can do that you can't even do there is nothing to like you know you can't jumble up the order of letters to actually get anything there so um so so a hash map i said you like you can treat this whole thing like a hashmap i joined hashmap that's not going to work right so what you really are looking for is tries uh tries will help us solve both the issues it will help us do typo correction and it will help help us do prefix uh lookup so let's begin with prefix record because that's the easiest so let's say my i'm trying to uh my i'm trying to type d o right so you will you will have this giant uh try where all you all the words that are there in your corporate corpus are indexed and uh at the leaf you you will have the actual word and then you will have the document id listings that i uh that i uh that i mentioned here right this document id listings will be at the leaf here so uh so somebody types d o for let's say a prefix completion for t o so you go to d then o then you will you will be able to do a dip breadth first order first traversal here and then identify dog and dot as your uh two words that you want and then you can pick the documents from the that are that contain dog and documents that contain dot and then you then you rank them or filter them apply those whatever filtering or sorting conditions that the query has and then return the results and uh but what what if the word has a typo let's say i'm trying to find the i'm trying to query for fire fire but i end up typing it f typing it as f a i r e so um what you can do is you can implement a levenstein uh distance search on try uh pretty efficiently uh so you you will come to f and then uh you you will try to go to a and then you try to go to i and then you find out that there is no i so you will backtrack and then you will try and find that oh there is no uh you i i there's no siblings to a then i go back to f and then again i come back to i and then i correct myself to find fire which is within one typo away from the word f-a-i-r-e so it turns out that um if all you're looking for is one to two typos errors then uh doing this on a try is pretty fast um you can you can because you are you're just especially if the typo is not on the first few letters because your try will start off as very fat and they become very thin pretty fast um so and generally people do not make mistakes on the first couple of characters although i have done that here but [Music] in general the performance is not too bad you can you can traverse uh very quickly and you can backtrack and you can do a levenstein distance uh uh searching on the try which is what types type sense implements we support uh deletion uh addition of a letter substitute substitution so for instead of f i r e if i if you type f r i e will be able to find that um we are able to find that very quickly on the try and identify the candidate words and then get those documents and uh rank and filter them i have a question yeah uh so basically what you're saying is for all kinds of mistakes or the spell spelling mistakes the there is an extra note that is getting added in the tribe drive no no no uh this is this try is not going to be any mistakes it's it is it is going to have your actual words so in for example let's say uh your corpus had only these one two three four five words right dog dot pump fat and fire you're going to build a try out of it right and when you type the query right fire let's say that queries if i are you fine then all you're doing to do is go to the root go to f go to i go to r go to e you're done right but what happens is the if there is a mistake like there's a f a i r e so you go to f and then you go to a and then you realize there is no i there's no i there's only t right so what you do is you will backtrack uh you you will backtrack you go up and then you try to find another path where so what you will do is you will go back to f and then you pick i right and you will have a counter that will i'm very simplistically saying this it's a pretty involved algorithm but you will have a counter that you will maintain on the on your stack that says that hey i'm at i uh but i have actually skipped a so that is like one type of error right then i can go r e and i find out that hey i'm able to get to fire with just one type of error but let's say there is something else like they say this instead of r there is a x here f i x and then i will do one more counter i will say that hey i have now done two typos if i and then if i find e well good i have done one or two typos i can still find uh the word i can get out but let's say there is on one more f i x w or something like that the minute i find the third typo i realize hey i cannot this is not working out i will i don't have to go through that part of the tree anymore i can go back and quit so uh if you do more than two typos it becomes uh really expensive but one two type two typos uh you kind of uh it is pretty fast uh we can you will hardly you you can i've done like on millions of words i you get like two to three milliseconds for to do single or double titles it's pretty fast and this dictionary uh or the try tree is created based on the all the information yeah all the words all the words in the uh words in your uh corpus let's say uh let's say the only word in the corpus is kishore and then that will be the only word in the try but if there are like ten titles uh you will pick all the titles all the unique words will go into the tribe okay hey hi kishore i i have a double yeah yeah um so like let's say like uh so this is for a per word right let's say for let's say i'm uh typing i want to type dog food uh space in between and then i miss misspell dog into dod and then food right uh so dod can we can either go to d o g or d o t right but the food the next word right that adds the context and then like probably with that you can look back and correct it right so that contextual correction is is it possible or uh yeah so uh yeah it's a very good question ah it's a it's also a very difficult answer because contextual correction will mean uh you will immediately have to go into uh if you want to do it properly it is better to do frame it as a machine learning problem um and uh like let's say it's like transformers and uh like semantic uh thing you should be able to get uh you will be you need to have a weight of a word in context of those surrounding words so that's what you really want uh then it you have to frame it as a machine learning problem uh but what we just do in type sensors we do a brute forces and since everything is in memory all your data structures in memory and we implement like really performant intersection routines uh you can get away with brute force uh search let's say you and generally queries are especially for again our for the use cases that we are solving like e-commerce and and uh like app searches and you know sas where you're looking for people's names people's email addresses phone numbers they're all like typically very short queries i am not going to go to it will not be like uh like a google query like where can i find x right uh like that's it's going to be very short and pointed queries and for those cases uh two to three words we just group force it so we'll do like uh we'll start with we'll assume that there are no typos so we'll do direct lookup of all three words if any one word is not found then we will what we'll do is okay we'll do uh all type of variation one of word one uh with zero of word two and zero four three then zero one zero two one one and so on so it's a brute force right uh but yes it's fine it is uh generally um it work it has in the real-world data if you of course if you benchmark it with artificial data it might not come out very well but in real-world data thankfully uh uh it is it is it really works and uh yeah yeah and it avoids having to build like a complicated uh model and uh and the semantic world has its own set of problems if you have a lot of uh your models cannot be updated every day if you so if there's a new record that's get added with a new unique word you will not be able to find the context for it so there are different set of problems for that um but for for the large guys right for example amazon you will notice that they don't actually use instant search that's because for them the value that they get out of machine learning and the contextual stuff is more important than the ability to spell correct right so um they so they they don't do an interactive search but for like for all our other use cases that is that requires a lot of bandwidth to pull off okay so is is this part is it like plug-in like is that like if i want to build on to this correct let's say for relay let's say i want to plug in some ml model just to you know so is that feasible in titans as of now not right now but we do have plans to introduce a vector data type uh because at the end of the day uh all the models all they are all they're doing is doing nearest neighbor search on two vectors so you will have a query vector and you have a bunch of document vectors and you learn the similarity between them during the model phase and then during the search phase you are trying to find uh given a vector try to find the most similar documents that's that's the crux of how the search is at least search is implemented with uh transformers and like other ml libraries neural network libraries so we are planning on introducing a vector data type where for every record you will have its vector which describes the which describes the semantic meaning of uh the words uh maybe this is another entire you could probably talk about today talk fill an entire talk with that uh it's a very interesting area but yeah we once we have that then you can mix both of them you can you have the best of both worlds so you can have like a keyword search and uh apply a semantic layer on top for uh ranking or if you could do a semantic search and uh like not do keyword at all you can choose it as per your business but yeah so tries are uh uh look really good on paper but they are uh they have a lot of drawbacks uh navy implementation tends to be very memory intensive and since type sense is already memory bound in the sense we store all our inverted lists like document ids position listings all of them are in memory uh if your tries will be a really really even today they are the biggest or the fattest data structure uh that we that we have in terms of memory usage so a name implementation let's say you are indexing every byte in a node let's say every letter every letter is a byte uh two power eight is 256 uh uh you can have two fixed values although ascii if you're looking at english you don't have the entire 256 but you want to go into unique audio indexing uh chinese japanese the unicode characters are they they they can very uh they can have a larger range so every uh navy implementation will be like having 256 branches for every uh node uh and then that means that you know you're going to uh be very memory hungry and each node is going to be a pointer and that means 64 bytes so uh enabling implementation is very slow and it's also not very um cpu cache friendly uh because let's say you're doing this try travel so you'll be doing a lot of traversals as i say i say explained earlier when there is a typo error when you realize that you know you don't you're not able to find the exact sequence of characters that the query has the words the characters in the word uh in the query you will have to do a back traversal and come for like move for forwards and backwards right and every time you do that you will have to jump you will probably jumping to a different part of the gram uh for example b and p they they are adjacent in your try but they will be in totally different parts of your cpu ram so there will be no cache locality so when i retrieve your d i will not benefit uh from p being close to close to it it won't be right it'll be in a totally different part of the ram because you'll be like doing a allocation of this p independently of d so um they will be very far apart so we cannot uh so it will really slow things down right so thankfully uh we can do better there are there is there are like a class or family of data structures that kind of improve on the radix tree that x3 is a tree with like 256 leaves that's the neatest implementation um uh for example you the one that type sense uses is called art it's called the adaptive radix tree uh this you can find the more details in that in the paper uh uh i have it on screen um so it does a few uh tricks uh for example it does path compression what compression is when you have like in this case pump pump like without any siblings anywhere so you don't want to like just have four nodes for each word each letter so you can just compress that word into a single node and uh and you save one like the memory usage for all the other nodes right and uh you also have adaptive length so instead of having 256 uh children for each node regardless of whether you have it or not you will have adaptive or length um again you cannot have like a random length you cannot have like things like three nodes or five arrays of sizes of three or seven or nine because you want to be uh you want it to be cache friendly and cpu friendly so uh what art does is it splits the nodes into like sizes of fixed sizes of 4 16 48 and 256 and so it it kind of fits into the cpu cache and they use some tricks to be able to like apply a cmd instructions uh to to do traversals and do to do like lookups and so on um and uh so again it's uh in the interest of time i will i'm not going into the details but uh implementing this and implementing uh we so um um i found a library which which was open source which implemented a basic version of art uh uh on and we we had to implement the whole uh 11 strain distance uh typo tolerant traversals on top of that so that was something that we did and i i i i'm not aware of like another open source implementation at least that has that that has implemented 11 strain type of connection on top of a tribe one question that uh the correction algorithm on top of art yeah is it open source from your end yeah yeah type sense is completely open source uh if you go to github.com type sense slash type sense that's uh c plus plus repo uh the file is art dot cpp you can look into that awesome thanks um yeah uh maybe you cannot go into the entire thing uh right now but maybe i i do not understand how this can art can make it uh cash-friendly can you explain that a little bit right right okay cool let's say let's uh let's come back to that uh this this particular example of uh this type of correction right so um let's say i am looking for the word um let me pick uh something else um okay let's go with this word itself r e right so the minute i come to f a right and i realize uh i so these two in in a in in a in a classical fry implementation the the the letters a and i will be part of two different uh differently allocated uh nodes so if you look up um if you do if you have for example if you have a struct as each of the node uh a and i will a and i will be like uh they will the indexes into n i will will be created using a new operator so they will be in different parts of the ram uh what r does is they will have a care array where a and i will be right next to each other so when i when i go look up fair um the way you're do where you will do is you you will always be greedy the way the implementation limit strain implementation works is you will always be greedy even though if you you when you find f and a even though you've found a you will actually look at i also because you might you do not know whether a has a a is a typo or not and in this case it turns out that a is a typo because f a and i'm not able to find i so in the actual implementation of the levenstein distance what we do is we will look up at a and also look at all the siblings of f because we do not know whether the next letter is a typo it will lead to a the correct word or is a type or they're also missing letter right there right so when you do this uh uh lookups uh having these characters right next to each they really speed it up otherwise you will be doing a pointer indirection uh into the into into the sibling character which will hit a different part of the memory uh this is again uh explained in the paper uh um so it is it is it is not so much of a problem if you're just doing straight up lookups uh where if you not find the next character you just bail out it might it won't help but especially for uh like uh like fuzzy lookups it makes a lot of difference uh yeah thanks good so um so so with with all so we spoke about the network uh latency part how we solve for that by not having short and going uh to a single node and and and you know having a geo-replicated draft cluster to take care of the high availability part uh and then we spoke about uh the data structures that it takes to for you to really do uh quick uh prefix lookups and quick fuzzy searching and uh there are also a lot more other things uh that we do i have not spoken about uh filtering i have not spoken about uh sorting um um and these again a lot of hand crafting has gone uh micro benchmarking and tweaking the code has gone into all of that so with with all this uh uh set of things you are able to get to sub 50 millisecond results uh with millions of records so that's what we aim for um for a vast set of use cases this is excellent for mobile apps and so on you are able to get like a really good interactive search you you when you make a search you don't even realize whether that request is going to your back end coming back or it is being stored within your mobile phone um and and your and you're just getting it from there so um and i i actually showed these demos uh like earlier maybe i can maybe do this uh jio search so like this is again uh we also support uh geographic uh datasets so let's say this is some airbnb data right um again you can you can look at different uh points look at uh like you know things with heating things with kitchen or whatever and then you can do like bits due to 16 and it instantly updates so this is again a kind of a use case that you can build with uh type sense uh because instant search is not just about just the keyword lookups it's also applying filtering sorting and faceting this is called faceting you're breaking down a property type into like the individual values that occurs in the data set and doing counts on them so you can very quickly see which are most popular and then that helps you decide right so um so this is another demo uh i've shown the other two demos here and then uh this one we built this demo as a like uh for the linux kernel that crossed one million comments uh earlier i think last year uh last year so we built this site for that so you can basically go and search all the commit messages you can just like for example check floppy what what people are writing about that um race condition and and so on right and you can see okay what is how much uh what is linus what has written what is the commits that linus has written about race conditions and so on so anyway so this is the this is a this is another demo and uh it is another thing that will be really fast uh some weeks of 15 milliseconds end to end and and and the reason this is um uh like fast is also because uh uh uh because of our tax sense uh i can maybe quickly show you uh our cloud but it's really there's really nothing uh different than what is whatever is available open source so uh if you look uh for the the demo that uh is being hosted it's uh it it's in these five regions sydney sao paulo in frankfurt mumbai and anytime i i search here it knows that the client library knows that i am in chennai and is closest to mumbai it will route me to the nearest uh uh server so that that logic sits in the client and we have to implement it it's only javascript that we have to implement for um and maybe ios um and uh that that logic will that helps us uh find the closest um server for server to like serve the request so uh our household cloud offering like allows you to do uh like do that very fast but really there is nothing stopping uh you from doing it on your own infrastructure and on your own set of servers um so that's that okay and let's get to user interface ah this is like the final section of my talk um it yeah yes as i said earlier like if you are doing just static pages or maybe even a static search where you have to type something and press a submit button and a page renders that is significantly easy to build but with a user interface you saw how many elements were jumping on the screen right as i was typing your first accounts were changing your filters were constantly getting updated and your results were getting re-rendered as and as how you typed so it's really difficult to build this and um so thankfully we uh so let's look at the elements that are getting updated on screen as i'm typing here you will have to update this facet columns uh and accounts associated with the release date and you know 1970s all the accounts associated with it then you will have to render this uh like i want to search within artists so you have to render that and support searches within that uh then you also need to do pagination show the results found and then you have to do highlights uh like i have typed only like in this result or wonder wonder wall or cover only the first two letter words have to be highlighted because that's what the key what the query has so um so you have to do all of that so it's not easy right so what we have done is we have integrated with a library called instance.js it's actually uh interestingly it's from our competitor called algolia uh but it is it's a great library so we don't uh uh we don't have any uh regrets in integrating with them and it also helps uh their customers to like uh migrate over uh because it's easy for them to like because we integrate with the same front-end library so uh so i wanna show you like this e-commerce demo that we saw right this entire thing including all this uh sub filtering of parents and like pagination and all that stuff um uh it's only about 250 lines of code uh front end 282 to be precise because you have like a bunch of widgets that instant search offers like search box pagination refinement list hits is the list of results that you see uh slider and ratings and so on so you just initialize type sense you give a list of notes uh you could be one node or it could be five nodes if you have like a geographically distributed uh setup and then uh then you just add those widgets right you say search box is going to be in this container and you have a bunch of options what you want to do with it like placeholder and so on similarly for pagination and all the other things you put all of this together and you get a you get this and you just have to apply some stylings you can get a very nice this is like the rails crud equivalent of search um and uh and exactly the same code is about 250 to 80 lines powers this powers this so you can imagine how fast you can get running let's let's say you want to give a prototype or a demo to your customer like for a new feature you can like really quickly build it using instance.js so you don't have to manually deal with all these state transitions like what do i do when the slider goes back and forth how do i update all these things and maps and all this you don't have to like uh go through all of that so um that's that and uh yeah if you uh love to have more contributors like uh uh the core engineers in c plus but um uh uh we have uh libraries in a bunch of uh languages if you uh wish to contribute for languages that uh it is not there uh we love to like uh um you know have your help like so we support these languages i think there's also go but it's not here in the documentation yet uh but yeah we have a nice uh clean api you don't have to unlike elasticsearch they don't have to go through hundreds of pages thousands of parameters it's a good swiss army uh you know it's a good swiss army knife it can do everything you want it to do you can make it bend however you want it to then you can make it flow how you want it to flow but for the use cases that type sense targets are uh it is much much easier much much faster to like you know to just uh do it like you you it also supports this it also supports keemon so it's strongly tight if you want uh we have also have auto schema detection and so um you can mix uh with the best of both worlds if you want some integrity for your uh data through types you can do that or you can also have like a automatic schema detection and so on so um yeah i think that's it from me uh i will stop now um uh and open to uh any questions uh thank you sure how do we warm up this data so since everything is uh in memory it's like radius we have a snapshot of the raw data so when you index let's say like a million documents everything is the raw documents are stored as they are uh in json on the on the local uh uh we use rocks tv as a key value store um and uh and on when the instance starts up we re-index it back into memory and that's another reason why i said we support only like uh you know 100 million 100 million ish uh data sets uh because uh you cannot do this like if you have like a billion or like if you have like 10 terabytes you cannot even if you've found a provider who can give you 10 10 tb of ram you just starting the whole thing up when once it if it goes down or during a version update is just not possible uh hi so thanks thanks so much for the stock it was really enlightening so one question i had is uh uh what about indexing and indexing throughput is it uh built for a certain kind of frequency of throughput of indexing or and not for frequently updated data like what's the build yeah yeah it is built for like extreme high in in indexing speeds because one of the problems we we constantly encountered is elasticsearch poor updates right they really lag behind it's really tough and with all the shots moving all across uh like getting a cluster uh green uh is very tough uh i managed a 10 terabyte uh uh was it tender or 50 terabit something like that uh humongous cluster uh it's snappier um and uh and it was really painful watching those uh because because uh ultimately what uh leucine is a immutable uh uh library so uh the way elasticsearch have implemented them as like immutable segments you have different segments on this so everything is an append and when you want to do an update it just takes the entire document updates the field and writes it back and then in the background they join it uh which is really slow and uh um and that's one of the reasons why the updates are very slow but in type since uh again there's a new data structure for that um we call it as unrolled linked list uh which we use to like really speed up uh uh your uh updates uh which was implemented only in the last uh version of typesense uh until then it was like uh i mean that's the thing about uh that's the cool thing about memory and memory is fast enough for a lot of people so you used to have like a giant array you want to insert something in between you just like rewrite the whole thing and put it back on right but of course we reached a point where that wasn't working for the larger customers especially people with like tens of millions of records so we we had to redesign that data structure and uh rewrite it um and that's how we ended up with that on uh on role link list or you can maybe google that up so with that we are able to get like uh we amortize the cost of updates uh hi kishore here so one concern i felt while working with elastic searches like you know how do you deal with the nested fields so like you know they if they have somebody there and then you have to use like nested queries which is slow so are we doing like similar thing here or are we treating like nested documents in different here yeah so the number one uploaded feature request on typesense uh with like i think 35 upwards now is uh nested field support uh right now we just ask people to flatten their documents um it might not always be possible for for vast language or majority of the cases let's say you have a person object with a first name and last name you can just do a person dot first name person or just flatten it and before indexing it uh i agree it might not always be possible let's say you have a list array of objects it's not possible to very easily do that so uh so the way we will be and that's going to be the number one feature we're going to work on in the next release uh is what we are planning to do is uh flatten it and store it uh and um and on the display we will uh nest it back and give it so you will still have that uh column wise uh speed because you are not going to mostly uh query the entire nested object only specific fields usually um uh so it will get flattened it will be the flattening happens will right now people are doing the flattening manually but the system will take care of flattening it in future thanks how does this authentication is handled or is the responsibility of the client to handle the authentication or authorization yes so uh the way that what we do is uh we so we generate we allow you to generate an apa key from a parent api key so what you do is uh because see let's say i have let's say i have 10 000 customers and i it is very difficult for me to generate one apa key for every customer uh if i want to scope them to specific either collections or i want to scope them to a specific filter for example let's say there's only one collection and then i have a field called org id right and i want uh i want to scope uh a particular customer to a particular org id what i can do is uh i will generate a key on the client side not server side type sense doesn't manage it what happens is you generate a key and you ask and the key will have the filter baked into it so let's say the filter is org id equals 100 okay so what i do what we do is we we okay cryptographically hash it we create we get a signature from that hmac uh so that uh and then what we'll do is when we when the client makes a request we take the key uh get the hmac part of it uh signature part of it and then we we and the filter itself is part of the key in a base uh based encoded form so we will get the filter rehash it compare the hmac signature with the one on the key and verify that you know it is not being tampered so by doing this you are basically able to generate keys on the client side um and without having to like generate thousands of keys on this uh server and managing them so you can and so we you you can generate these uh what we call this as scoped api keys uh and then we are able to ensure that you know nobody is able to do a field right if i want this let's say somebody is trying to do like org id 200 when they are not supposed to they cannot do it because the minute they change the search query to org id200 and you send the same key the cryptographic uh the hmac signature is going to mismatch and we will reject that request okay thanks so uh i mean so one was about the throughput i think has there been a pattern about the kind of use cases that you've been finding a lot of adoption for especially on the commercial side um what other kind of industries use cases that you've really seen given this whole vertical scaling constraint etc yeah i see the thing is uh most people are 95 of the people don't have large data and uh but they they spend an enormous amount of time setting up like let's say you want to search only 10 000 records the amount of time that it takes to set up elasticsearch understand what it means and all that is like humongous right so we are targeting that other 95 percent of the people uh which is a which uh which is like a huge market because um even if you take the even if you take the databases right post page databases and stuff like that they they like 10 million 100 million rows is like a real big amount of data already for a lot of people uh the guys who are having billions of rows they are not looking to do searching on top of them they are looking to do a durable storage and like lookups and so on so they're very it's very only log storage and metrics and monitoring and stuff like that for example security incidents that's where you have like a large volume of data and you want to be fast and you want to do quitting and we're not going there uh what we are focusing on uh is e-commerce um where search literally prints money so like all the things about speed and stuff comes into play and none of the things like the amazons will be able to do like machine learning is possible because you don't have that many customer data and that you don't carry that many products you cannot do data mining you cannot do personalization there the only key thing that can really uh make a difference is the speed of search and uh you know you've seen that statistics right every one person every 10 milliseconds to go down is one percent revenue or loss or something like that so uh so that really helps and the other use cases that we see is like uh just uh you know like searching uh like uh blogs or searching uh like internal um community like internet intranet kind of uh internal apps uh those things white broad basket there is not a small data is far more prevalent than we think if you are exposed to lead to the large world is a huge one of people who just don't have that much data but they still want like good search experiences on top of them thanks dude uh guys you you have any any more questions to ask for okay okay so a couple more so one is uh so in recent times besides algolia right there have been a bunch of attempts to build something similar like miley search i think in rust and a bunch of others right like how do they compare what how much of a threat are they uh where do they stand yeah so uh broadly speaking um from a business so i answered it in two ways i launched it from a business perspective from a from from a technical perspective from a business perspective i think uh uh i'm not worried about competition because it's such a large market uh and uh in fact algolia is a leader in this space they are property they're not open source but if you go and ask developers every time i talk to developers they know elasticsearch and that's what they reach out for first they don't even well let's probably spend tens of millions of dollars in their brand building marketing over the last seven eight years but people just don't know them right so that's how the big the market is and it's growing much faster so uh more competitors are welcome uh and we are not anyway like uh you know in a rush to like we're playing a really long game here uh we're not in a rush to like you know like exit or anything so that's fine uh the other the other thing about um the other uh main players here is obviously uh elastic as a app search uh um which is an offering which is hosted which basically simplifies elasticsearch for you but it's a hosted offering it's not a uh it's not a open source thing and you will still there are still rough edges that elasticsearch for example doing fuzzy searching with sorting on a field is really tough like you cannot say that you know i want uh type words and records with no typos to be above records with typo1 and typography there's no way possible you can do that elastic so they inherit some of those problems um there's algolia which is uh which is which which probably uh is trying to do that but the one way that we refer from both algolia and military searches are also trying to do it in open source format they have a very major restriction where uh if you want uh if you want to sort by a different field you have to duplicate your entire index so imagine i have 10 million records and this is uh so they they are very fast because they are pre-sorted the disk uh the data is pre-sorted on the disk so they have had customers are always coming to us and like hey you know we wanted another sort order but now it build doubles so and we don't want that and doesn't does it doesn't build confidence that when you're saying that you know i have to duplicate they will do it for you it's not like you need to create index and manage it but the minute you want to sort by another thing they will have to do and that takes a lot of uh i o it takes a lot of um cpu um and but what typesense has done is they are able we have solved this on the fly sorting and on the fly filtering really fast right so so it offers a lot of flexibility for people it will be a little slower on large data sets because obviously it's an n log n you cannot beat it or n log k in search well because you are not trying to sort all the records but only the top few hundred or whatever you need um but it's fast enough for uh for like a vast majority of use cases uh and we might add pre sorted uh indices in future if that is a that is a threat uh but that's one major feature that uh um research they have kind of inherited the flaws uh flaw that uh elasticsearch i mean um alcohol has which we have solved and there are a few other things like we support infix searching in the next release which uh which is which is not supported by either of these two engines and uh and we have a lot more things planned i think uh so it's just getting started face we are in just that phase because uh like it's not an mvp that you can build on a weekend or in a month uh it has taken five six years to build this out so we have now a nice base to operate from and so the differentiations will occur pretty fast awesome normally i think i think it's it's seriously impressive what you've been able to do how big is your team now or how many people besides the two of you and uh we are very small we are uh four people uh and uh and uh and and uh we might uh some some interns come and go we sometimes you hire people on a part-time basis but yeah it's a very small team so that this includes everybody who's taking care of the cloud infrastructure and yes clients on call and everything wow yes yes yes so it's just built over a very ridiculously long period of time right uh and uh you should check out uh typesense blog i wrote about it um uh the amount of stuff that you can do just by spending an hour or two a day like you can literally build it out the first three years were just like building the core cpp engine uh no client libraries nothing and then next three years were like client libraries and the last one year is type since cloud so if you look at it that way the magic is gone but uh but yeah but it's interesting we uh we are uh we you can launch uh we support like nearly 18 or 20 regions uh uh or you can launch a cluster that spans across four continents three continents uh we keep everything synced and uh yeah we we're have to manage with just party awesome since nobody's asking i'm going to keep going for a couple more for you okay sure yeah okay i'm free what's what's been the experience with c plus plus epsilon yeah see yeah yeah see the um it's at most both wonderful and grateful if i have to summarize uh my my thing i'm not i'm not a language freak in the sense uh i i don't necessarily think that you know like uh uh lang and i have a mature on this over the years uh languages do not really make or break companies for example drastic cpp if you ask me like if i went back in time would i pick cpp 100 yes i'll pick cpt because uh some of the things that for example mini search doesn't have a clustering solution they don't they are just a single instance so uh they don't even have an aha right nobody serious in business would uh would would be like you know we'll have second thoughts of picking such a solution uh and the reason is very simple there is no mature rust clustering library out there uh there is hashicorp has a very good library in go which is raft they use it for their products and then baidu has a really good one in c plus plus which we use for typesense but there's just no nothing stable in rough i'm sure the ecosystem will catch up over time but uh and uh and uh like you know with facebook and google a lot of interesting data structures all of them land most research data structures research papers their implementations land in cpp4c first right and anyway most libraries need a c binding because you want to expose it to python you want to expose it to ruby and none of them are going away so c library and node.js so c is a very common uh lingua franca there and that's not going to get replaced so uh i completely uh uh feel vindicated by choosing c plus and anyway seven years back uh would be crazy to choose another language there's not much there go was ruled out because we were doing just uh the gc process will just not work for our uh interactive searches uh scenario so but uh from uh from an from a hiring and point of view yes if you restrict yourself it's like scala uh which uh in my previous uh in thought works uh used a lot of scala it's the language is a large footprint and there's a lot of areas where you can shoot yourself in the foot um leaks memory leaks and stuff you don't if you're not careful uh but uh yeah but given overall the trade-offs of the rich ecosystem and the ability to hire i think uh i think it's uh it's not a it's not a big deal but did you did you uh actually know that you programmed in c plus before you were actually starting from scratch when you started i had maybe one year of experience in c plus uh before uh like some of the stuff we did at index us we wrote some analytical engines on cpp but uh but basically i learned cbp through my uh uh through through texas so yeah i was always in fact the first time when we open source the code i was very nervous uh because i was like i'm not a c plus plus veteran by any means and uh and when you're posting to hacker news or read it uh like uh i was very scared like oh this is ub you be undefined behavior because the language is you can very easily have undefined behavior all over the place it will still work but it will break one fine day but yeah [Music] nervous cool yeah that's that's all i had thank you thank you excellent talk thank you so much yeah if any of you are looking uh to use type sense or i mean any search engine or elastic search or like you know if you're thinking of moving or if you're having some issues with elasticsearch uh uh feel free to email me i'll be happy to chat not to make you move to type sense but to i don't even offer i've done fair bit of wrangling with elasticsearch so i know the nuts and bolts so feel free to reach out to me happy to help sure sure this is fantastic i think just having seeing somebody do this out of india is fairly rare i think there have been a few examples i think narayan built uh sahih early in the league he was one of the early guys narayan and then i think you and there's a bunch of others but there's very few i think it's a real inspiration i mean uh kudos i hope you guys really thank you uh thanks thanks so much for coming and sharing these insights i think it's very useful thank you thank you bye-bye amit over to any yeah nothing i mean it was very interesting session to watch and really really inspiring thanks for this special thank you thank you nice chatting bye-bye and thank you so much thank you thank you