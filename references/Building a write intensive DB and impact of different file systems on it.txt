zoom hello hi folks um so let me start with my introduction um or maybe with dev day's introduction first so dev day is an informal event um for the speakers um not for the speakers but for everyone in tech industry so that they get a platform where they can come and contribute and give back to the community so it's not associated with uh entirely uh it's an opponent everyone can come here and talk about their opinions in tech in general so um yeah so today's today i'm the speaker and i'll be talking about a problems uh statement that we recently solved and said problem was about um building a right intensive application in which um we will get a high amount of load from via http calls and we need to update it all right so let me share my screen and let's get started so yeah people can ask questions whenever uh they have any questions so before starting with the slides let me just go to terminal and let's try out a small experiment so does everyone know uh so i i've just written a small command where we say hey ls hyphen lh so does everyone know what this command does okay so let's start i mean let's let me just mix another leg let's say we do an ls hyphen lh right so what does it do so we have a bunch of things here so first one is name of the directories or the files right and then let's start from left to right and we'll try to figure out what everything there is right so the first one is either it's d or it's a hyphen right so what does it mean and won't do right so if it's a d there then it means it's a directory otherwise it's a file or it can be an executable any of those things right and then um we have a bunch of these d w x r or it's a hyphen right so if we club these together then it basically means that this file is readable writeable as well as executable for the owner of this for the user and this is for the group and this is for the uh ch groups and then we have this number two does anyone know what this means to or these ones so these are the links so where the on the this file is being referenced from these are the links to that and then we have this root uh does everyone know what this root means so this is the owner of the file all right so who owns this file and this there's root again so why why do we have a second route anyone have any guesses no can you repeat the questions right so yeah so the question is uh over here we have a rule which means that um the owner of this file is basically this root correct but why do we have group again what does the second root signify it's a group sorry correct so it's this is the group and then we have these 6 77 165 all these numbers so what does these mean these are permissions i guess no no so the permissions are these one all right where we say whether it's readable writable and executable or not and this is something different i think is it uh and that's actually correct so uh if the uh size of file is a bit larger than few blocks then it's shown in um you know in these or bits if you provide a edge with it otherwise it's shown in kilowatts and then we have this date over here july sorry yeah july 21st and the time so what does this time signify right so this is modified time now the question comes is when uh where is this time stored right [Music] we have all of this so let me just do a small experiment for this where we will go towards yeah do you want me to make it larger okay is it better now okay so we are going to deck where we have a bunch of uh files uh if someone is speaking can you please go on mute if you're not speaking uh there's some background noise from your end yeah so we have a bunch of files here and all of them are around 32 uh mvs and what we're going to do now is run a read script on this right and which basically does nothing but it iterates over this directory and it gets all these files towards demo right so there are no write operations whatsoever involved here now when we run this command to execute the script and um wait and then we run i o top on it then you can see that um read reads are being increased as well as there are some spikes in write so now the reads have gone away but there was again a right so does anyone know why these rights occur so i think these files are being sequentially read by the goods and it keeps them in the buffer to make it available easier okay so um so they were dead correct then why uh why why are there disk rights i mean even if it's trying to keep it in the buffer which is in memory i believe then uh why where why are the rights increasing that's the question does anyone have any idea or any guesses maybe it's writing them in the memory for the next file uh the information for the next five lasers um not really but um so our chan has basically said something on the chat which says file excess time is modified and that's actually correct so basically when we um if we have multiple files over here right and if we do a stat on any of these files then you will see there's there's a bunch of information there which there's excess time modified time and change time right so whenever some user tries to access these files even if it's a simple cat operation even then the excess time is increased it it goes up and now if i again run this uh script again right the read script now we're going to run this again and we're going to see how the basically the interaction between the disk goes now so what do you guys think will happen now we've done the same thing uh i mean just five minutes ago so what do you think will happen now it's the same command we're running there are no modifications whatsoever we've done to um the file system or in the directory what do you guys think will happen now it should remain the same or okay so let's say we run this now and we say are you top now you will see the even the reads are as it is however there are a few rights so initially the right winds up and it might go up again but basically you now even now we we try to read the file right all of these again 64kbps around right so why is why uh so basically excess time is still being modified and that's the reason why rights are being changed but why are there no reads in the i o top so for those of you who do not know your top iotop basically monitors the input output operations performed on the disk so do you guys have any idea why the reads did not go up and i mean it's not like a one-off thing even if we run it again and again we say are you top even then the rights won't go up sorry right so there's something uh called buffered cash which can store these so initially it's when the system boots up it is empty however when we read files and files these things are being cached but how do we basically say when is it a good time to use it or how do we modify is it is another question so we'll be talking about such things in this talk and let me now start with the slides okay i will chat and these side hoovers are not visible on um on this stream right okay so what are we doing that's the first question over here and so let me go to this tweet which yogi posted a couple of months ago when we were building this um when we were trying to solve this problem in our organization called biggo and what exactly is bigger and why do we have it so this is a tweet from yogi which says hey the idea is to develop better intuition for how these systems work and the memory and this performance the trade-offs which we uh which needs to be taken when um when we decide on which database should we use and these can only be built when we you know try to implement it on our own so in in this problem statement we were given that hey you can use any language and libraries but you cannot use any existing data source or any cache or anything of this sort right but all the default uh libraries and modules are accessible so how would you build it and what's the next performance it could support now um the problem statement i believe everyone should have read it it was in the prerequisite of this talk and it's a pretty straightforward problem statement where uh we on our server we we are given a c um we are given a machine i'll talk about the machine in a bit but the key aspect of it is um we have to expose an http server in which you will make a port request um which will contain a probe id so there are multiple probes across um the server right and these port requests will come in and you will given a probe id sorry font sure is it better now okay yeah so a call uh so simultaneous calls will be made to the server from different probe ids so different pro um um yeah different probes will make a request to your server and you have to make an absurd request right so if the provider is present there with the uh then basically update it otherwise use this provided and update the data okay and then the other key part of it is you have to always keep the latest copy you can always forget about the earlier copies so let's say we get a um payload at time x and then after some time so x plus 10 we get another uh request from the same probing then we can forget about the we can delete the earlier one and we can overwrite it with a new data right however um so to test the and we have to make it persistent and to test whether it is persistent or persistent or not what they'll do basically the benchmarking team will do is they will try to send a sick term signal to it and by sending a signal uh sick term signal it will ensure that the system goes down right and then they'll restart the server and um they might even restart the instance to see whether it is persistent in the disk or not okay and there's another condition where we say hey the 99 response time for writing a record should not go beyond 200 milliseconds so our latency is also in concern where we say hey 99 should be less than 200 millisecond so in the crunch this is um the problem statement and yeah for the gate request there's there's a there's a one to one ratio for every put request within some time there will be one get request for it so yeah so on a high level basically there's a benchmarking server and there's a data store server which we need to implement and there will be calls made to this server right so let's now zoom in into the data store server where we'll see what all do we need here second yeah so what do you guys think should be the components of this data store server anyone so we've established that um http calls will be made to our data store server and we need to write it somewhere right so what all components will be required in our data store server to you know end to end process it anyone can uh search anything so there will be multiple components not just two so what do you guys think what should be the components um yeah a http server to basically receive the payloads right uh the request and then send it to someone and do something so what should be the other components correct so to store it in a file or anywhere we will need a disk right so there's a server and there's a disk so and in order to basically extract the information out and do our algorithm as to where the file should go to right which directory should it go to and how we structure it right for that we are going to need a service so yeah um that's what we have next where we say hey there's http server data service and disk and even on okay sorry some message broker for async so it's not it's not actually async communication over here um yeah anyway so for http server to data so service i mean this is this is what we usually do on our projects as well right a request will come in to uh and we will have a controller from where we'll pass it to the service we'll perform some service um operations there some business logic and then send it to repository which will then connect to db and send it to the database however over here we do not have any db right so our data store service has to do the uh work of db where it has to do the a business business logic which is to absurd and send it to the disk now what interactions do you think so what other components do you think are required for you know making the interaction between data store service and the disk what all will be required or yeah so basically data store service you could choose any language so it could be java go anything right and this is your hardware disk so what how does the interaction happens between your java or whichever language you choose to the disk is the question do you require file system component like that interacts directly with the file system right but so let's say you uh you're uh you're using java for example right then how does um your basically java application talks to the file system is the next question are your calls right and for io calls it is going to require um operating system right so essentially our java application will interact to os saying that hey i want to either open this file or i want to write into this file in these things and then again file systems will um file system will interact with operating system and in collaboration with kernels in order to um you know interact with drivers and then with disk so yeah so we have established now that uh that we just put it up yeah that hey data store will talk to os saying that hey i want to open this file or write into this file which will del file system and these things now next question is what is a file system uh sorry just give it some minutes a few people have joined in yeah so our application will tell um that hey we need to send uh we need to save a file right so let's say we give an uh file name and the data which belongs to it which should belong to that file which we need to write into that file so how will it happen so what uh yeah sorry so what is a file system is the next question what do you guys think where does the file system come in into this whole picture i think it's a representation of the storage in an operating system right so correct so basically in disk we have um yeah so or maybe let's start so basically file system helps us to interact with the disk right like you said it um gives a structure it provides us a way to deal with the disk right so first let's dig deeper into what is a disk so let's say we have a hard drive or anything right so what's inside of a disk what essentially is a disc it's a storage machine right but how does it store it right correct so that's for uh right but even if we go to the lower level if you're looking at the bottom most level watch what are there or how does it store it so basically there are bits right at the lowest level there are supposed to be bits correct so there's bits of bits of data um i'm getting an apple yeah if if you guys are not speaking can you guys please go on mute yeah thank you so yeah we have basically um established that in a disc we have bits at the lowest level we are going to have bits right now os or kernel has to talk to disk correct now how will it know or how will we basically tell that hey which bit you need to go to there has to be some addresses or something around those bits right that hey i'm sending you this data which i need to write into this file and then so basically i am asking about what is the this what is the mapping between these two so we have a disk in which we have different bits right and we have an operator or our application right which is going to send an um command to operating system saying that hey this is the file name and this is the data that i want to write so how do you think this mapping happens there are certain pointers which are connected to each other and there is a certain pointer which is um right so what's your name sorry cannon okay so uh karen here is saying that um there are certain pointers which will basically point towards link which acts like a linked list and it goes through uh each bit right but even then we um in a persistent manner we need to know where so basically whenever the system restarts right even then we need to know where everything is mapped so there is a certain pointer file specifically can we can we use this and this will take the input is it yes can you can you repeat again speak can you try here do a mic test please try now excuse my audible to everyone on zoom can you say something again hello yeah so there is some eco but yeah it is there's some echo but it's audible oh maybe we can pass this and do that just turn it off hello so actually what i have studied according to it there are some pointers and one pointer means there are many pointers the database so some pointer has index related to all the information regarding it so whenever we want to access some file we go to the index pointer we check whatever we have needed and we go directly to access to that so that's true that um there has to be a mechanism through which we'll point to everything right but if we do it on so let me try to demonstrate this one example right so let's say we have 1 gb of disk right which means we are going to have 1 0 2 4 mbs and 1 four kbs and one zero two four uh by uh bytes right and then into it which will give us bits total number of bits now this if we were to address each and every bit right if we were to give address to it you know each and every bit of it right then we it would be very it would be a very long number right and to store this number even if it's in a linked list or somewhere in a persistent manner it will take again to store for a single bit if we have to use these many numbers right assuming that it's a uh it's in base 10 right then that would be a huge number so to make our lives easier and by i mean by saying r i mean to make file systems life easier what we do is we try to basically group these bits into a block and this that's a smaller unit smallest unit in a file system which is called a block so because we cannot basically count on every each and every um these bits right so what we do is we create multiple blocks over the disk and say okay for um and we define a block size saying that hey minimum sorry the block size is going to be let's say on an average on our usual systems it's around 4 kb which means even if we are writing a file of 1 kb into the system then it means it is going to occupy 4 kb on the uh disk because the small the smallest block size is 4 kb right now which means um now the other question which comes to our mind is now how do we decide what should be the block 6 right so what do you guys two types of think one can be variable and one can be fixed as you said there are minimum sizes that would be a fixed block sorry um yeah so karen here is saying there are two types of blocks one is uh variable and one is fixed size however in file systems uh at least in ext4 and excel is the usual file systems there are only fixed size blocks of any defined number right and it stays consistent across all of it because otherwise think about it we'll have to keep and track of which blocks are of which size as well right that's an added variable to the equation right so to make it easier right no so basically uh the reason why fragmentation occurs is because uh like we gave an example right we we have a block size of 4 kb however we are storing files of one or two k right now if this is a one-off case where we let's say we have a um around 1 gb of disk out of which we are saving let's say 8 files of around 100 mb each so 800 mb is being used pretty good in a good manner right there's not much fragmentation there however if n then we let's say add only one or two files which are of one or two kb right in this scenario kind of makes sense right because there's not much fragmentation there right however let's say our use case is uh even in this problem statement right we have a use case where the average um load load size is going to be 4 kb and the maximum is going to be 20 kb right but if let's say if it were 1 kb then it wouldn't make sense to use a block size of 4k because then we are essentially uh wasting um our disk right so that's the uh sort of key component of it to decide what is a blocks um what should be the block size there is no silver bullet to it there is no uh you know a straightforward answer to say that hey your block size should be this much it depends on users need in our workload so if and there's another variable to it which is if we have a very small block size then um writing to the disk will take a lot of time because then performance takes a hit right because essentially there are multiple calls to it right so um that's that's the reason why um block size should be is dependent upon your workload so there are two types of fixed size blocks ones are similar then the 4kb are divided totally by five of them okay i think you should sorry uh let me pass through my start from top so the fake size uh blocks there are two type of fixed size blocks one is if uh if you take the example of 20 kb then there are five blocks of 4k before kb4 kb 4kb there are self similar means they are similar and while the other is 2 kb 4 kb 8 kb and 16 kb i guess so which makes total 20 something like that when then what is the condition then which block size we use over m is as you said earlier that we use the fixed size block and this then which is which fixed size block we use the second one here yeah so let me just demonstrate a small command on a normal rail server where we're going to say hey uh just give us the information about this disk of us and there we can find that here the block size is around 4096 which is in bytes which translates to around 4 kb right and there are a bunch of other things as well now let's forget about it for a minute okay so let's let's take another example so let's say we are trying to build our own file system okay and we have let's say so yeah so so far we've established that there's a disk and there are multiple blocks in it right now let's say we're trying to build our own file system and it's around 10 kp okay so 10 kb means there are around these many bytes and these many bits right now we also established that hey storing this information directly is a bit hard so we are going to make blocks of it correct so let's say there are uh we make all uh so since it's a small number so we are making only we are only getting around 160 blocks okay assuming that it's around 512 bytes each block okay now os issues a command saying that we want to store this file in um in the disk and this is the file name and this is the um basically content of it um it comes to us and we say okay these blocks are free right and we try and we say okay go ahead and write it there now we do it for multiple files eventually let's say there's a power failure or the there's a system restart right now we were just writing it down to ourselves whenever a new command came in we said okay this file is there go go there and update it either or create a new file correct now let's say there's a restart or there's a power failure and system tries to recover now now what will happen so let's forget about the power failure scenario right now just think about there's a normal restart now since this was all in memory right now the system uh boots up again there's a disc in which there's there's this block of information residing there but how do we know in which block we have what information there are some recovery methods no but so even for a normal restart we we should not use a normal recovery method right so when when we boot up our system right so there should be a way to know that hey in which block what information is there what i'm trying to say is if we just keep all this in memory then it won't uh all this information so we have file system right we need to make sure that hey whatever we are trying to write into the disk we keep this information right now the other thing um what we can do is we can we should store this information that hey this file resides there and it starts from this block right and these these blocks will be used for this uh file we should write this down into our disk right only then when the disk restarts we will say hey go to this file right this will give you all file system related information and we will basically read through it and we will form a directory structure so let's say um the core of um any operating system right the of any disk that's slash right the root directory so that's the first information we need to read up on because based on that we can build any everything else so we basically say hey this is this is the address of this root directory is this block and from there you will get other blocks correct so we need to store this information somewhere now if we store all this meta info and this is called meta info right because we are just storing information about the files that are inside the disk right so if we store this information inside a block then what will happen or basically what do you guys think where should we store it so we've established that we need to store this information inside the file system somewhere right so first question comes why not just store it into the block right we have multiple blocks so let's for each file we'll use a block and store it there what do you guys think will be a downside of it or will there be a downside for small files uh blocks space would be wasted right also even for large um large files right so let's say your meta info is usually of um so like we said earlier block size is configurable on a system where we say hey our um workload is more and we want uh let's say a block size of 64 mb right because we are only going to have files which are greater than 64 mbs right then in that scenario if you are also saving the meta info inside a block then basically to store a few kb worth of info we are going to waste a few blocks right so there should be a different um manner in which we store these files uh and this that's the reason why um basically blocks should be these blocks should be in a different location and there's another problem with it which is uh or maybe we'll come to it in a bit so yeah so basically all these blocks will contain um the disk information which which is related to the files all the file idle information will go into the blocks and all the metadata will go into the inodes so going back to that command that we ran earlier you will find there are free blocks some what and there are free inodes as well now and again if we do an ls and stat on any file yeah then you will find that um this inode number as well so this inode number signifies where uh the information or where the metadata of this um basically file resides in and this is a single number however so if we create a new file right and we do a ls iphone uh or maybe a stack on this new file then you'll see the blocks block size is zero right and when we basically write with it its size will increase however no matter what we write into it its inode number will remain same because metadata is always supposed to be same right because no matter how how big your file gets your metadata which is basically these information right excess time modifier time all of these things and even the bits that we talked over here ls hyphen lh and i was asking you guys right where is this information store so all these informations are stored into the inode right so basically let's say we do um this right and let's say i start on dot as well so there's inode number to it and all so basically even the so let me start hyphen right so this i know number is 128. so basically the directory of it is um so the the root directory contains the i know number 128. now when you when we so what what is what do you think should be the metadata info of the root directory or any directory for example so access time and all of these are there and blocks well since it's a directory and every so even directories are files in linux systems that's why there's a block associated to it however it will remain zero right the number of blocks associated with it is zero however there's a inode number associated to it right so this inode number will give you information uh will give you info about all the um so basically what happens is when we want to read something right it will give us an i know number that hey by default let's say os issues a command and it says hey go to that directory and give me this or maybe get me that file right in order to get to that file it needs to basically um get to the directory first right so let's say i want to go to your home for that i need to get your address first right and towards first to your uh town and then to your home right so basically it starts from the first inode which is this one to root directory and then we go to the other one which basically gives us uh inodes of all the entries inside of here so let's say i do an lsi ls on this right so all of these will be contained inside that inode so the inode of 128 will contain a linked list to all these inodes not a linked list but basically a list of all these anodes saying that hey in this single uh so this single directory references to all these other files does it make sense now there's multiple info um there are multiple informations in the inode all basically who can write to this file and all these basic basically permission information as well as who is the owner owner of it what is the name uh and the information which we saw in instead as well all of this are these are included in the inode okay now let's talk about the um scenario that we were talking about earlier the power failure one so let's say now we have inodes which basically will give us information about directories as well as files inside their directories right so let's say os issues a command saying that hey i want to write this file and um basically let's say it's a huge file around 150 mb or something not a huge but relatively huge and during that time while we are writing it there's a memory uh there's a power failure right we were writing it and basically few i nodes were created to it saying that hey i want to write um inode will only contain meta info right so or you to take even a bit more complex example let's say we are copying a directory from which will have sub directories right now when we're copying the subdirectories to other place it will again create um other inodes right now while this is being uh done there's a power failure so basically your inode data gets corrupted as well as your blog data will get corrupted right now when you do a so even in a in your recovery mechanism what will happen is it will basically it will be very hard for it or so or maybe these recovery uh mechanisms came in later i mean so we we are trying to build a file system from scratch right so in this scenario how do you think the system will recover is there any way for the system to recover no but in this scenario how yeah is uh i mean can you suggest anyone in which we how right so for log base we have to first create the logs right so that's where uh let me go to this article so basically in fat fat32 hfs all these file systems this is how things were implemented basically there were inodes and blocks but there were no journals and uh yeah so basically um ford fed 32 and these there were blocks and nine nodes but there were no journals and then they realized this power failure um problem where if some thing if power goes out then there's a chance that all the data that was being uh written is lost as well as there are um instances where even the whole disk is corrupted so how do we recover from from that that was the problem and then we came up with journal file systems journal base file systems and what happens with uh yeah so in the other late 1990s such as ntfs uses journaling to solve this problem so we all uh understood the problem right that hey when we're writing to it whole list gets corrupted so what uh yeah does i not exist for every block no no no in the diagram you showed it felt like that that's right no no okay sorry yeah so basically uh you do here ask that whether i node exists for each and every block so that is not the case because i mean if it did then this wouldn't make sense um what we had over here right if we see over here we have free blocks and free inodes if it were to exist if i think think of iron as as a table like a map and and pointers to the files locations right so for each file yeah so for each file we will have an inode but each file can have multiple blocks right yeah if i have multiple blocks i am talking about i know right right right so each file have multiple blocks but each file will only have one inode and the blocks right so yadhu here has asked that whether um blocks can be non-continuous and that is true so there are instances where um you know continuous blocks are not available right so it will basically point say hey from this to this these blocks are being used and then these and all of this information is stored inside the inode yeah well just one question so i i know that we see here is the starting point of the of the file right at least that's my understanding uh which which i know so i know that you are showing here uh so the stats that you did right um so so far so straight for this so if we do a stat for root directory then it gives us 128 which is uh so before it it's possible that you know i nodes for um basically the file system and other things it might be there but for the root directory it starts from here and because from root directory we can essentially get every we we can get to every other location right yeah that's what i am saying so that 128 is the location of the first one yeah from where it starts and then there are 28 links right also after that right right so if you if actually if you check it on the file that makes that will make more sense right so that's that's the location where it essentially is there right okay yeah so uh what will we okay does the item store all the addresses for each memory block or are the memory block associated to a link this basically will tell you where the next movie block is certainly i don't have all the data saying for the five memory blocks are all of the file in different locations saved in the item um so inode will contain a linked list to all the memory blocks that are being uh used for it and actually it also depends on the file system that you're using because um not each file system but depending on the file system the implementation might be different but inodes will be there in pretty much all of them so for recovery can you not just use that information like whatever is going to be written in the iphone just try to follow that right the question is can we not use that information for recovery if the inode has information like where which are the blocks that are already written and for how many blocks the data is written can we now just follow the right path and see till where it's written and then continue from there right but what happens when uh if there's power failure while we're writing to the inode exactly so that's why um journaling came in where we basically write so journaling is like a right of or let me not go there so journaling will basically write everything so uh yeah so everything will be written to a journal first basically we'll associate some memory beforehand so it's like you can think of it like uh as a pre-staging area right where we write everything there and when we are sure like okay after every five seconds or whatever time we decide on we can write everything into the disk so everything essentially will be first written to the journal and then bit time whatever time we specify it will be written back to disk plus everything yes now there's uh there are different mechanisms on it as well let me go back to this block uh is the font visible let me minimize this yeah cool so there are three modes in which we can use the journal so this is for ext3 and ext4 so there are three modes first one is journal mode in which basically metadata is the inodes as well as blocks all of them are first written to the journal and hence it it is the lowest risk mode because everything is first written to the journal and if there's a power failure or anything then we will read back through the journal and say okay this data is corrupted we can forget about it and continue and because of this there um because of this there's a decrease in performance right because first you have to write to the journal which is again in disk and then you have to write uh basically put everything to other place right in the disk so when the journal is filled up or when the certain time has passed we need to write everything into the disk and if while we are writing to the disk if there's a power failure anything then we can make use of this journal which you were saying right the recovery mechanism this is the recovery mechanism where we say we make use of this journal and basically correct our data exactly so this is uh more time consuming and hence the signal uh there's a significant decrease in performance right and that's why there's another in the default mode in linux distributions so this is what i'm trying to uh explain here that not every in uh not in every not every mode is um basically applicable on every uh so basically depending on your approach right or depending on your need you have to select your mode so if you want a very stable system and your performance is not an issue right in that mode journal will make more sense to you that hey this is very information very important information right and even in case of power failures i want this information to be there in those scenarios you should use journal and there's the this other mode which is ordered in which is default in most linux distributions in which metadata is written to the journal right and then um this the disk info all these all the real data is written to the disk in the first one both of them metadata as well as the data is has to be written into the journal however in the second one what they did was write only the metadata into the journal and then write the uh data into disk and this is ordered which means the first metadata has to be committed to the journal and only after that um basically data can be written to the disk so that in case of a failure or anything you will only lose this much data so everything that resides in your blocks is data and metadata is your inode everything that goes inside your inode is metadata okay now the third one which uh is the least safe is right back what happens is it will basically find which one um um so depending on the situation it will say okay it right now it makes more sense to write the uh metadata first to the um or maybe the data first to the disk right basically this is the most performant one but it is leasing it will write everything to the journal as well not everything but the metadata but it not in a specific order in the above one it is always uh guaranteed that the data is first uh written to the sorry metadata is first related to the journal and only after that data is written to the file system over here it does not do that it basically looks at the performance like which will be more performant [Music] yeah so kirin here is asking can't they do it simultaneously so the problem is um iops are limited right the input output operations you can perform on disk are limited and hence we cannot do it similarly i mean all of this is paralyzed so let's say there are two different files that has to be written that information can be paralyzed right however if for a single file only you have to write the data as well as metadata if there are more number of input output device then can we write it simultaneously so no reason being if you have a disk right and you want to write some data to it then that data has to go into this disk says uh metadata info as well right so basically if the block resides in this disk then the inode also has to reside in this disk right and hence you cannot paralyze it depending on on how many um disk you have okay does it make sense any other questions or anything from anyone no okay so this is most performant one but the problem with it is if um if there's a power failure then you will lose your data and as well as your journal but across all these three one thing is guaranteed that your whole disk right will never be corrupt because of a power failure um there might be data loss but it won't be current so now coming back to our problem statement so we we've discussed all of this right but how does it link to our talk so then we thought about okay which language to use right and then we started off with uh you know like if we do a normal google search right hey which is the fastest programming language and well it starts with in and obviously we skipped all of these because who wants the number 10 and these right we just want to go to the first one which is the fastest it was c but it is quite complicated to coding right so we thought okay let's not do this then there was c plus plus but with c plus plus there are there are concurrency issues right and you have to handle it hence we thought okay let's go with the third one which is rust and that did not turn out well the reason being uh we didn't do the research properly and we basically okay and then we did another um google search where we say okay benchmarking of basically the fastest uh http server sorry so when you say sorry so when you say you wanted to find a language which is fast so fast enough means uh what what aspects you are looking for the faster is it like cpu intensive fast or you want to write fast enough to the disc or something okay so in the problem statement or basically the problem that we were trying to sorry just give me a second yeah so the instances that we were given had only two cpus right so obviously it was quite clear to us that hey uh since it's only two cpus we need to make sure that uh our language basically whichever language we use does not occupy most of it like right if if uh let's say we use python for instance right then most of the time will be spent in parsing the data and doing trivial operations however it won't as much it won't spend as much time on the writing stuff also other bit is even when we say we want to write things on disk right that consumes cpu power even though cpu is very fast but even writing to the um disk will consume your cpu the reason being like we just um you know studied so far that hey all of this data you need to go to the inode and then get these info basically all of this processing has to be done via your cpu so hence there were multiple um so when we say fastest programming language our main concept was which would be the fastest for the http server because that's our entry point it has to be fastest there right and hence we ran out we used intex plus tokyo which was marked as the best http server and the results were around 10 000 requests per second and this was just http server there was no data involved or anything there was just put request being made to the server and it was replying with back with a status of 200 and we were only getting um ten thousand or eleven thousand requests per second or maybe even less so then um um my teammate malik version had earlier worked on um a similar on last year's biggo he had worked on a similar project where they um they again had to use an http server so which was built using golang so we used that instead and the performance at gate was really amazing so let me increase the font let me go to the slideshow yeah it's visible right everyone yeah so this is just the http server we use for golang um fast http it's called and with that we were getting around 123k request per second and 99 tile time was around 2 milliseconds so that's really good right and this is with two core cpu so if we go uh ahead then basically we did a top uh so while these requests were being made we did it we ran a top on that server and we saw that hey this 99 cpu time is around 199 which means we are getting most out of out of our cpu right and the benchmarking server from where we were running it was so it had around eight course right and out of which around seven are around fifty percent so it it strength pretty hard to benchmark a server so which was a really good sign and we said okay this is good enough for us right and then what we thought were what we thought of is why not just write the uh so basically we are going to get uh request right with probe id and we have to write the data into the disk so rather than using a very complex system we thought why not just extract the probe id out of it and write data directly into disk by using that probe id as file name okay so we did that and um these are the results with xfs file system or maybe let me go back to another question first which is how do we decide which file system should we use so we've talked so much about file system so far so how do you know which file system should be used so there are plenty of options out there in the market uh there's like we read ext4 xfs zfs correct it again depends upon the your requirement right now most of these um or that's one way to do it or yeah so basically there's a good article from reddit over here which talks about this let me zoom in types of file system and so now we know the requirement right so the requirement is now you guys are let me change this yeah so now we know the requirement which is that hey there will be around these many requests coming in right and we need to write it into the um disk simultaneously right we are not creating a anything complex just one directory in which we are going to dump all of the data so which and on an average file size is going to be around 4 kb so what do you think should be i mean how do we now decide that which file system to use and this is where we had to do some research and this is a really nice article that i came across recently which talks about xfs as well as ext4 file system which is the default for um real as well as ubuntu as well so most of our files on the file system that we use usually are ext4 right and these are the questions that you need to keep in mind when you think of which file system you should use how do you have a large servers and all of these things right and there's this interesting bit where it talks about um yeah another way to categorize is that exe file system variants tend to perform better on system that have limited i o capability and that was our case because in our system we had an i o i ops uh limited to 6 000 and yeah and the benchmarking we did on the server was around 150 160 mbps right so this so hence ext would perform better however for anything if you have large files right then xfs tends to perform faster for anything else so um if we go back to our slides we've done http and then we basically ran benchmarks for it right so on xfs we did this and we got around 99 k request per second which is sort of good enough right and then we went to um ext4 to do the same thing nothing has changed everything remains the same same application the code that we wrote is same for both of them just the file system is the only component that has changed and now here we got around 11. 1k request per second and um this latency aspect of it as well but the um average require sorry the 99 request time is around 27 millisecond over here and in the upper one it was 175 which is quite bad so why do you think this um difference is there xfs is like really struggling compared to ext4 the request per seconds is more as well as the 99 percentile time is down do you guys have any guesses sorry because of two cpus um that is partly gross so first you do said journaling so even on xfs we have journaling but it's not as advanced at ext4 so that's one reason and the other core reason is um so why do you think um two cpus will make a difference for ext4 but not for or maybe for ex xfs but not for um ext4 so basically the number of cpus remains the same in both those scenarios right so yeah it was a guess only since you mentioned that export works better with the less if we have less ios higher resources then exe4 works better so i took a guess something right so that's a really good educated guess but the next thing is why do you think um it ext4 performs better when we have low basically small files basically low load that's the other question right so what are the scenarios when we would have low load that is when we are working with small files because when we are working with small files then what happens is we need to update the metadata continuously right so inodes are being updated as well as the blocks are being and this is where um xfs travels but ext4 does and why does that happen it's because of their implementation but this is something that we do not you know usually know and while choosing a file system we do not keep in mind so hence ext4 worked really well and then this interesting there's something interesting let's try it out now uh so let me first show you guys that we uh have a server in which we have around 100 jb worth of things right and the file system we are using is ext4 okay and now let me go through tmp slash data and we are going to um so the new people here are okay let me just tell you guys what dd is so dd basically copies the content of a file you give it an input file this if is the input file where we say hey from dev random it will random will give you random values okay or you can even use zeros or anything here right use uh we are using random using random here and saying hey out write all of this file into file.img the reason why we're doing that this way is because input file won't have any overhead because this has been coming from the cpu right so there's no disk operation involved in the input file and we're only writing to an output file and we're saying hey block sizes of 4 kb which is average and in our use case as well it was given that the average payload size will be 4 kb right and the count is sorry oh man yeah and the count is 262 four which means around one gb worth of data so how much time will it take over here right and we have another file system over here let me increase the font size here as well and we're going to run this same command over here let me just see whether there are any files over here and there's nothing so let's run this command everything remains same right so this is the machine is same as well so same as in these are different instances this is 189 this is zero but the type of the machine is same so this one is xfs right uh over here and the first one is ext4 right now if we again um yeah now we're going to run the same command again first on xfs and then on ext4 so again we're trying to read from the random file and then we are overwriting the existing file which resides on the disk which was file.img so when we ran it first time it created uh a new file but this time it it has to overwrite it okay now how does this perform and let's do same for the ext bit as well so in over here it shows that hey it gave the same performance however on ext4 we're going to see something very strange of sorts so the performance has dipped drastically over here we are now getting 68 mbps right earlier we got 162 mbps but when we are overwriting it it has dipped to 68 mbps do you guys have any guesses as to why this might have happened okay so what's your name sorry over here we are overwriting it so first it might have to so the file system might have to uh delete everything that exists already and then have to do it right so let's try another experiment okay we're saying we are going to remove this file right now we don't have any uh i mean we don't have that file now and what we're going to do is we're going to touch it touch file dot img okay now if we do a stat on it it contains zero blocks there's nothing there right now again now basically if your theory is correct then if we run this now it should basically do it quickly right let's run it so it obviously did did not complete within seven seconds right so there is still uh drop in performance yeah again 68 mbps so anymore any other guesses even everyone on the call is on why this might have happened maybe it is changing the inode information frequently it is changing the inode information frequently no but so we're writing to a single file right so output file remains the same so it should only change it once and if it only if it's only changing it once then the delta between uh writing it shouldn't be 10 seconds so it's not like it is going to spend 10 seconds just to change that that single line why should journals make a difference so um by default it's ordered mode right right now it says mounted in ordered mode so only the metadata is written to the journal other than that all the data is written there so it's not journaling actually the answer to it even i don't know but we realized it that hey when we try to overrate it the performance is low right and the number we are getting over here ext4 update the file of pro with new data was around 11 000 this is this right so we thought okay what if we remove these right we basically before writing to the file again rather than overwriting it why don't we delete that file and then create a new file what how will uh that turn out and there were other optimizations as well we'll talk about it in a minute yeah so and then it went up to 19 235 uh requests per second which was which is a huge delta right it basically almost doubled so yeah that was a um really nice finding and this under the uh thing to talk here as well so no uh the uh so i'm talking about the mount options over here right no a time no diy day time no bad uh let's just focus upon these two first so what these two options so in um um initially when this talk started we i showed you guys the excess time right basically when we were reading um the files or in a directory it was trying to um do some write operations using this no a time it disables that so let's wrap this up and yeah this is my last last slide where we talk about the read rps so we are in this operation we are not doing any rates what uh sorry any rights whatsoever we are just trying to read everything from um the http server and over here we got around 50 000 requests per second and the reason was the buffered cash everything was inside the buffered cash hence everything was really fast um yeah [Music] so they were all in the memory because whenever we perform a read operation on the os it basically caches it into the memory and we can actually tune that parameter that's that's all i had guys do you guys have any questions or anything we did actually but the perform uh yeah sorry so amity is asking did we try any other file systems and we did we tried uh better fs which is better fs actually which is right optimized file system but it did not really perform that well and basically out of all the combinations ext41 was the best infrastructure related okay so amity has asked whether we did any data structure related optimization by relating to the disk we did not it was a really simple um you know method it's it's like a 10 of 15 lines method where you're saying pass the request from the http call and get the probe id out of it and write it to a fight as simple as that first remove that file and then write to hitter file that's it how did it compare to actual databases so kathy has asked a question where he's asking how did it compare to the usual databases right so we actually didn't run it on this specific machine however on macbook pro uh which has around 16 cores uh yogi tried it with level db and it gave around 80k requests per second for reads which over here is 50 however the comparison is quite you know different because it has 16 cores and the aws server that we were given and this request this is done with two posts and the and for rights it was 19k for both of them [Music] that payload has a variable um structure to it right so that's true it still work because the average is four however if there are more uh if the so the average was four but the maximum payload size was 20 gb so even with that what happens is it basically tries to um even the larger files are created but for 20kb it only has to do like five more operations right and these are the outlets on an average it's only going to be 4kb however if on an average it were let's say 10 or 15 kv then it would have made sense to change the block size of the disk to say um 10k or whatever that number was because block number as the same as right any other questions anyone it is an nvme ssd for this instance yes we didn't we did not test it but these are concurrent requests and it's going up to 19 234 requests per second and these are all simultaneous and there was one more uh variable in the question which was each probe will only come in after 10 seconds so there won't be any concurrency issues all right changing the way you store like journaling yes so journaling was quite slow uh we got around eight or i uh if i remember four or five thousand requests per second with journaling and then it doubled to 11 or something and then we implemented this remove first and then try to write it and then it again doubled so it went to around 19k requests per second all right any questions from anyone all right uh that's about it thank you guys for joining in hope you guys had a good time thanks thanks