uh so uh in the last talk as we uh discussed about the deployment and development and deployment of llm based QA Q&A system uh there are various challenges such as uh result accuracy and uh um production monitoring and scalability um so in uh in our next talk by venkatesh and Su uh we they will shed light on these challenges over to our speakers yeah uh a few details about our speaker uh venkatesh has over 12 years of experience in the area of computer vision machine learning and NLP uh embedded Ai and analytics he has delivered Solutions across a wide variety of industry and shared shares his expertise with Community by speaking at conferences some of the conferences he has attended is uh AI Summit London machine learning uh de developer Summit in Bangalore and AI expo at Santa clar USA and so on um and our next speaker and our other speaker Su Paul Chri brings over seven years of experience in computer vision machine learning and deep learning he's a passionate uh researcher in data science and um has contributed to communities knowledge pool speaking at conferences and conducting uh workshops over to our speakers okay uh hi everyone good evening uh glad we made to the last session of the day um I'll just start off by saying this uh ktia shuti siin has talked about the good good parts of the r we will talk about so the not so good part of the r because we are talking about the production challenges right so huh so now uh since we are talking about the production challenges this talk would focus more on to what are the challenges or what are the factors right that we need to take into consideration before we take any application be it RGS or llms to production right what are the things that we need to evaluate be it in terms of the metrics do we need to evaluate before putting these things to production these are the factors that we'll look into uh let's talk about the other perspective let uh let's say if we want to deploy not an RG but also the llm there are various factors that we need to uh consider be it uh factors regarding to the cost factors regarding the latency the throughput which particular model should you choose whether it's an open source model whether it's a proprietary model like open AI what is the cost associated with it all the things that are related to production we'll rather have a broader look into it right we have ensured that we have put up a lot of materials into one particular talk so that you don't have to look at otherwise all of the things that you need to take into consideration while before putting into production this is the one place to be right so with that said uh I'll uh talk by starting off with uh the four major areas in which the industries are focusing right now with respect to the generative AI right uh the first thing that we generally see that a lot of companies that are building up with the help of generative AI is document processing and information extraction right so given an example I'll rather give up an example let's say you have a lot of medical documents right of the the forign patients out there uh information related to name address blood group phone number and every other things can we uh automate in such a way that we would rather extract information out of it through the help of generative Ai and put that in the form of a more of a structured manner right so this part of pipeline is what we call it as a document processing and information extraction a lot of companies are doing work on this particular domain right there is another domain which a lot of uh earlier speakers have talked about today called the RG which we'll be going into the much more detail about what are the things that are need to be considered for production but one particular domain is knowledge base and question answering right the other part of the uh where the industry people are rather focusing upon is building up conversational agents right now what is conversational agents let's say you are in an uh Logistics industry right you want to build up a particular conversational agent related to the logistic industry right so you would rather make up a custom boat which is focus more onto the logistic industry right you want to build up a customer support agent so you would rather put up a or make up a board which would rather Focus just on the customer part right a lot of companies are focusing on building customade solution conversation ational agent uh the last part or like one major part that I see right like companies like zapier right you would rather heard the name or you would see a lot of YouTube videos ads what they do is this what they call it as a workflow automation right you have a lot of emails that you have in your inbox when you are in a business you have to read up a lot of emails what if if you can put all those email inbox all the emails into the generative AI pipeline it will read up Thea email it would rather put up a suggestion of to your email accordingly and makes the entire workflow automation very seamless for you right this helps you to accelerate your business accordingly right so these are the four broad areas on which Industries are focused upon these are not inclusive exclusive there are other areas as well but broadly industry people are focusing on this right uh I will not go into the details of RG system again because a lot of speakers have talked about it just to give you a very brief about what RG system is about ideally we have three things one is the data injection pipeline where we have a chunk of data you take up this chunk of data inform take up uh important information convert it into meaningful chunks as you rightly saw in the earlier talks you convert that chunk into embeddings and then you save that embeddings into the vector database right then you have a data retrieval pipeline right we are talking about the RG part there you have a retal pipeline where you have a query this query goes gets converted into an embedding this embedding does a semantic search on the vector database and as a result you get a relevant information chunks right and then at the last you have a data synthesis pipeline where this relevant chunks goes to the llm this llm may be open source this llm may be a proprietary llm this goes to the llm where llm takes up all the relevant information and the the query or the relevant chunks curates that information and presents to the presents the information in a much more meaningful manner right this is the end results that you get out there in the uh at the as a response that you see right llm does a curation and gives you much more a synthetically curated answer for you so that the end user can understand now the question the bigger question is now that we know that the RG pipeline is there should we just deploy it uh the answer is like building up demo R system is quite different from building up production grade R system there are a lot of factors that we need to take into consideration what are the factors first uh we have to as a ml engineering team or the data science team or the entire ecosystem as such our job is to ensure that the pipeline is generating a high quality response what do I mean by that when I say pipeline that means the RG itself is an enter pipeline it itself constitutes different parts of it there's an inje there is a retrieval there is an augmentation or synthesis when you combine them together it's a RG full pipeline we have to ensure that those particular uh pipeline is Will generate a high quality response what do I mean by that it should not hallucinate right it should not make up answers and give you an answer on its own we have to ensure the quality check is there apart from that uh there was a question where we talked about the chunks which I'll give you an answer better ways to do that but there are ways where you can compare different RG approaches right there can be various RG approaches we have to compare each of them find out which one works best and then ensure that only that part of the pipeline goes into the production right third we have to ensure that each individual as I said there is a full one end to end Pipeline and then there are sub chunks to it right and information retrieval augmentation and each of those pipeline we have to ensure that each of the pipeline also works well right and then we have to track the performance of the pipeline over time as in like we emplo deploy or employ ml Ops we have to make sure that all of those performance tracking does haveen over time and you keep a track of it whenever the performance degrades over time immediately you have to trigger an alarm saying that we need to cross check the system again correct yes thank you so okay so what are the challenges with RG systems right we talked about all the good parts now we'll look at the bad part of so what are the challenges with the na R system the first thing is bed retrieval right what do you mean by bed retrieval I told you there are two components one is the retriever one is the generator right the when we talk about bed retrieval we are talking about the context or the relevant context given by the retriever right there are two things that we need to focus upon one is the low Precision what do you mean by low Precision that not all the chunks that are retriever relevant I'll give you an example let's say you upload a PDF of the mugal Emperors all throughout who has been reigning over India right then you ask a question that uh uh where is tajma located and who built it right and then you get a the answer that you get is agar and banur and shahan while you may say that some people some of you may say that the answer is correct right AG Taj is located in Agra that's for sure and it was built by sh that's also correct but from where did the banur C right so if you go back you would rather see into the retrieve chunks you would see that shahan's wife was mumas she died in the city of banur because there is a relevance between shahan wife mumtaj when you did the information retrieval one of the chunks also came into the top results and that's why when they try to generate a response you also get agar and banur right and that is the reason why we why I say low Precision not all the relevant chunks that are retrieved are relevant in this case banur is not relevant for us that's one news case Second Use case low recall not all relevant chunks are retriev now if I say when was Taj Mahal built answer is 1631 shahan built Taj in the memory of mumas right but right after uh like uh before before that particular tajil was built there was an war between him and uh some rajput Warrior which happened in 161 right but if the answer came out to be 161 is because that all the relevant chunks that are retrieve are also not correct so there are two things that you need to consider one is the low Precision that you need to cater into one is the low recall right now that is with respect to the retrieval part now let's go to the llm the synthesis part right what are the challenges that you face hallucination why hallucination because let's say if your llm is whenever you take up an llm it's already pret on a billion of billions of other data that it already has in memory whenever it sees an unknown question it tries to make up an answer we have to make sure that those type of hallucination do not occur right or irrelevance right sometimes it just makes up Gish answer gives you an answer which is totally out of context and the third is toxicity there are at times when the llm will generate a response which is offensive and harmful so in a like s also talk about the guard rails right K also talk about the guard rails we have to make sure that those things are put into place before we put this into production right these are the challenges with the energy knife R system so what do we do at the first step before putting this to production we go for the evaluation right so what do you know so this diagram you saw earlier right there are two parts to it one is the retrieval part right so we need to do an evaluation on the retrieval part right so there are various uh various uh evaluate evaluation metrics that we need to consider like context Precision recall heat rate mrr if you just have seen this for the first time do not worry we uh yeah we'll do that fine then we talk about the uh llm uh generation where are the different evaluation metrics right like rose blue correctness faithfulness Etc that these are the different evaluation metrics now let's talk about the retrieval part right we said we have to evaluate each and individual part we are talking about the retrieval part I've taken up an very nice example just have a look at it now I have a data set of 10 movies right there are seven superhero movies and three movies related to pul fiction right the one that you see in red and color P fiction uh Shong Redemption and the metrics these are not superhero movies right now an user ask a question as what is give me list of superhero movies the answer that the that it gets from the llm RG is this part right these eight answers that you get from the L M now we talked about four different evaluation metrics let's look at it what do we mean context recall what it says how many of the relevant results that exist are returned so ideally how many superheroes movies were there seven but inside the Box how many were written 1 2 3 4 5 six so six out of seven that is around 92% of my uh relevant context was return return so this is a evaluation metric that you you can actually consider right these are quantifiable Rel evaluation metrics right let's talk about the context Precision right how relevant the results return are so results return these are the eight relevant results and out of this this six are the relevant results so 6 by 8 which is around close to 80 84% these are my values right or heat rate what it says that out of the uh answer that we got is there any relevant answer of course there are six relevant answer it's more of a binary answer as zero or one right or mrr mean reciprocal rent what it says that okay you have given me the answer tell me whether the relevant the first answer is relevant or not the first answer is a superhero movie Wonder Woman so that's a relevant answer right so these are the four metrics that you generally use for retrieval evaluation of retrial pipeline right now we come to the generation part right we talked about the evolution of the retrieval now for the generation now you have a generation llm that has generated aex you have some reference context it says the breathtaking Sunset covered the sky in a beautiful blend of colors and this is the summary that uh LM generated for you the stunning Sunset covered the sky in a mix of Hues right the uh like the first metric that we talk about is Rouge right it says that how good the llm is at summarizing right so if you look at these two sent sentences it does a pretty well good job in summarizing what it has meant to be right and this is what you generally do by Rogue it gives you a value between zero and one right we also talk about blue blue is another metric that tells you okay out of these two llm summary and the reference context how many words are overlapping so if you look at it sunset cover these are the sky these are the words that are overlapping right it will give you again a quanti quantifiable measure or meteor right what does it say is that given this particular two uh sentences it gives you a comparative comprehensive review by checking the synonyms now breath taking and stunning both mean the similar if those two meanings are similar it will give a higher score or we can also go for the llm evaluation llm evaluation means faithfulness like how how correct the answer is or like does it actually go for hallucination or answer relevancy right so now just in case if you want to try out your experiments click a picture of this can this you will get end up in my GitHub repo there is a notebook into it you whatever we have discussed you would rather you can replicate those experiments right perfect so now like since we have done the hard work you don't have to do these are all the Frameworks that you uh generally have to uh like you can used to evaluate frame all the Frameworks there is a length chain LMA index DP ragas right I know karun is not a big fan of ragas but uh if you use ragas you more oress would get all the SCS out there right so my personal preference is ragas I can recommend it it's up to you if you want to but huh these are more of if you want to take a picture go ahead but these are all the things that you would rather find in this Evolution metric right perfect so now we have talked about the individual evaluation called the retrieval we have talked about the generation now you stitch them together now you have the enter pipeline also we have to check the what is the end to end evaluation of the enter Pipeline and this is what I call it as a RG Triads this is another evaluation metric right it gives you three things that you need to consider and please focus on this and because this is very important you have a query user asks a query you get a response from the llm and you have a relevant context context means from where the source or the retrieve context that it has taken there are three things that generally uh like R Tri looks into answer relevance whether the answer or the response is related to the query that's one metric groundedness whether the response is supported by the evidence or the retrieve context right and the context relevance whether whatever it's retrieve is it even relevant or not so answer relevance groundedness and context relevance if you ask me of the all the research that I did context relevance is the most important if you why because these are the things that you like if the llms are good it will generate a response but if the re retrieve results are not good there is no way you can get a good response so for me cont relevance takes the bigger cut of the PIP correct let's take up an example which I personally did so let's say you know about Andrew Andrew NG or what we say she has uploaded a book called How to be good in the career in AI I have uploaded that particular book right and loaded that particular data set likewise as ainka showed I indexed the data set and created a vector store right now in the evaluation part what I did is that I generated 11 set of questions right these are the questions that I generated and if you want to take a picture take a picture of it this is what you call it as a true lens it's a package that helps you to generate those three R Triads right answer relevance context relevance and the groundedness right so when you run the true relevance package you get this type of answer right now I would want your attention again look at this particular score right the context relevance as I said which is the most important part is the value is low around 0.27 that means it has not retrieved good amount of content because it has not retrieved good amount of content the groundedness is low like 0.79 is on the medium value that means it is making up some answer because it is making up some answer the answer relevance though it's high as a result you could have just just said oh the answer of relevance is high let's deploy it but if you go back to those numbers you will understand that because the context relevance is low that means it has not got the retrieve content correct content right now the question is okay we did the evaluation as a first man you said okay we did the evaluations is it good enough to ship it right so not yet right we can still improve the performance I'll come to your question now right we'll talk about RG optimization that's the next part of the talk where the first optimization that we can do in RG is sentence window retrieval and that's a very beautiful concept look at it if you have a question let's what are the concerns surrounding the OK earlier what we used to do is that this particular chunk will come over the 21st century OK will likely to be W correct now what you say to the uh uh the retriever is said take five sentence above take five sentence below also include while before sending it to the llm and this is what you call it as a sentence window retrieval you increase the window of the sentence so now that retrieve content whatever you had let's say it is just three lines because you have increased the sentence window now it will look around 8 to nine lines now because it is looking into a broader context more context better answer and this is what you see earlier it was 0.27 now look at the contact relevance it's 0.4 that means it has got more contexts now more context better more confident more confident better answer right because the context relevance has increased groundedness has also increased because the groundedness has increased answer relevance has also increased right so you see like how do I how do we evaluate RS right we can't just take up this particular number and then send it to production we have to to look into those R Tri understand those values what does those values imply and then take up a call accordingly right Auto marging retrieval that's another technique right what is auto marging retrieval now again a very beautiful concept think of it you have asked a question right and let's say in this page let's say there is a page where it has four paragraph right in the second paragraph it has found three chunks that are important it has found out three chunks and in the third paragraph it has found one Chunk in the ideal scenario it will just take up this three chunks and that particular one chunk and then send it to llm for Generation but what we say is that if the number of chunks are three out of the bigger chunk that originally let's say this bigger paragraph it was divided into four chunks but when you did the query this three chunks got retrieved I would say no no don't send this these three chunks rather than send the entire paragraph right and because it has just retrieved one chunk only one chunk will go so as a result you understand now you are sending the entire paragraph and that one chunk instead of I individual three chunks as a result of it now see the context relevance Auto margin it's increasing now 0.55 because it has increased 0.95 and 0.85 now we are confident enough that okay our Rags are good enough to go to production and this is how you evaluate you just don't put it to production and we would rather evaluate on it right again I would wait for 2 seconds if you want to click a picture work out on the experiments you can scan it here and you would rather find out like all the experiments that we have done okay and finally uh some R optimizations I talked about sentence window I talked about automar on the two r the optimization there are a lot of optimization that you can do I have talked about two of the advanced retrieval optimization Ain talked about reranking so just look at this right the more you go towards the right side better the answer but more money and higher latency the more you come towards the left side quicker less money but lower accuracy so it's a tradeoff that you have to put up before you go to production right these are the things that you need to consider various other things right now there are other things like llm agents that you can also deploy which uh WESH will talk in detail about it right but I hope you could take some chunks out of it and you would rather use it while putting this before production over to you bangesh yeah hi everyone good evening uh thanks Su for talking us through some of the nuances related to the production challenges he uh basically ran you through the um evaluation metric and how you optimize them right now let us uh uh talk about the llm agents so before we get into uh like llm agents right why llm agents are needed first let's understand that so what are some of the problems or the pain points with the llms right so of course we spoke about the hallucination and all so one of the first thing that you will talk about is it can answer only based on the data that it is trained on and second one is it does not have the um access to the utilities like calculator for example if you ask um question like what is the answer to the 4.1 to the power 2.1 then the answer it gives this is with the GPT 4 okay 177.816 57 okay so the reason why that happens is it does not have like just like the humans if you are considering what you do is when you come across slightly moderately complicated mathematical operation to be done what you do is you get the access to the calculator and perform the operation right so but what it does is basically the way the llms are trained uh then it would have memorized the mathematical operations right and another thing with that is the way it processes it is meant for language modeling so it goes with the sequential processing that does not work for the mathematical operations so because of these two problems it failed in that case and another problem is it does not have access to the web so if the model is trained on the data till let us say 2021 if you are expecting it to answer on uh some question that is Rel let us say very recent events then you don't be able to answer that yeah so what are llm agents whatever problems you saw in the previous slide basically it enables the llms to be able to do those kind of tasks right so the for a while assume that put humans into llm okay uh you don't have the calculator just the example that I gave and someone gives me the moderately complicated mathematical operation to be done then I take the calculator and perform it and someone asks me like uh information that I am not aware of I get the access to the web I do the Google search and I get it right now in case of uh like llm how it works let us understand so llms are basically uh uh sorry agents are basically llm powered knowledge workers the way it works is uh in its true sense like llm you have the input you have the output but agent uh agent is basically uh it is an llm that is basically it is given a prompt to perform certain set of actions uh when it is provided with set of tools so basically tools are your like a standalone functions like it can be like a calculator it can be like a uh like access to the Google search and toolkit is basically set of all the tools that are meant to perform the similar task okay and agent executor is basically uh your agent and Tool they cannot run on their own you need a run time right that's where agent executor comes into the picture so what are some of the llm agents we have been talking about like Lan chain llama index htac and all all of these Frameworks do provide some of the agents for example let us say uh you want to build a programming assistant bot right what you need for that is like uh basically python let us say for the Python language python agent because python agent is the one that helps you to execute the python code generated by llm without this agent you won't be able to like uh evaluate the code execute the code whatever you want to do right now let us talk about another scenario where you are building a bot to generate the code in pandas the query is related to some pandas data frame operations then what you need is pandas data frame agent and python agent because whatever code is generated for the meant for the pandas you want basically that code you need the python executor for that that's where you need the python agent as well now let us take a look at basically there are some example platforms that provide the agents these are of course paid Services uh this one is by Abacus AI um so if you want to build a something like uh programming assistance tool right the this is how system will look like basically you have like AI agent that is provided with the code executor which is a AI uh like you know that helps you with the to execution of let us say for example python code and then of course there is like data that you can query to there are lot of llm models that you can have access to and another interesting thing is uh while generating while working with a code let us say you want to perform um uh optimization operation anything of that sort you you can have access to the m ml models as well with the this kind of ecosystem you will be able to basically build a programming assistance sort of tools so this is another interesting proposition by basically there is a float bot AI uh who came up with this concept called Master agent basically your llm master agent acts as an orchestrator for your different llm agents let us say there's a very big application Enterprise application that you are working with and uh some of the like you know various applications that come under this big applications are let us say there is a bot meant for the sales help desk and customer service right you want the utilities or the functions that help you to do let us say for example uh CRM Erp or you you may want to do the question answering on their PDF data so for all these things you need the different LM agent and the master agent acts as orchestrator for that so um moving on from the llm agents there is a concept called M multi-document agents llama this is from llama index um we have so far whatever we have been talking about the entire rag setup we are talking about basically um let let us say like you know you upload the documents you want to build the bot and there is a vector database you query and get the response how about let us talk about the scenario there is a large data set and you want to do the QA on uh one specific document that is one use case another one could be maybe you want to do the QA on the summarization that is generated for these documents and let us say maybe you want to compare two different documents okay and another use case could be there is a summarized version of the documents and you want to compare the two summarized versions of the documents if you want to perform any of these all four of these right with the in its original state with the just rag setup it will be very difficult and by difficult I mean you you won't be uh happy with the responses that you get right uh let us take a example of like uh uh let us say there are documents that are created for various cities in India let us say document one is for Pune document two is for Bangalore and so on hyra Abad and Chennai okay so it has all the content related to these cities okay now where the llm agents help you is basically uh because you want to perform the tasks in such a way that let us say my qu is can you compare uh the startup culture in Pune and Bangalore right uh if you have single document that is created with all the data that includes about all the cities in India then the response you you will get out of that you won't be happy with it because if you are expecting in a summarized fashion right you have the dedicated llm agents here that are meant to do for example uh like the there is a document and it directly if you see uh it is stored in the form of object index and these two basically they are doing summarization because that's what I want in some cases I'm just comparing few things between two cities in some cases let us say I uh give another question um can you uh basically com compare Su compare and summarize uh the food options between Chennai and Bangalore let us say right so for those kind of uh uh for those kind of problem statements basically multi-document agents is what you need now uh so whether it is to do with the llm or any llm based applications or the when you are working with the rag right one of the most important thing is basically how you analyze the cost and of course how you optimize for that of course like uh uh before this like people have spoken about uh fine-tuning and beyond that let us see what are some of the methods right so of course like if you are using the um open AIS apis right once in a while you will be happy with the cost and if you are working for a company like sahaj you will get the urement for that and let us say you if you are exhaustively using it on a daily basis right your cost can be more than your AWS costs let us I'm using it for some bought for 24/7 right so that's why it is really important these are like really expensive stuff you have to do the cost analysis so when we talk about cost analysis uh basically it is uh it happens in three parts one is cost of initial setup and inference what I mean mean by that is basically expenses that are associated with storing the models and uh basically running the any cost that is involved in making the predictions right that is basically initial setup and inference cost and cost of maintenance is basically uh let us say you want to improve your models on the custom data that you have then you want to be able to do the uh fine-tuning training so for that you need the labels so there's a cost involved in uh whether you use the third party uh uh platforms for that or you use the human annotators it always comes with the cost the entire data labeling task and then there is of course cost involved in fine tuning and deploying of the new model okay then other Associated costs um for example you all are aware of the um CO2 emission carbon footprint that these models generate that is another cost and another interesting one could be human resources right the whole world of the way the entire spectrum of the llm is so CPAC that uh every day there are like Publications researches being done right so for that what happens is if you want to build application that requires let us say some of the stateof the art techniques then either you need to train your people there might be a training cost involved or you want to hire somebody who is basically aware of that who can quickly build it so it is a cost associated with the human and skills right now first let us talk about cost of initial setup and uh inference so I'll be uh taking you through two uh apis one basically third party apis one is as you know by the open uh uh open AIS and if you see uh there is of course cost for input that is the cost that is involved in generating your prompt or input token and there is a cost for output and some of the U apis like coare they have the cost per request as well so you should be aware of uh in some cases let us say you are happy with the results of the chir then of course you should be evaluating it cost as well you are basically it is generating the per request cost as well now uh everyone will basically we all will think like in many cases if we want to do some uh simple to moderately complicated stuff why to make use of and third party API right and why why pay for that why not have the basically uh host the llm models on premise right that's what you will think of uh so one of the major thing is when you are working with llms if you are working with 3 billion parameters or at Max 7 billion parameters as long as you have a decent Hardware of course you can host it locally what about like 30 billion 70 billion right then of course you will go for the cloud and if we are to talk about the cloud instance cost right it can start from .6 per hour and go up to basically $45 per hour for the a 800 Nvidia gpus uh that comes with a massive 640 GPU GB GPU Ram okay so the decision making will be how you can decide is you will be aware of number of the parameters that it comes with based on that you know that where it goes so if you have to compare for example let us say you you are working for your application you need a model that comes with let us a 70 billion parameter so you have to do the cost analysis compare between eventually no point in like let us say yourself hosting that model on the cloud and you end up spending more than the third party API no point in doing that right but unless and until of course if you are building application for some of the clients that don't want to use the third party apis maybe they are working with some classified data want to host the model themselves right now let us talk about cost of Maintenance here basically when we as I said cost of maintenance is like cost behind fine tuning or uh training your models first one is by Vex AI uh automl models uh it includes the four components basically first one is when you want to fine tune or train the model you need the data so they charge you for the uploading as well beyond of course first thousand Pages uh until then it is free and then there is a training cost there is a deployment cost and when you put it into uh basically in uh to do the inference or run the prediction there is a prediction cost cost uh cost structure of like this is for the open AI apis slightly different here they don't charge for the upload of course they do charge for the training and uh input usage and output usage right now let us do the comparative cost analysis between let us say you have a scenario uh you want to build some simple applications you want to decide whether I should go for fine tuning or rack of course um um shti did speak about employ both as well but I'm talking about I have these two options and I want to select one of them based on the cost because I have this much budget how can I plan for that right if you see the first thought that comes to you is like rag like why F un it can get I mean it can be costly right but it really depends upon I have taken this example let us assume you are working with like 10 million tokens it could be across maybe you are building a conversational agent across many conversations there is a 10 million tokens and uh let's imagine you are running this for 15 uh days in a month right and what is the total cost basically when it comes to fine tuning you have two components one the cost behind llm model or embedding model and of course you multiply by like whatever tokens you are working with and you get the cost for that and then compute power cost I'm doing 15 into 24 because I'm assuming like it is running for 15 days in a month and total cost is some of that which is $250 and in case of rag what happens is of course there are um Vector databases that are like uh uh free you can use them and both of them paid one and free they all come with the pros and cons I'm imagine let us say I'm using the pine cone which I mean it's cost is $70 but here the major difference if you see with the rag what happens is um the cost mainly the $437 that you see on the right side it is coming because of the output to tokens right because when you are working with rag there is a cost associated with the input tokens or the prompt or the output tokens that is generated because of that if you see the entire total cost that is like $723 that's why so here the point is um you there is no like Universal answer like um find cost wise fine tuning is better or rag is better it depends on the underlying task it depends on the kind of the LM model you are using it depends on the number of the tokens kind of the application you are building where let us say conversation just ends in two to three like conversation between bot and human or in some cases it can go up to like 100 to 200 conversation difference is mainly the tokens that you are working with right now this comparative cost analysis if it is done across different llms what is the so we have seen that for like basically Cloud Model where the like with for in case of rag it was $723 and in case of uh like fine tuning it was 251 but if you compare GPT 3.5 they are almost close and let us say you are using a free Vector database then rag will be lesser than cost behind rag will be lesser than 5 un so that's what I meant by there is no single answer which is better when it comes to cost now let us talk about cost optimization there are various approaches I think uh three of them are like already covered prompt engineering caching with Vector stores I think uh sbin and karik spoke about it uh fine tuning covered by shti so chains for long documents this is like if you have worked with the long chain basically uh I think while coding ainka showed retrieval QA chain um there are like multiple uh chain uh like Concepts that Lang chain provides if you have worked with apachi Hadoop right you you must be aware of basically map re map reduce map rerank and refine so it is a similar concept that they provide the like chain and what I'm going to focus on is basically summarization for E efficient chart history so um imagine you are building the conversation agent where um you want the context to be maintained across the conversation right because human is asking a about and after while he refers to something that he would have asked 10 minutes ago right for that to to be able to maintain the context you need basically chart history to be appended with the context that you are aware the problem with that what happens is as you move forward with the conversation then the token size keeps increasing and if it is let us say 2048 is your token length limit then after that you will lose the context right that's a problem so uh yeah one of the ways to basically solve it is conversation summary memory this is a like memory class that is available from the L chain and usual case what happens is you do the conversation buffer memory where you will come across the problem that I explained basically you will uh uh like you know hit the the token length limit and because of that you will lose the context right what happens with the conversation summary memory right basically uh before appending the chart history to the context what it summarizes it and then appends it Mak sense when you are having the long conversation it may not make sense if you are like let us say just doing hi hello good morning summarization may come up with the more tokens than the actual summary right so this is the way you can initialize the uh basically um conversation summary memory in L chain and let us see this example right what what I explained earlier conversation summary memory so your current summary is I'm not sure whether people in the back can read it uh current summary is let us say human asks AI what it thinks of artificial intelligence and AI thinks artificial intelligence is Force for good uh so next the conversation is human asks why do you think it is a Force for good right it says because um AI will help humans reach their full potential if you see new memory it is like properly curated it is not like just it appended to that answer right so that's where it reduces the token length because it is like properly curating that answer for you but still there is a issue with this approach what is that right even when you use the summarization of course just that compared to the regular approach you may not lose to begin with context but after a while still you will lose the context what is the solution for that it is is basically whatever uh solution is offered by rack pipeline to the llms right through the vector DBS you do the same with the chart history right uh you generate the embeddings put it in the vector database and whenever there is a current conversation going on then you generate the embedding for that and then basically you query and get the response I think yeah this we already discussed yeah so what I uh the interesting point that I wanted to show is if you look at that graph right uh the one in blue is like it is better to begin with because summarization is not efficient when you are talking about conversations in the beginning because to be able to summarize you don't have much of the data it is just like um uh formalities you are saying hi hello good stuff like that but as you move forward right of course in normal cas cases which is conversation buffer memory it linearly grows so uh y AIS you have the tokens per call and xaxis you have the number of the um uh interactions but as you move forward like let us say 50 conversation dialogues or even 70 right the summary memory turns out to be better than the buffer memory now uh let us uh talk about the latency and throughput Analysis for rag or llm so four important metric that that are at play here when we talk about uh uh like latency and throughput basically first one is time to uh first token basically if you have used the chart GPT whatever that initial delay when you ask a query it's let us say you asking like uh you are giving a 2,000w uh input and asking it to summarize it and it starts generating the response right whatever that initial delay time is there that is the time to First token okay and uh basically it is time taken for model to process the prompt before it generates the first output token uh in technical way so time per output token is basically it is the time to generate the output token for every user that is querying our system okay uh the latency is basically time to First token plus time per output token into the uh I mean you multiply by the number of the tokens that need to be generated that are part of the response throughput is basic Bally rate at which you generate the output tokens these four are really important when you want to talk about basically you want to get the best possible inference speed right so what is our goal when we are working with the llm uh serving right we basically want to generate we want our model to generate the text as fast as possible for all the number of the users that we want to support but there is trade-off between uh when we talk about this there is a trade-off between the throughput and time per output token what I mean by that is let us say you talk about concurrently supporting 16 users then uh if you do that throughput wise you will be better but you will lose on the time per output token so there's a trade-off between those two uh some of the useful huris stics this is based on if you understand uh this stuff then it will be very easy for you to understand this the first one if you read out output length dominates the overall response latency what I mean by that is when you calculate the latency it has hardly has anything to do with the time uh it takes for the input token right because it is mainly driven by the um the second factor that you would have seen um it gets multiplied by the number of the output tokens right uh so that is the one that decides the uh the response latency and input length is not significant for the performance but it decides the hardware requirement let me explain this so why I say it is not significant for performances in case of latency the second factor which involved the multiplication that mainly decides it basically out output token uh number of the output tokens to be generated and time per output token right when it comes to throughput it is anyway not dependent on the input token it is the rate at which output tokens are generated so that's why it is not sign ific when we talk about performance but why it affects the hardware requirements right so each of the llm they all come with the limit on the like uh context length of the token limit right that they can support the moment you keep on increasing the input length then it directly affects the context they can maintain what you do is to be able to solve that you may use the bigger model uh or the models that are trained with a better number of parameters that's where you will have you need the better Hardwares to be able to uh like you know support in terms of memory and compute cost the last one is basically overall latency scal sublinearly with the model size what I mean by that is um basically your speed ratio is it does not match the parameter count ratio let us take the example of um Lama 2 and MPT model so in case of Lama 2 LMA 2 70 billion the latency that you find in Lama to 7 billion parameter um model is X let us say and 70 billion is 2x this may not be true for like you know uh if you take some other model 7 billion and 70 billion versions so that's what I'm trying to convey here so this is like we all spoken so far we did speak about cost uh and latency analysis but how to do it right so there is a open source Library llm analysis which helps you with basically um five five things latency throughput uh time uh then the memory okay so what what its main capabilities is basically it uh it helps you have the platform to try out the different combinations when you are working with the training and inference okay so theoretically only you can find out these things before you go and put the things into the production right because you want to understand if you are going ahead with some setup whether is it going to lead to the out of memory error or not stuff like that and when you are working with a like production grade environment of course you want the best possible batch size you want to use because you want to ensure the peak Hardware utilization and you want to use the right data type whether it is fp16 int stuff like that and it also helps you with the time it takes uh basically uh for the given setup whether to do the training or inference and of course uh as I already said it helps you with how the latency and memory changes depending upon those parameters so this is one um table that explains these things how the LM analysis tool or a library is able to do uh the meaning of flop efficienc is basically your Hardware utilization uh literature uh or the researchers found that in case of training it is 0.5 for the when you are working with llms and in case of inference it is on an average 7 basically that is the 70% utilization of your Hardware or GPU that's what it means memory efficiency is basically how efficient memory reads and rights are happening between your Dam CPU and GPU because computer is going on on GPU or CPU depending on what you are trying to do and then the weights are present in the dam so the memory efficiency once once you make these assumptions right the llm analysis tool gives you let us say you select the prompt token uh completion token it gives you rest of the parameters right based on that you know the latency you know time for completion is like basically through uh so tokens per second is your throughput and then uh time for completion is your latency then it is helping me with the price as well if you see um uh basically uh compare two things right if you want to know what is the right batch size for me you can do this kind of analysis using that tool okay I think this is just whatever I wanted to cover the most important part so far it is done it is just I want to show one thing to you people throughout this day more or less whatever we have covered they all belong to some of these things so it your llm orchestration for example just to give an example there is uh tool cost tracking tool by AIO AI uh what it helps you with is let us say there are some anomalies associated with the cost you you are running your application across the globe and it is hosted and let us say there's a sudden Spike uh in price maybe there are sudden uh increase in the number of the requests raised I'm talking about Beyond like something that can be easily found but with respect to like let us say the U consumption of whether it is llm in terms of compute memory then that tracker tool will help you to do that which is basically part of your monitoring or observability yeah I think yeah this is just we have uh spoken about these things yeah thanks if you have any questions please thanks WESH and Su for giving us insights on the practicalities of uh maintaining llm on production um we will take questions [Music] now uh this is a question to su's conversation around evaluation metrics um the metrics revolved a lot lot around the inputs that we used to gauge what the trial values would be but how do we gauge whether those inputs that we're using are going to be something that is going to be substantially close to can you repeat the question and a bit louder please I'm sorry um the evaluation metrics revolved a lot around the inputs that you use to gauge the um values correct how do we um understand whether those inputs that we use in the first place is something that would be very close to what we expect to see in production okay okay I'll give you the answer in two parts right one when you talk about the input values or the input questions right uh there is a research paper which I'll try to put in those GitHub slides as well ideally it says that whenever you want to put llm to production if you are doing a good research it's always good to have around 20 questions for your evaluation at the very very first stage right in order to have a more concrete uh answer to whether your llm is or your RS is good enough for production it is always advised to have around 100 evaluation questions now how do you do that there are two parts to it either as a human if we are not uh very uh good enough knowledge about the domain that we are working up in let's say you are working on a healthcare domain you are working on a kidney patient or kidney dialysis you have a document out of it you are a domain expert and kidney dialysis expert you will find out and list of 100 curated questions all by yourself and then you will put up those 100 questions to R triades to give you an answer right else what you can do on the other part which I also done is take this questions or give this document to GPT 4 ask it to generate questions for you along with the answers that's an automated way of doing it right one is uh the automated way actually will give you the answer but then there is no way to rationalize those questions and the answer but if you are a domain expert let's say in kidney dialysis you know what what are the things that people may ask or what are the relevant questions you would rather curate those set of 100 questions take this questions to a true lens which does an RG Triads it will give an answer and also with those answer relevance contact relevance and the groundedness you will look at the answer if you go to the true lens package it will give you like what was the answer what is the score and why did it give the score you can get a detailed RCA or root cause analysis right you can actually find out that okay maybe this was the answer that was given by the llm but actually this is not the answer it did not get the context and when you find out those answers you become more and more confident enough that okay my RGS or llms are good enough for taking this two production so to answer to your question two ways either you do it if you are a domain expert do it by yourself curate a 100 list of question before you go to production or you give it to gp4 to generate those questions and the rest would be taken care of I have two follow-ups if I have the time for it yeah please go ahead um so my concern is that in a real life use case context relevance from the end user side may not really be high because they are not domain experts usually right like they may may or may not be domain experts and their natural language is not going to be something that uh Associates to our Source input in the first place okay we'd have to design for those kind of use cases as well uh and how do we even recognize those in the first place now good question so so there I look at it in two ways right uh either you are building up an application which is life critical in those case you not give to a generic user you would rather take up an domain expert would rather like if I talk about the kidney dialysis I don't depend upon some person who doesn't have much idea i' rather bring bring in or looping that person who is a domain expert for those kidney dialysis who knows the uh enter domain end to end asks her to read the document and generate a list of 100 questions in case of a normal let's say I'll give you another end of the spectrum let's say you are building up a customer service bot or let's say in Flipkart or for a particular product you are building up an FAQ right so for FAQ would you do not have to be so critical about those answers in those case you can actually take up one particular generic user or particular employee to say that find out a list of those relevant questions and put it up right in those case uh you have to decide upon what is more important for you if these are life critical uh applications that you are building we have to be very cautious right uh Kika showed a slide where chat GPT gave 83% of the wrong predictions this may be fatal for users who have just got admitted into a hospital for those cases you can't take risk but for building up an application which is customer Centric like uh uh building up a FAQ or a customer service board these are not life fetal decisions right for those you can choose up accordingly so it depends on what you what are you building up right uh so do we have any uh concept of tooling around observability for these usage like these questions that actually come into the system I know we have observ on tooling and like what are costs Etc but very much focused on what kind of prompts we get from users and how we respond to them what the success ratio over there is ET ideally there is no one such rule or no one one one Centric rule it's all about uh like if you have this particular domain analysis you have to understand what the user might ask right if I talk about let's say flip cart uh a dishwasher 12 plate dishwasher I might the the first question that comes to my mind what is the dimension of it that's the question that comes as an user right what is the dimension I have a tri BK house I want to slide it near my washing machine what would be the size of it or how many plates can it wash can I put normal detergent on it these are the question that you know beforeand but on the other side on the domain Centric where specifically for the kidney dialysis you know user might ask very specific question sorry just clarifying this uh what you're talking about now is before you productionize I'm talking about observability after you do after you productionize ideally uh as I said like there are various monitoring tools LM llm monitoring tools that you can employ put those numbers so take out so ideally in even in ml pipeline what we do is that every fortunately we we take up a part of the chunk of the question that users have been asking regularly we take up a part of the chunk of the questions uh put into one of the evaluation pipeline right so let's say true lens has been deployed in one of your remote test server and you keep on evaluating those part of the question a chunk of the question over time if you see that the context relevance value goes down from 0.55 all the way to 0.39 then you understand that the the context that it might want to generate specific to the user query is decreasing you put up a threshold saying that okay we might have to generate more relevant questions or we might have to put in additional information to enhance the performance of the RG and that's how you do it right uh there is no one first rule you can either choose to deploy it and then like when whenever user says they it's giving up a wrong answer you then take a um measure or you may take a proactive measure take a part of the chunk of the question put it those part of the chunk of the question into your test pipeline every fortnightly and check the performance and then come up with proactive measures to deal with it that's how we would rather deal with it if you have better suggestions please let us know no thanks that's pretty good uh thanks WESH and Suman